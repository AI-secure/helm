: "
Run RunSpecs in parallel on the Stanford NLP cluster with Slurm.
To bypass the proxy server and run in root mode, append --local.

Usage:

  bash scripts/run-all-stanford.sh --suite <Suite name> <Any additional args>

To kill one of the Slurm jobs:

  squeue -u $USER
  scancel <Slurm Job ID>
"

# NOTE: this script is deprecated.  Use the `run-all.sh` generated by `benchmark-present` instead.

function execute {
   # Prints and executes command
   echo $1
   eval $1
}

conda_environment="crfm_benchmarking"
cpus=4
num_threads=8
work_dir=$PWD
suite=$2
logs_path="benchmark_output/runs/$suite/logs"
mkdir -p $logs_path

default_nlp_run_args="-a $conda_environment -c $cpus -w $work_dir --mail-user $USER --mail-type END"
default_args="--num-train-trials 3 --max-eval-instances 1000 --priority 2 --local --mongo-uri='mongodb://crfm-benchmarking:kindling-vespers@john13/crfm-models'"

models=(
  "ai21/j1-jumbo"
  "ai21/j1-grande"
  "ai21/j1-large"
  "openai/davinci"
  "openai/curie"
  "openai/babbage"
  "openai/ada"
  "openai/text-davinci-002"
  "openai/text-curie-001"
  "openai/text-babbage-001"
  "openai/text-ada-001"
  "openai/code-davinci-002"
  "openai/code-cushman-001"
  "anthropic/stanford-online-all-v4-s3"
  "cohere/xlarge-20220609"
  "cohere/large-20220720"
  "cohere/medium-20220720"
  "cohere/small-20220720"
  # Offline evaluation models
  "microsoft/TNLGv2_530B"
  "microsoft/TNLGv2_7B"
  "together/bloom"
  "together/glm"
  "together/gpt-j-6b"
  "together/gpt-neox-20b"
  "together/opt-66b"
  "together/opt-175b"
  "together/t0pp"
  "together/t5-11b"
  "together/ul2"
  "together/yalm"
)
log_paths=()

for model in "${models[@]}"
do
    job="$suite-${model//\//-}"  # Replace slashes and prepend the suite name  e.g., openai/curie => <Suite name>-openai-curie

    # Override with passed-in CLI arguments
    # By default, the command will run the RunSpecs listed in src/benchmark/presentation/run_specs.conf
    log_file=$job.log
    execute "nlprun $default_nlp_run_args --job-name $job --priority high -g 0 --memory 24g
    'benchmark-present -n $num_threads --models-to-run $model $default_args $* > $logs_path/$log_file 2>&1'"
    log_paths+=("$work_dir/$logs_path/$log_file")

    # Run RunSpecs that require a GPU
    log_file=$job.gpu.log
    execute "nlprun $default_nlp_run_args --job-name $job-gpu -g 1 --memory 24g
    'benchmark-present -n $num_threads --models-to-run $model --conf-path src/benchmark/presentation/run_specs_gpu.conf
    $default_args $* > $logs_path/$log_file 2>&1'"
    log_paths+=("$work_dir/$logs_path/$log_file")
done

printf "\nTo monitor the runs:\n"
for log_path in "${log_paths[@]}"
do
  echo "tail -f $log_path"
done

# Print out what to run next
printf "\nRun the following commands once the runs complete:\n"
command="benchmark-present --skip-instances --models-to-run ${models[@]} $default_args $* > $logs_path/run_all_skip_instances.log 2>&1"
echo "nlprun $default_nlp_run_args --job-name generate-run-specs-json-$suite --priority high -g 0 --memory 8g '$command'"
command="benchmark-summarize --suite $suite > $logs_path/summarize.log 2>&1"
echo "nlprun $default_nlp_run_args --job-name summarize-$suite --priority high -g 0 --memory 8g '$command'"
