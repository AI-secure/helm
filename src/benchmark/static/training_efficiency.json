{
    "energy" : {
        "huggingface/gptj_6b": {
            "value" : 33.5,
            "description" : "GPT-J (6B) was trained using 256 TPU v3 cores, or a single pod slice in europe-west4-a for five weeks of real time. We assume an average TPU power draw of 283 W (Patterson et al., 2021) per chip with two cores per chip.14 Thus we have 128 chips running at 283 W for five weeks for a total of 33.470976 MWh. Since europe-west4-a has a carbon intensity of 410 gCO2eq per kWh. We also assume a PUE scaling of 1.1 in the above calculation to align with other figures. We find a total carbon output of 13.8 metric tons of CO2eq."
        },
        "gooseai/gpt-j-6b": {
            "value" : 33.5,
            "description" : "GPT-J (6B) was trained using 256 TPU v3 cores, or a single pod slice in europe-west4-a for five weeks of real time. We assume an average TPU power draw of 283 W (Patterson et al., 2021) per chip with two cores per chip.14 Thus we have 128 chips running at 283 W for five weeks for a total of 33.470976 MWh. Since europe-west4-a has a carbon intensity of 410 gCO2eq per kWh. We also assume a PUE scaling of 1.1 in the above calculation to align with other figures. We find a total carbon output of 13.8 metric tons of CO2eq."
        },
        "gooseai/gpt-neo-20b": {
            "value" : 60.1,
            "description" : "Metrics for GPT-NeoX (20B) were provided by the authors (Black et al., 2022). The authors report 66.24 MWh of energy usage and 35 metric tons of CO2eq. We note that authors suggest that this number includes training, scaling, testing, and evaluation. For fairness and since we were only able to identify information on. For the training process the authors estimate 31.73 metric tons of CO2eq and roughly 60.0512914286 MWh." 
        },
        "openai/babbage": {
            "value" : 9.8,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "openai/text-babbage-001": {
            "value" : 9.9,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "openai/curie": {
            "value" : 49.2,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
            },
        "openai/text-curie-001": {
            "value" : 50.0,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "openai/davinci": {
            "value" : 1287.1,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "openai/text-davinci-001": {
            "value" : 1308.3,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "openai/text-davinci-002": {
            "value" : 1308.3,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "microsoft/TNLGv2_530B": {
            "value" : 1111.8,
            "description" : "For MT-NLG (530B), we assume the 280 8-A100 servers used for training were run in California based on press releases for an estimated 47 days; this results in 896 kW per hour of power draw, plus a PUE scaling of 1.1, yielding 1111.7568 MWh or 265.145597365 tCO2eq at the California average carbon intensity of 0.23849244489880175 kg Co2/kwh. The reduction of carbon figures are due to the lower carbon intensity in California, where the datacenter was located, and the efficiency improvements of A100."
        }
    },
    "carbon" : {
        "huggingface/gptj_6b": {
            "value" : 13.8,
            "description" : "GPT-J (6B) was trained using 256 TPU v3 cores, or a single pod slice in europe-west4-a for five weeks of real time. We assume an average TPU power draw of 283 W (Patterson et al., 2021) per chip with two cores per chip.14 Thus we have 128 chips running at 283 W for five weeks for a total of 33.470976 MWh. Since europe-west4-a has a carbon intensity of 410 gCO2eq per kWh. We also assume a PUE scaling of 1.1 in the above calculation to align with other figures. We find a total carbon output of 13.8 metric tons of CO2eq."
        },
        "gooseai/gpt-j-6b": {
            "value" : 13.8,
            "description" : "GPT-J (6B) was trained using 256 TPU v3 cores, or a single pod slice in europe-west4-a for five weeks of real time. We assume an average TPU power draw of 283 W (Patterson et al., 2021) per chip with two cores per chip.14 Thus we have 128 chips running at 283 W for five weeks for a total of 33.470976 MWh. Since europe-west4-a has a carbon intensity of 410 gCO2eq per kWh. We also assume a PUE scaling of 1.1 in the above calculation to align with other figures. We find a total carbon output of 13.8 metric tons of CO2eq."
        },
        "gooseai/gpt-neo-20b": {
            "value" : 31.73,
            "description" : "Metrics for GPT-NeoX (20B) were provided by the authors (Black et al., 2022). The authors report 66.24 MWh of energy usage and 35 metric tons of CO2eq. We note that authors suggest that this number includes training, scaling, testing, and evaluation. For fairness and since we were only able to identify information on. For the training process the authors estimate 31.73 metric tons of CO2eq and roughly 60.0512914286 MWh." 
        },
        "openai/babbage": {
            "value" : 4.2,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "openai/text-babbage-001": {
            "value" : 4.3,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "openai/curie": {
            "value" : 21.1,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "openai/text-curie-001": {
            "value" : 21.4,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "openai/davinci": {
            "value" : 552.1,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "openai/text-davinci-001": {
            "value" : 561.2,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "openai/text-davinci-002": {
            "value" : 561.2,
            "description" : "For OpenAI models that are not in Instruct series, we rely on estimates from Patterson et al. (2021) and Brown et al. (2020). In particular, Patterson et al. (2021) explicitly reports estimates for GPT-3 (175B); for other model variants, we scale the number of floating-point operations by the size of the model. We assume that InstructGPT models required 1.01648351648 more compute (and energy/carbon) than baseline models. We arrive at this since authors note that the PPO version used 60 petaflops/s-days on top of 3,640 petaflops/s-days for the GPT-3 base. As such, we estimate the same additional proportion of compute for all models."
        },
        "microsoft/TNLGv2_530B": {
            "value" : 241.1,
            "description" : "For MT-NLG (530B), we assume the 280 8-A100 servers used for training were run in California based on press releases for an estimated 47 days; this results in 896 kW per hour of power draw, plus a PUE scaling of 1.1, yielding 1111.7568 MWh or 265.145597365 tCO2eq at the California average carbon intensity of 0.23849244489880175 kg Co2/kwh. The reduction of carbon figures are due to the lower carbon intensity in California, where the datacenter was located, and the efficiency improvements of A100."
        }
    }
}
  