# A minimal example where we compare a few models on a few benchmarks.

"mmlu:subject=philosophy,data_augmentation=canonical,model=openai/davinci": {status: "READY", priority: 1}
"mmlu:subject=philosophy,data_augmentation=canonical,model=openai/text-davinci-002": {status: "READY", priority: 1}

"mmlu:subject=anatomy,data_augmentation=canonical,model=openai/davinci": {status: "READY", priority: 1}
"mmlu:subject=anatomy,data_augmentation=canonical,model=openai/text-davinci-002": {status: "READY", priority: 1}

"boolq:data_augmentation=canonical,model=openai/davinci": {status: "READY", priority: 1}
"boolq:data_augmentation=canonical,model=openai/text-davinci-002": {status: "READY", priority: 1}

"bold:subject=all,model=openai/davinci": {status: "READY", priority: 1}
"bold:subject=all,model=openai/text-davinci-002": {status: "READY", priority: 1}
