# Main `RunSpec`s for the benchmarking.

entries: [
  {description: "boolq:model=openai/text-davinci-002,data_augmentation=canonical", priority: 1}
  {description: "natural_qa:model=openai/text-davinci-002,mode=closedbook,data_augmentation=canonical", priority: 1}
  {description: "mmlu:model=openai/text-davinci-002,subject=abstract_algebra,data_augmentation=canonical", priority: 2}
  {description: "imdb:model=openai/text-davinci-002,data_augmentation=canonical", priority: 1}

  ## Contrast sets (these are separate runs since we will only consider Instances that have contrast sets
  {description: "boolq:model=openai/text-davinci-002,only_contrast=True,data_augmentation=contrast_sets", priority: 2, groups: ["robustness_constrast_sets"]}
  {description: "imdb:model=openai/text-davinci-002,only_contrast=True,data_augmentation=contrast_sets", priority: 2, groups: ["robustness_contrast_sets"]}
  {description: "imdb:model=openai/text-davinci-002,max_train_instances=all,data_augmentation=canonical", priority: 1, groups: ["ablation_in_context"]}

  {description: "mmlu:model=openai/text-davinci-002,subject=abstract_algebra,method=multiple_choice_joint,data_augmentation=canonical", priority: 1, groups: ["ablation_multiple_choice"]}
  {description: "mmlu:model=openai/text-davinci-002,subject=abstract_algebra,method=multiple_choice_separate_original,data_augmentation=canonical", priority: 1, groups: ["ablation_multiple_choice"]}
  {description: "mmlu:model=openai/text-davinci-002,subject=abstract_algebra,method=multiple_choice_separate_calibrated,data_augmentation=canonical", priority: 1, groups: ["ablation_multiple_choice"]}

  {description: "imdb:model=openai/text-davinci-002,data_augmentation=robustness_all", priority: 1, groups: ["robustness_individual"]}

]
