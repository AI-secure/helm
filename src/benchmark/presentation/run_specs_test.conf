# List the RunSpecs that we want to display on the website in this file.
# A RunSpec specifies how to do a single run, which gets a scenario, adapts it, and computes a list of metrics.
#
# RunSpecs are unique. Assign them a priority where a RunSpec with a lower priority value gets run
# before a one with a higher priority value.
#
# Place your new RunSpec description under the appropriate section.

# TODO: Set num_train_trials=3 as the default


##### Generic #####

##### Question Answering #####
# Scenarios: BoolQ, NarrativeQA, NewsQA, QuAC
# Scenarios: NaturalQuestions
# Scenarios: CommonsenseQA, HellaSwag, OpenBookQA, TruthfulQA
# Scenarios: MMLU

## Reading comprehension

"boolq:model=text,data_augmentation=canonical,num_train_trials=default": {priority: 1}
"narrative_qa:model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"news_qa:model=text,data_augmentation=canonical,num_train_trials=default": {priority: 3}
"quac:model=text,data_augmentation=canonical,num_train_trials=default": {priority: 1}

## Reading comprehension and closedbook QA variants

"natural_qa:model=text,mode=openbook_longans,data_augmentation=canonical,num_train_trials=default": {priority: 1}
"natural_qa:model=text,mode=closedbook,data_augmentation=canonical,num_train_trials=default": {priority: 1}

## Closed-book QA with multiple choice

"commonsense:model=text,dataset=commonsenseqa,method=multiple_choice_separate_calibrated,data_augmentation=canonical,num_train_trials=default": {priority: 3}
# Adaptation method is set to ADAPT_MULTIPLE_CHOICE_SEPARATE_CALIBRATED and echo=True
"commonsense:model=full_functionality_text,dataset=hellaswag,method=multiple_choice_separate_original,data_augmentation=canonical,num_train_trials=default": {priority: 1}
"commonsense:model=full_functionality_text,dataset=openbookqa,method=multiple_choice_separate_calibrated,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"truthful_qa:model=text,task=mc_single,data_augmentation=canonical,num_train_trials=default": {priority: 1}

# For MMLU, we sampled the following 10 subjects, which cover diverse topics across humanities, social sciences and STEM.
"mmlu:model=text,subject=abstract_algebra,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"mmlu:model=text,subject=anatomy,data_augmentation=canonical,num_train_trials=default": {priority: 3}
"mmlu:model=text,subject=college_chemistry,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"mmlu:model=text,subject=computer_security,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"mmlu:model=text,subject=econometrics,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"mmlu:model=text,subject=global_facts,data_augmentation=canonical,num_train_trials=default": {priority: 3}
"mmlu:model=text,subject=jurisprudence,data_augmentation=canonical,num_train_trials=default": {priority: 3}
"mmlu:model=text,subject=philosophy,data_augmentation=canonical,num_train_trials=default": {priority: 3}
"mmlu:model=text,subject=professional_medicine,data_augmentation=canonical,num_train_trials=default": {priority: 3}
"mmlu:model=text,subject=us_foreign_policy,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"mmlu:model=text,subject=astronomy,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=business_ethics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=clinical_knowledge,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=college_biology,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=college_computer_science,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=college_mathematics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=college_medicine,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=college_physics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=conceptual_physics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=electrical_engineering,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=elementary_mathematics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=formal_logic,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_biology,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_chemistry,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_computer_science,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_european_history,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_geography,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_government_and_politics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_macroeconomics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_mathematics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_microeconomics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_physics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_psychology,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_statistics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_us_history,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=high_school_world_history,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=human_aging,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=human_sexuality,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=international_law,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=logical_fallacies,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=machine_learning,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=management,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=marketing,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=medical_genetics,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=miscellaneous,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=moral_disputes,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=moral_scenarios,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=nutrition,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=prehistory,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=professional_accounting,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=professional_law,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=professional_psychology,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=public_relations,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=security_studies,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=sociology,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=virology,data_augmentation=canonical,num_train_trials=default": {priority: 4}
"mmlu:model=text,subject=world_religions,data_augmentation=canonical,num_train_trials=default": {priority: 4}


##### Information Retrieval #####
# Scenarios: MS Marco (Regular), MS MARCO (TREC)

##### Sentiment Analysis #####
# Scenarios: IMDB

"imdb:model=text,data_augmentation=canonical,num_train_trials=default": {priority: 1}


##### (Miscellaneous) Text Classification #####
# Scenarios: RAFT

"raft:subset=ade_corpus_v2,model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"raft:subset=banking_77,model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"raft:subset=neurips_impact_statement_risks,model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"raft:subset=one_stop_english,model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"raft:subset=overruling,model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"raft:subset=semiconductor_org_types,model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"raft:subset=tweet_eval_hate,model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"raft:subset=twitter_complaints,model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"raft:subset=systematic_review_inclusion,model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"raft:subset=tai_safety_research,model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"raft:subset=terms_of_service,model=text,data_augmentation=canonical,num_train_trials=default": {priority: 2}


##### Toxicity Detection #####
# Scenarios: CivilComments

"civil_comments:model=text,demographic=all,data_augmentation=canonical,num_train_trials=default": {priority: 1}
"civil_comments:model=text,demographic=male,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"civil_comments:model=text,demographic=female,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"civil_comments:model=text,demographic=LGBTQ,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"civil_comments:model=text,demographic=christian,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"civil_comments:model=text,demographic=muslim,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"civil_comments:model=text,demographic=other_religions,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"civil_comments:model=text,demographic=black,data_augmentation=canonical,num_train_trials=default": {priority: 2}
"civil_comments:model=text,demographic=white,data_augmentation=canonical,num_train_trials=default": {priority: 2}
