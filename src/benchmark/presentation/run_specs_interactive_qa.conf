# Subjects used for InteractiveQA. Run:
# benchmark-present --local --priority 1 --suite interactive_qa_mmlu  --num-threads 1 --num-train-trials 3
# --conf-path src/benchmark/presentation/run_specs_interactive_qa.conf --max-eval-instances 10

entries: [
  {description: "mmlu:model=openai/text-davinci-001,subject=college_chemistry,for_interactive_qa=True", priority: 1}
  {description: "mmlu:model=openai/text-davinci-001,subject=global_facts,for_interactive_qa=True", priority: 1}
  {description: "mmlu:model=openai/text-davinci-001,subject=miscellaneous,for_interactive_qa=True", priority: 1}
  {description: "mmlu:model=openai/text-davinci-001,subject=nutrition,for_interactive_qa=True", priority: 1}
  {description: "mmlu:model=openai/text-davinci-001,subject=us_foreign_policy,for_interactive_qa=True", priority: 1}
]