# List the RunSpecs that we want to display on the website in this file.
# A RunSpec specifies how to do a single run, which gets a scenario, adapts it, and computes a list of metrics.
#
# RunSpecs are unique. Mark the RunSpec with either READY or WIP (work in progress):
#    For READY RunSpecs, we evaluate and generate metrics.
#    For WIP RunSpecs, we just estimate token usage.
#
# Place your new RunSpec description under the appropriate section.
#
# Note: `num_outputs` has to be 1 for the Anthropic and MT-NLG models

##### Generic #####

##### Question Answering #####
# Scenarios:

"boolq:model=default_and_limited": {status: "READY"}
"boolq:model=default_and_limited,data_augmentation=robustness": {status: "READY"}
"boolq:model=default_and_limited,data_augmentation=fairness": {status: "READY"}

"narrative_qa:model=default_and_limited": {status: "READY"}
"narrative_qa:model=default_and_limited,data_augmentation=robustness": {status: "READY"}
"narrative_qa:model=default_and_limited,data_augmentation=fairness": {status: "READY"}

"natural_qa:model=default_and_limited,mode=closedbook": {status: "READY"}
"natural_qa:model=default_and_limited,mode=openbook-longans": {status: "READY"}
# TODO: Resolve how we are doing NaturalQA augmentations, i.e. do we do for both closedbook and longans?

"news_qa:model=default": {status: "READY"}
"news_qa:model=default,data_augmentation=robustness": {status: "READY"}
"news_qa:model=default,data_augmentation=fairness": {status: "READY"}

"quac:model=default_and_limited": {status: "READY"}
"quac:model=default_and_limited,data_augmentation=robustness": {status: "READY"}
"quac:model=default_and_limited,data_augmentation=fairness": {status: "READY"}

"commonsense_qa:model=default_and_limited,dataset=hellaswag,method=mcqa": {status: "READY"}
"commonsense_qa:model=default_and_limited,dataset=openbookqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=default_and_limited,dataset=commonsenseqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=default_and_limited,dataset=piqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=default_and_limited,dataset=siqa,method=mcqa": {status: "READY"}
# TODO: Resolve how we are doing CommonsenseQA, i.e. do we do for all 5?

# For MMLU, we sampled the following 10 subjects, which cover diverse topics across humanities, social sciences and STEM.
"mmlu:model=default_and_limited,subject=abstract_algebra": {status: "READY"}
"mmlu:model=default_and_limited,subject=anatomy": {status: "READY"}
"mmlu:model=default_and_limited,subject=college_chemistry": {status: "READY"}
"mmlu:model=default_and_limited,subject=computer_security": {status: "READY"}
"mmlu:model=default_and_limited,subject=econometrics": {status: "READY"}
"mmlu:model=default_and_limited,subject=global_facts": {status: "READY"}
"mmlu:model=default_and_limited,subject=jurisprudence": {status: "READY"}
"mmlu:model=default_and_limited,subject=professional_medicine": {status: "READY"}  # It will take about ~335,221 tokens to evaluate.
"mmlu:model=default_and_limited,subject=philosophy": {status: "READY"}
"mmlu:model=default_and_limited,subject=us_foreign_policy": {status: "READY"}
# TODO: Resolve how we are doing CommonsenseQA, i.e. do we do for all 10 subsets?
"mmlu:model=default_and_limited,subject=us_foreign_policy": {status: "READY"}
"mmlu:model=default_and_limited,subject=us_foreign_policy,data_augmentation=robustness": {status: "READY"}
"mmlu:model=default_and_limited,subject=us_foreign_policy,data_augmentation=fairness": {status: "READY"}


##### Information Retrieval #####
# Scenarios: MS Marco, TREC

# TODO: Resolve what the final run is, which will likely influenced by token quota
# Conf file is a mess, so skipping for the moment

##### Summarization #####
# Scenarios: XSUM, CNN/DM

"summarization_cnndm:model=default_and_limited,temperature=0.3": {status: "READY"}
"summarization_xsum:model=default_and_limited,temperature=0.3": {status: "READY"}
"summarization_xsum_sampled:model=default_and_limited,temperature=0.3": {status: "READY"}

##### Text Classification #####
# Scenarios: IMDB, RAFT, CivilComments

"imdb:model=default_and_limited": {status: "READY"}
"imdb:model=default_and_limited,data_augmentation=robustness": {status: "READY"}
"imdb:model=default_and_limited,data_augmentation=fairness": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models, but for raft it's 5
"raft:model=default,subset=ade_corpus_v2": {status: "READY"}
"raft:model=default,subset=banking_77": {status: "READY"}
"raft:model=default,subset=neurips_impact_statement_risks": {status: "READY"}
"raft:model=default,subset=one_stop_english": {status: "READY"}
"raft:model=default,subset=overruling": {status: "READY"}
"raft:model=default,subset=semiconductor_org_types": {status: "READY"}
"raft:model=default,subset=tweet_eval_hate": {status: "READY"}
"raft:model=default,subset=twitter_complaints": {status: "READY"}
# TODO: Resolve how we do RAFT for augmentations
"raft:model=default": {status: "READY"}
"raft:model=default,data_augmentation=robustness": {status: "READY"}
"raft:model=default,data_augmentation=fairness": {status: "READY"}

"civil_comments:model=default_and_limited,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=all": {status: "READY"}
"civil_comments:model=default_and_limited,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=all,data_augmentation=fairness": {status: "READY"}
"civil_comments:model=default_and_limited,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=all,data_augmentation=robustness": {status: "READY"}