# List the RunSpecs that we want to display on the website in this file.
# A RunSpec specifies how to do a single run, which gets a scenario, adapts it, and computes a list of metrics.
#
# RunSpecs are unique. Mark the RunSpec with either READY or WIP (work in progress):
#    For READY RunSpecs, we evaluate and generate metrics.
#    For WIP RunSpecs, we just estimate token usage.
# Place the RunSpecs alphabetically within the sections.

##### Generic #####
"boolq:model=default": {status: "READY"}
# TODO: run on anthropic once the following issue is resolved
#       https://github.com/stanford-crfm/benchmarking/issues/283
# "boolq:model=limited_functionality": {status: "READY"}

"imdb:model=default": {status: "READY"}

"msmarco:model=default,task=passage,topk=20,num_eval_queries=10": {status: "READY"}

"narrative_qa:model=default": {status: "READY"}

# TODO: @Shibani @Dimitris - Look at results and decide what we want.
"natural_qa:model=default,mode=closedbook": {status: "READY"}
# TODO: look into why the following failed for GPT-2:
#       https://github.com/stanford-crfm/benchmarking/issues/290
"natural_qa:mode=openbook-longans": {status: "READY"}
"natural_qa:mode=openbook-wiki": {status: "READY"}

"news_qa:model=default": {status: "READY"}
"quac:model=default": {status: "READY"}

# TODO: @Bobby @Dimitris - Look at results and decide what we want.
"raft:model=default,subset=ade_corpus_v2": {status: "READY"}
"raft:model=default,subset=banking_77": {status: "READY"}
"raft:model=default,subset=neurips_impact_statement_risks": {status: "READY"}
"raft:model=default,subset=one_stop_english": {status: "READY"}
"raft:model=default,subset=overruling": {status: "READY"}
"raft:model=default,subset=semiconductor_org_types": {status: "READY"}
"raft:model=default,subset=systematic_review_inclusion": {status: "READY"}
"raft:model=default,subset=tai_safety_research": {status: "READY"}
"raft:model=default,subset=terms_of_service": {status: "READY"}
"raft:model=default,subset=tweet_eval_hate": {status: "READY"}
"raft:model=default,subset=twitter_complaints": {status: "READY"}

# TODO: @Faisal - Raises an error in loading data in line 76 of summarization_scenario.py.
# NotADirectoryError: The following is not a directory: 
# /.cache/huggingface/datasets/downloads/extracted/f3a2a22fcda5c84f59f855b9f32ac518c7eabcf5236bdcbf90d2cacd51bcc743/cnn/stories'
# "summarization_cnndm": {status: "READY"}

"summarization_xsum:model=default": {status: "READY"}

# TODO: @Dilara - Add TREC.


##### Language #####
# TODO: Support AI21 tokenizer for language modeling:
#       https://github.com/stanford-crfm/benchmarking/issues/189

# TODO: @Yian @Rishi - Look at results and decide what we want.
"blimp:phenomenon=island_effects": {status: "READY"}
"blimp:phenomenon=anaphor_agreement": {status: "READY"}
"blimp:phenomenon=argument_structure": {status: "READY"}
"blimp:phenomenon=determiner_noun_agreement": {status: "READY"}
"blimp:phenomenon=subject_verb_agreement": {status: "READY"}
"blimp:phenomenon=ellipsis": {status: "READY"}
"blimp:phenomenon=control_raising": {status: "READY"}
"blimp:phenomenon=quantifiers": {status: "READY"}
"blimp:phenomenon=irregular_forms": {status: "READY"}
"blimp:phenomenon=npi_licensing": {status: "READY"}
"blimp:phenomenon=binding": {status: "READY"}
"blimp:phenomenon=filler_gap_dependency": {status: "READY"}

# TODO: @Yian @Rishi - Decide what subsets we want of The Pile.
"the_pile:subset=OpenSubtitles": {status: "READY"}

# TODO: @Yian - Update with improved info once we get from Brendan et al.
"twitter_aae:demographic=aa": {status: "READY"}
"twitter_aae:demographic=white": {status: "READY"}

# TODO: @Nathan @Rishi @Percy - Decide how to handle intersectional groups for ICE.
# TODO: @Nathan @Rishi @Percy - Decide on which regional subsets to include for ICE.
# "ice:subset=CAN": {status: "READY"}
# TODO: @Nathan - Add East Africa.
# "ice:subset=HK": {status: "READY"}
# "ice:subset=IND": {status: "READY"}
# "ice:subset=JA": {status: "READY"}
# "ice:subset=PHI": {status: "READY"}
# "ice:subset=SIN": {status: "READY"}
# "ice:subset=USA": {status: "READY"}

# "ice:split=spoken": {status: "READY"}
# "ice:split=written": {status: "READY"}

# TODO: @Nathan - Fix issue with gender on cluster; file extensions break.
#       https://github.com/stanford-crfm/benchmarking/issues/277
# "ice:gender=F": {status: "READY"}
# "ice:gender=M": {status: "READY"}

# "wikitext_103": {status: "READY"}


##### Knowledge #####

"commonsense_qa:model=default,dataset=hellaswag,method=mcqa": {status: "READY"}
"commonsense_qa:model=default,dataset=openbookqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=default,dataset=commonsenseqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=default,dataset=piqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=default,dataset=siqa,method=mcqa": {status: "READY"}

# TODO: @Michi @Percy @Dimitris @Rishi - Decide if we are doing clm strategy in main benchmark.
# Note: We can always include clm as an ablation.
# "commonsense_qa:dataset=hellaswag,method=clm": {status: "READY"}
# "commonsense_qa:dataset=openbookqa,method=clm": {status: "READY"}
# "commonsense_qa:dataset=commonsenseqa,method=clm": {status: "READY"}
# TODO: Currently produces an error - see issue 223
# "commonsense_qa:dataset=piqa,method=clm": {status: "READY"}
# "commonsense_qa:dataset=siqa,method=clm": {status: "READY"}

# TODO: @Michi - Decide what subjects we want of MMLU.
# TODO: @Michi - Improve docstring for MMLU.
"mmlu:model=default,subject=abstract_algebra": {status: "READY"}
"mmlu:model=default,subject=anatomy": {status: "READY"}
"mmlu:model=default,subject=college_chemistry": {status: "READY"}
"mmlu:model=default,subject=computer_security": {status: "READY"}
"mmlu:model=default,subject=econometrics": {status: "READY"}
"mmlu:model=default,subject=global_facts": {status: "READY"}
"mmlu:model=default,subject=jurisprudence": {status: "READY"}
"mmlu:model=default,subject=medical_genetics": {status: "READY"}
"mmlu:model=default,subject=professional_medicine": {status: "WIP"}  # It will take about ~335,221 tokens to evaluate.
"mmlu:model=default,subject=philosophy": {status: "READY"}
"mmlu:model=default,subject=us_foreign_policy": {status: "READY"}

# TODO: @Neel @Hongyu @Michi - Decide what subjects we want of WikiFact
# TODO: @Neel @Hongyu @Michi - Make the scenario name more specific; "wiki" is very vague (and we have wikitext)
"wiki:model=default,k=5,subject=P31": {status: "READY"}


##### Reasoning #####

# TODO: @Tony W. - None of HumanEval, APPS, LSAT, Dyck-n, number relationship are included.
# TODO: @Tony W. - Please add whichever ones should be run at this stage.

# TODO: @Frieda @Tony W. - Decide on exact subject/difficulty levels for MATH based on results.
# TODO: @Frieda @Tony W. - Do we ever vary "use_official_prompt" for default evaluations?
# TODO: @Frieda @Tony W. - Or will we vary "use_official_prompt" for some ablation/analysis?
# TODO: run on all default models once the following issue is resolved:
#       https://github.com/stanford-crfm/benchmarking/issues/285
"math:subject=number_theory,level=1": {status: "READY"}
"math:subject=intermediate_algebra,level=1": {status: "READY"}
"math:subject=algebra,level=1": {status: "READY"}
"math:subject=prealgebra,level=1": {status: "READY"}
"math:subject=geometry,level=1": {status: "READY"}
"math:subject=counting_and_probability,level=1": {status: "READY"}
"math:subject=precalculus,level=1": {status: "READY"}

# "math:subject=number_theory,level=2": {status: "READY"}
# "math:subject=intermediate_algebra,level=2": {status: "READY"}
# "math:subject=algebra,level=2": {status: "READY"}
# "math:subject=prealgebra,level=2": {status: "READY"}
# "math:subject=geometry,level=2": {status: "READY"}
# "math:subject=counting_and_probability,level=2": {status: "READY"}
# "math:subject=precalculus,level=2": {status: "READY"}
# 
# "math:subject=number_theory,level=3": {status: "READY"}
# "math:subject=intermediate_algebra,level=3": {status: "READY"}
# "math:subject=algebra,level=3": {status: "READY"}
# "math:subject=prealgebra,level=3": {status: "READY"}
# "math:subject=geometry,level=3": {status: "READY"}
# "math:subject=counting_and_probability,level=3": {status: "READY"}
# "math:subject=precalculus,level=3": {status: "READY"}
# 
# "math:subject=number_theory,level=4": {status: "READY"}
# "math:subject=intermediate_algebra,level=4": {status: "READY"}
# "math:subject=algebra,level=4": {status: "READY"}
# "math:subject=prealgebra,level=4": {status: "READY"}
# "math:subject=geometry,level=4": {status: "READY"}
# "math:subject=counting_and_probability,level=4": {status: "READY"}
# "math:subject=precalculus,level=4": {status: "READY"}

"math:subject=number_theory,level=5": {status: "READY"}
"math:subject=intermediate_algebra,level=5": {status: "READY"}
"math:subject=algebra,level=5": {status: "READY"}
"math:subject=prealgebra,level=5": {status: "READY"}
"math:subject=geometry,level=5": {status: "READY"}
"math:subject=counting_and_probability,level=5": {status: "READY"}
"math:subject=precalculus,level=5": {status: "READY"}

"numeracy:relation_type=linear,mode=function": {status: "READY"}
"numeracy:relation_type=plane,mode=function": {status: "READY"}
"numeracy:relation_type=parabola,mode=function": {status: "READY"}
"numeracy:relation_type=paraboloid,mode=function": {status: "READY"}

"synthetic_reasoning:model=default,mode=pattern_match": {status: "READY"}
"synthetic_reasoning:model=default,mode=variable_substitution": {status: "READY"}
"synthetic_reasoning:model=default,mode=induction": {status: "READY"}

"synthetic_reasoning_natural:model=default,difficulty=easy": {status: "READY"}
"synthetic_reasoning_natural:model=default,difficulty=hard": {status: "READY"}

"gsm:model=default": {status: "READY"}

# TODO: @Dor @Tony W. @Percy - Decide on BABI tasks. 
"babi_qa:model=default,task=1": {status: "READY"}
"babi_qa:model=default,task=2": {status: "READY"}
"babi_qa:model=default,task=3": {status: "READY"}
"babi_qa:model=default,task=4": {status: "READY"}
"babi_qa:model=default,task=5": {status: "READY"}
"babi_qa:model=default,task=6": {status: "READY"}
"babi_qa:model=default,task=7": {status: "READY"}
"babi_qa:model=default,task=8": {status: "READY"}
"babi_qa:model=default,task=9": {status: "READY"}
"babi_qa:model=default,task=10": {status: "READY"}
"babi_qa:model=default,task=11": {status: "READY"}
"babi_qa:model=default,task=12": {status: "READY"}
"babi_qa:model=default,task=13": {status: "READY"}
"babi_qa:model=default,task=14": {status: "READY"}
"babi_qa:model=default,task=15": {status: "READY"}
"babi_qa:model=default,task=16": {status: "READY"}
"babi_qa:model=default,task=17": {status: "READY"}
"babi_qa:model=default,task=18": {status: "READY"}
"babi_qa:model=default,task=19": {status: "READY"}
"babi_qa:model=default,task=20": {status: "READY"}

"legal_support:model=default": {status: "READY"}


"code:model=code,dataset=HumanEval": {status: "READY"}
"code:model=code,dataset=APPS": {status: "READY"}

"lsat_qa:model=default,task=all": {status: "READY"}
"lsat_qa:model=default,task=grouping": {status: "READY"}
"lsat_qa:model=default,task=ordering": {status: "READY"}
"lsat_qa:model=default,task=assignment": {status: "READY"}
"lsat_qa:model=default,task=miscellaneous": {status: "READY"}

"dyck_language:model=default,num_parenthesis_pairs=2": {status: "READY"}
"dyck_language:model=default,num_parenthesis_pairs=3": {status: "READY"}
"dyck_language:model=default,num_parenthesis_pairs=4": {status: "READY"}


##### Robustness #####
# "boolq:data_augmentation=all": {status: "READY"}
# "natural_qa:mode=closedbook,data_augmentation=all": {status: "READY"}
# "news_qa:data_augmentation=all": {status: "READY"}
# "raft:subset=one_stop_english,data_augmentation=all": {status: "READY"}

##### Harms #####

"copyright:model=default,pilot_study=true": {status: "READY"}
# TODO: @Chen - Can the non-pilot be run/why is this WIP?
# "copyright:pilot_study=false": {status: "WIP"}

"disinformation:model=default,capability=reiteration": {status: "READY"}
"disinformation:model=default,capability=wedging": {status: "READY"}

# TODO: @Ryan - Add BBQ, BOLD, and CivilComments.
# TODO: @Ryan @Rishi - Add the above with correct arguments specified.

# TODO: run on all default models once the following issue is resolved:
#       https://github.com/stanford-crfm/benchmarking/issues/286
"real_toxicity_prompts": {status: "READY"}

"truthful_qa:model=default,task=mc_single": {status: "READY"}
# TODO: @Dilara - Implement generative version based on classification results.


##### Interaction #####

