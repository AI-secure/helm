# List the RunSpecs that we want to display on the website in this file.
# A RunSpec specifies how to do a single run, which gets a scenario, adapts it, and computes a list of metrics.
#
# RunSpecs are unique. Mark the RunSpec with either READY or WIP (work in progress):
#    For READY RunSpecs, we evaluate and generate metrics.
#    For WIP RunSpecs, we just estimate token usage.
# Place the RunSpecs alphabetically within the sections.

##### Generic #####

"boolq": {status: "READY"}
"boolq_contrast_sets": {status: "READY"}
# IMDB raises following error with --dry-run with davinci
# Token indices sequence length is longer than the specified maximum sequence length 
# for this model (1378 > 1024). Running this sequence through the model will result in indexing errors
"imdb": {status: "READY"}
"imdb_contrast_sets": {status: "READY"}
# Look at data for Narrative QA - one of the questions appears in all caps.
# See: WHO NORMALLY DELIVERS THE OPENING PROLOGUE IN THE PLAY?
"narrativeqa": {status: "READY"}
"natural_qa:mode=closedbook": {status: "READY"}
# News QA will break since no data; @niladri
# "news_qa": {status: "READY"}
"quac": {status: "READY"}
# There is no space after the colon for "Label"
"raft:subset=ade_corpus_v2": {status: "READY"}
"raft:subset=banking_77": {status: "READY"}
"raft:subset=neurips_impact_statement_risks": {status: "READY"}
"raft:subset=one_stop_english": {status: "READY"}
"raft:subset=overruling": {status: "READY"}
"raft:subset=semiconductor_org_types": {status: "READY"}
"raft:subset=systematic_review_inclusion": {status: "READY"}
"raft:subset=tai_safety_research": {status: "READY"}
"raft:subset=terms_of_service": {status: "READY"}
"raft:subset=tweet_eval_hate": {status: "READY"}
"raft:subset=twitter_complaints": {status: "READY"}
# TODO: Raises an error in loading data in line 76 of summarization_scenario.py
# NotADirectoryError: The following is not a directory: 
# /.cache/huggingface/datasets/downloads/extracted/f3a2a22fcda5c84f59f855b9f32ac518c7eabcf5236bdcbf90d2cacd51bcc743/cnn/stories'
# "summarization_cnndm": {status: "READY"}
"summarization_xsum": {status: "READY"}


##### Harms #####

# "copyright:pilot_study=true": {status: "READY"}
# TODO: set this to READY when increase the OpenAI rate limit:
#       https://github.com/stanford-crfm/benchmarking/issues/97
# TODO: Seems like there are 20k tokens used per instance; is that right/how is that possible?
# "copyright:pilot_study=false": {status: "WIP"}
# TODO: Did not test yet since big file download - rishi
# "real_toxicity_prompts": {status: "READY"}
# "truthful_qa:task=mc_single": {status: "READY"}


##### Interaction #####


##### Language #####

# BLiMP scenarios will break if no eval instances (e.g. --skip-instances)
# See Issue #207
"blimp:phenomenon=island_effects": {status: "READY"} 
"blimp:phenomenon=anaphor_agreement": {status: "READY"}
"blimp:phenomenon=argument_structure": {status: "READY"}
"blimp:phenomenon=determiner_noun_agreement": {status: "READY"}
"blimp:phenomenon=subject_verb_agreement": {status: "READY"}
"blimp:phenomenon=ellipsis": {status: "READY"}
"blimp:phenomenon=control_raising": {status: "READY"}
"blimp:phenomenon=quantifiers": {status: "READY"}
"blimp:phenomenon=irregular_forms": {status: "READY"}
"blimp:phenomenon=npi_licensing": {status: "READY"}
"blimp:phenomenon=binding": {status: "READY"}
"blimp:phenomenon=filler_gap_dependency": {status: "READY"}
"the_pile:subset=OpenSubtitles": {status: "READY"}
"twitter_aae:demographic=aa": {status: "READY"}
"wikitext_103": {status: "READY"}


##### Knowledge #####

"commonsense_qa:dataset=hellaswag,method=mcqa": {status: "READY"}
"commonsense_qa:dataset=hellaswag,method=clm": {status: "READY"}
"commonsense_qa:dataset=openbookqa,method=mcqa": {status: "READY"}
"commonsense_qa:dataset=openbookqa,method=clm": {status: "READY"}
"commonsense_qa:dataset=commonsenseqa,method=mcqa": {status: "READY"}
"commonsense_qa:dataset=commonsenseqa,method=clm": {status: "READY"}
"commonsense_qa:dataset=piqa,method=mcqa": {status: "READY"}
"commonsense_qa:dataset=piqa,method=clm": {status: "READY"}
"commonsense_qa:dataset=siqa,method=mcqa": {status: "READY"}
"commonsense_qa:dataset=siqa,method=clm": {status: "READY"}

# TODO: should we include all of the 57 subjects available in MMLU?
"mmlu:subject=abstract_algebra": {status: "READY"}
"mmlu:subject=anatomy": {status: "READY"}
"mmlu:subject=college_chemistry": {status: "READY"}
"mmlu:subject=computer_security": {status: "READY"}
"mmlu:subject=econometrics": {status: "READY"}
"mmlu:subject=global_facts": {status: "READY"}
"mmlu:subject=jurisprudence": {status: "READY"}
"mmlu:subject=medical_genetics": {status: "READY"}
"mmlu:subject=professional_medicine": {status: "WIP"}  # It will take about ~335,221 tokens to evaluate.
"mmlu:subject=philosophy": {status: "READY"}
"mmlu:subject=us_foreign_policy": {status: "READY"}

# TODO: are we evaluating on all subjects?
"wiki:k=5,subject=P31": {status: "READY"}

##### Reasoning #####

"synthetic_reasoning:mode=pattern_match": {status: "READY"}
"synthetic_reasoning:mode=variable_substitution": {status: "READY"}
"synthetic_reasoning:mode=induction": {status: "READY"}

"synthetic_reasoning_natural:difficulty=easy": {status: "READY"}
"synthetic_reasoning_natural:difficulty=hard": {status: "READY"}

"babi_qa:task=1": {status: "READY"}
"babi_qa:task=2": {status: "READY"}
"babi_qa:task=3": {status: "READY"}
"babi_qa:task=4": {status: "READY"}
"babi_qa:task=5": {status: "READY"}
"babi_qa:task=6": {status: "READY"}
"babi_qa:task=7": {status: "READY"}
"babi_qa:task=8": {status: "READY"}
"babi_qa:task=9": {status: "READY"}
"babi_qa:task=10": {status: "READY"}
"babi_qa:task=11": {status: "READY"}
"babi_qa:task=12": {status: "READY"}
"babi_qa:task=13": {status: "READY"}
"babi_qa:task=14": {status: "READY"}
"babi_qa:task=15": {status: "READY"}
"babi_qa:task=16": {status: "READY"}
"babi_qa:task=17": {status: "READY"}
"babi_qa:task=18": {status: "READY"}
"babi_qa:task=19": {status: "READY"}
"babi_qa:task=20": {status: "READY"}
