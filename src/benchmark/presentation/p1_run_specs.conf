# List the RunSpecs that we want to display on the website in this file.
# A RunSpec specifies how to do a single run, which gets a scenario, adapts it, and computes a list of metrics.
#
# RunSpecs are unique. Mark the RunSpec with either READY or WIP (work in progress):
#    For READY RunSpecs, we evaluate and generate metrics.
#    For WIP RunSpecs, we just estimate token usage.
# Place the RunSpecs alphabetically within the sections.

##### Base Scenarios #####

# "imdb": {status: "READY"}


##### Fix GPT-3 davinci #####

# Fails on --skip-instances at logging
# TODO: Produces error when trying to log (line 101 of present.py)
# No such file or directory: 'benchmark_output/runs/imdb:model=default/run_spec.json'
# "imdb:model=default": {status: "READY"}


##### Sweep over all models #####

# Fails on --skip_instances before logging
# TODO: Produces error trying to load GPT-J (line 32 of tokenizer_factory.py)
# Unsupported model: huggingface/gptj_6b
# "imdb:model=all": {status: "READY"}


##### Sweep over number of in-context examples #####

# Works as intended in --dry-run, but will fail for logging reason mentioned above.
# "imdb:max_train_instances=all": {status: "READY"}


##### Run for 5 trials with different in-context examples #####

# Works as intended in --dry-run, but will fail for logging reason mentioned above.
# "imdb:num_train_trials=5": {status: "READY"}


##### Robustness to misspellings #####

# Works as intended in --dry-run, but will fail for logging reason mentioned above.
"imdb:data_augmentation=misspelling": {status: "READY"}

