from typing import List
from unittest import mock

from common.authentication import Authentication
from common.tokenization_request import TokenizationRequestResult, TokenizationToken, TextRange
from proxy.remote_service import RemoteService
from benchmark.adapter_service import AdapterService
from .tokenizer_factory import TokenizerFactory

# We use mocking for tokenization calls so no valid api_keys are required.
api_key = ""
auth = Authentication(api_key=api_key)
service = AdapterService(RemoteService("https://crfm-models.stanford.edu"), auth)

TEST_PROMPT: str = (
    "The Center for Research on Foundation Models (CRFM) is "
    "an interdisciplinary initiative born out of the Stanford "
    "Institute for Human-Centered Artificial Intelligence (HAI) "
    "that aims to make fundamental advances in the study, development, "
    "and deployment of foundation models."
)

TEST_TOKEN_REPRESENTATIONS: List[TokenizationToken] = [
    TokenizationToken(text="▁The▁Center▁for", text_range=TextRange(start=0, end=14)),
    TokenizationToken(text="▁Research▁on", text_range=TextRange(start=14, end=26)),
    TokenizationToken(text="▁Foundation", text_range=TextRange(start=26, end=37)),
    TokenizationToken(text="▁Models", text_range=TextRange(start=37, end=44)),
    TokenizationToken(text="▁", text_range=TextRange(start=44, end=45)),
    TokenizationToken(text="(", text_range=TextRange(start=45, end=46)),
    TokenizationToken(text="CRF", text_range=TextRange(start=46, end=49)),
    TokenizationToken(text="M", text_range=TextRange(start=49, end=50)),
    TokenizationToken(text=")", text_range=TextRange(start=50, end=51)),
    TokenizationToken(text="▁is", text_range=TextRange(start=51, end=54)),
    TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=54, end=75)),
    TokenizationToken(text="▁initiative", text_range=TextRange(start=75, end=86)),
    TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=86, end=98)),
    TokenizationToken(text="▁the", text_range=TextRange(start=98, end=102)),
    TokenizationToken(text="▁Stanford", text_range=TextRange(start=102, end=111)),
    TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=111, end=125)),
    TokenizationToken(text="▁Human", text_range=TextRange(start=125, end=131)),
    TokenizationToken(text="-Centered", text_range=TextRange(start=131, end=140)),
    TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=140, end=164)),
    TokenizationToken(text="▁", text_range=TextRange(start=164, end=165)),
    TokenizationToken(text="(", text_range=TextRange(start=165, end=166)),
    TokenizationToken(text="HAI", text_range=TextRange(start=166, end=169)),
    TokenizationToken(text=")", text_range=TextRange(start=169, end=170)),
    TokenizationToken(text="▁that", text_range=TextRange(start=170, end=175)),
    TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=175, end=188)),
    TokenizationToken(text="▁fundamental", text_range=TextRange(start=188, end=200)),
    TokenizationToken(text="▁advances▁in", text_range=TextRange(start=200, end=212)),
    TokenizationToken(text="▁the▁study", text_range=TextRange(start=212, end=222)),
    TokenizationToken(text=",", text_range=TextRange(start=222, end=223)),
    TokenizationToken(text="▁development", text_range=TextRange(start=223, end=235)),
    TokenizationToken(text=",", text_range=TextRange(start=235, end=236)),
    TokenizationToken(text="▁and", text_range=TextRange(start=236, end=240)),
    TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=240, end=254)),
    TokenizationToken(text="▁foundation", text_range=TextRange(start=254, end=265)),
    TokenizationToken(text="▁models", text_range=TextRange(start=265, end=272)),
    TokenizationToken(text=".", text_range=TextRange(start=272, end=273)),
]

TEST_TOKENS: List[str] = [
    "▁The▁Center▁for",
    "▁Research▁on",
    "▁Foundation",
    "▁Models",
    "▁",
    "(",
    "CRF",
    "M",
    ")",
    "▁is",
    "▁an▁interdisciplinary",
    "▁initiative",
    "▁born▁out▁of",
    "▁the",
    "▁Stanford",
    "▁Institute▁for",
    "▁Human",
    "-Centered",
    "▁Artificial▁Intelligence",
    "▁",
    "(",
    "HAI",
    ")",
    "▁that",
    "▁aims▁to▁make",
    "▁fundamental",
    "▁advances▁in",
    "▁the▁study",
    ",",
    "▁development",
    ",",
    "▁and",
    "▁deployment▁of",
    "▁foundation",
    "▁models",
    ".",
]

REQUEST_RESULT = TokenizationRequestResult(
    cached=False,
    text="The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.",
    tokens=[
        TokenizationToken(text="▁The▁Center▁for", text_range=TextRange(start=0, end=14)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=14, end=26)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=26, end=37)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=37, end=44)),
        TokenizationToken(text="▁", text_range=TextRange(start=44, end=45)),
        TokenizationToken(text="(", text_range=TextRange(start=45, end=46)),
        TokenizationToken(text="CRF", text_range=TextRange(start=46, end=49)),
        TokenizationToken(text="M", text_range=TextRange(start=49, end=50)),
        TokenizationToken(text=")", text_range=TextRange(start=50, end=51)),
        TokenizationToken(text="▁is", text_range=TextRange(start=51, end=54)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=54, end=75)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=75, end=86)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=86, end=98)),
        TokenizationToken(text="▁the", text_range=TextRange(start=98, end=102)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=102, end=111)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=111, end=125)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=125, end=131)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=131, end=140)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=140, end=164)),
        TokenizationToken(text="▁", text_range=TextRange(start=164, end=165)),
        TokenizationToken(text="(", text_range=TextRange(start=165, end=166)),
        TokenizationToken(text="HAI", text_range=TextRange(start=166, end=169)),
        TokenizationToken(text=")", text_range=TextRange(start=169, end=170)),
        TokenizationToken(text="▁that", text_range=TextRange(start=170, end=175)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=175, end=188)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=188, end=200)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=200, end=212)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=212, end=222)),
        TokenizationToken(text=",", text_range=TextRange(start=222, end=223)),
        TokenizationToken(text="▁development", text_range=TextRange(start=223, end=235)),
        TokenizationToken(text=",", text_range=TextRange(start=235, end=236)),
        TokenizationToken(text="▁and", text_range=TextRange(start=236, end=240)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=240, end=254)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=254, end=265)),
        TokenizationToken(text="▁models", text_range=TextRange(start=265, end=272)),
        TokenizationToken(text=".", text_range=TextRange(start=272, end=273)),
    ],
)

LONG_REQUEST_RESULT = TokenizationRequestResult(
    cached=False,
    text="The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.",
    tokens=[
        TokenizationToken(text="▁The▁Center▁for", text_range=TextRange(start=0, end=14)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=14, end=26)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=26, end=37)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=37, end=44)),
        TokenizationToken(text="▁", text_range=TextRange(start=44, end=45)),
        TokenizationToken(text="(", text_range=TextRange(start=45, end=46)),
        TokenizationToken(text="CRF", text_range=TextRange(start=46, end=49)),
        TokenizationToken(text="M", text_range=TextRange(start=49, end=50)),
        TokenizationToken(text=")", text_range=TextRange(start=50, end=51)),
        TokenizationToken(text="▁is", text_range=TextRange(start=51, end=54)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=54, end=75)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=75, end=86)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=86, end=98)),
        TokenizationToken(text="▁the", text_range=TextRange(start=98, end=102)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=102, end=111)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=111, end=125)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=125, end=131)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=131, end=140)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=140, end=164)),
        TokenizationToken(text="▁", text_range=TextRange(start=164, end=165)),
        TokenizationToken(text="(", text_range=TextRange(start=165, end=166)),
        TokenizationToken(text="HAI", text_range=TextRange(start=166, end=169)),
        TokenizationToken(text=")", text_range=TextRange(start=169, end=170)),
        TokenizationToken(text="▁that", text_range=TextRange(start=170, end=175)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=175, end=188)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=188, end=200)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=200, end=212)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=212, end=222)),
        TokenizationToken(text=",", text_range=TextRange(start=222, end=223)),
        TokenizationToken(text="▁development", text_range=TextRange(start=223, end=235)),
        TokenizationToken(text=",", text_range=TextRange(start=235, end=236)),
        TokenizationToken(text="▁and", text_range=TextRange(start=236, end=240)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=240, end=254)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=254, end=265)),
        TokenizationToken(text="▁models", text_range=TextRange(start=265, end=272)),
        TokenizationToken(text=".", text_range=TextRange(start=272, end=273)),
        TokenizationToken(text="The", text_range=TextRange(start=273, end=276)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=276, end=287)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=287, end=299)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=299, end=310)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=310, end=317)),
        TokenizationToken(text="▁", text_range=TextRange(start=317, end=318)),
        TokenizationToken(text="(", text_range=TextRange(start=318, end=319)),
        TokenizationToken(text="CRF", text_range=TextRange(start=319, end=322)),
        TokenizationToken(text="M", text_range=TextRange(start=322, end=323)),
        TokenizationToken(text=")", text_range=TextRange(start=323, end=324)),
        TokenizationToken(text="▁is", text_range=TextRange(start=324, end=327)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=327, end=348)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=348, end=359)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=359, end=371)),
        TokenizationToken(text="▁the", text_range=TextRange(start=371, end=375)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=375, end=384)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=384, end=398)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=398, end=404)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=404, end=413)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=413, end=437)),
        TokenizationToken(text="▁", text_range=TextRange(start=437, end=438)),
        TokenizationToken(text="(", text_range=TextRange(start=438, end=439)),
        TokenizationToken(text="HAI", text_range=TextRange(start=439, end=442)),
        TokenizationToken(text=")", text_range=TextRange(start=442, end=443)),
        TokenizationToken(text="▁that", text_range=TextRange(start=443, end=448)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=448, end=461)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=461, end=473)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=473, end=485)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=485, end=495)),
        TokenizationToken(text=",", text_range=TextRange(start=495, end=496)),
        TokenizationToken(text="▁development", text_range=TextRange(start=496, end=508)),
        TokenizationToken(text=",", text_range=TextRange(start=508, end=509)),
        TokenizationToken(text="▁and", text_range=TextRange(start=509, end=513)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=513, end=527)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=527, end=538)),
        TokenizationToken(text="▁models", text_range=TextRange(start=538, end=545)),
        TokenizationToken(text=".", text_range=TextRange(start=545, end=546)),
        TokenizationToken(text="The", text_range=TextRange(start=546, end=549)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=549, end=560)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=560, end=572)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=572, end=583)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=583, end=590)),
        TokenizationToken(text="▁", text_range=TextRange(start=590, end=591)),
        TokenizationToken(text="(", text_range=TextRange(start=591, end=592)),
        TokenizationToken(text="CRF", text_range=TextRange(start=592, end=595)),
        TokenizationToken(text="M", text_range=TextRange(start=595, end=596)),
        TokenizationToken(text=")", text_range=TextRange(start=596, end=597)),
        TokenizationToken(text="▁is", text_range=TextRange(start=597, end=600)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=600, end=621)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=621, end=632)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=632, end=644)),
        TokenizationToken(text="▁the", text_range=TextRange(start=644, end=648)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=648, end=657)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=657, end=671)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=671, end=677)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=677, end=686)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=686, end=710)),
        TokenizationToken(text="▁", text_range=TextRange(start=710, end=711)),
        TokenizationToken(text="(", text_range=TextRange(start=711, end=712)),
        TokenizationToken(text="HAI", text_range=TextRange(start=712, end=715)),
        TokenizationToken(text=")", text_range=TextRange(start=715, end=716)),
        TokenizationToken(text="▁that", text_range=TextRange(start=716, end=721)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=721, end=734)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=734, end=746)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=746, end=758)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=758, end=768)),
        TokenizationToken(text=",", text_range=TextRange(start=768, end=769)),
        TokenizationToken(text="▁development", text_range=TextRange(start=769, end=781)),
        TokenizationToken(text=",", text_range=TextRange(start=781, end=782)),
        TokenizationToken(text="▁and", text_range=TextRange(start=782, end=786)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=786, end=800)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=800, end=811)),
        TokenizationToken(text="▁models", text_range=TextRange(start=811, end=818)),
        TokenizationToken(text=".", text_range=TextRange(start=818, end=819)),
        TokenizationToken(text="The", text_range=TextRange(start=819, end=822)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=822, end=833)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=833, end=845)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=845, end=856)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=856, end=863)),
        TokenizationToken(text="▁", text_range=TextRange(start=863, end=864)),
        TokenizationToken(text="(", text_range=TextRange(start=864, end=865)),
        TokenizationToken(text="CRF", text_range=TextRange(start=865, end=868)),
        TokenizationToken(text="M", text_range=TextRange(start=868, end=869)),
        TokenizationToken(text=")", text_range=TextRange(start=869, end=870)),
        TokenizationToken(text="▁is", text_range=TextRange(start=870, end=873)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=873, end=894)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=894, end=905)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=905, end=917)),
        TokenizationToken(text="▁the", text_range=TextRange(start=917, end=921)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=921, end=930)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=930, end=944)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=944, end=950)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=950, end=959)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=959, end=983)),
        TokenizationToken(text="▁", text_range=TextRange(start=983, end=984)),
        TokenizationToken(text="(", text_range=TextRange(start=984, end=985)),
        TokenizationToken(text="HAI", text_range=TextRange(start=985, end=988)),
        TokenizationToken(text=")", text_range=TextRange(start=988, end=989)),
        TokenizationToken(text="▁that", text_range=TextRange(start=989, end=994)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=994, end=1007)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=1007, end=1019)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=1019, end=1031)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=1031, end=1041)),
        TokenizationToken(text=",", text_range=TextRange(start=1041, end=1042)),
        TokenizationToken(text="▁development", text_range=TextRange(start=1042, end=1054)),
        TokenizationToken(text=",", text_range=TextRange(start=1054, end=1055)),
        TokenizationToken(text="▁and", text_range=TextRange(start=1055, end=1059)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=1059, end=1073)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=1073, end=1084)),
        TokenizationToken(text="▁models", text_range=TextRange(start=1084, end=1091)),
        TokenizationToken(text=".", text_range=TextRange(start=1091, end=1092)),
        TokenizationToken(text="The", text_range=TextRange(start=1092, end=1095)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=1095, end=1106)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=1106, end=1118)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=1118, end=1129)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=1129, end=1136)),
        TokenizationToken(text="▁", text_range=TextRange(start=1136, end=1137)),
        TokenizationToken(text="(", text_range=TextRange(start=1137, end=1138)),
        TokenizationToken(text="CRF", text_range=TextRange(start=1138, end=1141)),
        TokenizationToken(text="M", text_range=TextRange(start=1141, end=1142)),
        TokenizationToken(text=")", text_range=TextRange(start=1142, end=1143)),
        TokenizationToken(text="▁is", text_range=TextRange(start=1143, end=1146)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=1146, end=1167)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=1167, end=1178)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=1178, end=1190)),
        TokenizationToken(text="▁the", text_range=TextRange(start=1190, end=1194)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=1194, end=1203)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=1203, end=1217)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=1217, end=1223)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=1223, end=1232)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=1232, end=1256)),
        TokenizationToken(text="▁", text_range=TextRange(start=1256, end=1257)),
        TokenizationToken(text="(", text_range=TextRange(start=1257, end=1258)),
        TokenizationToken(text="HAI", text_range=TextRange(start=1258, end=1261)),
        TokenizationToken(text=")", text_range=TextRange(start=1261, end=1262)),
        TokenizationToken(text="▁that", text_range=TextRange(start=1262, end=1267)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=1267, end=1280)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=1280, end=1292)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=1292, end=1304)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=1304, end=1314)),
        TokenizationToken(text=",", text_range=TextRange(start=1314, end=1315)),
        TokenizationToken(text="▁development", text_range=TextRange(start=1315, end=1327)),
        TokenizationToken(text=",", text_range=TextRange(start=1327, end=1328)),
        TokenizationToken(text="▁and", text_range=TextRange(start=1328, end=1332)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=1332, end=1346)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=1346, end=1357)),
        TokenizationToken(text="▁models", text_range=TextRange(start=1357, end=1364)),
        TokenizationToken(text=".", text_range=TextRange(start=1364, end=1365)),
        TokenizationToken(text="The", text_range=TextRange(start=1365, end=1368)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=1368, end=1379)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=1379, end=1391)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=1391, end=1402)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=1402, end=1409)),
        TokenizationToken(text="▁", text_range=TextRange(start=1409, end=1410)),
        TokenizationToken(text="(", text_range=TextRange(start=1410, end=1411)),
        TokenizationToken(text="CRF", text_range=TextRange(start=1411, end=1414)),
        TokenizationToken(text="M", text_range=TextRange(start=1414, end=1415)),
        TokenizationToken(text=")", text_range=TextRange(start=1415, end=1416)),
        TokenizationToken(text="▁is", text_range=TextRange(start=1416, end=1419)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=1419, end=1440)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=1440, end=1451)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=1451, end=1463)),
        TokenizationToken(text="▁the", text_range=TextRange(start=1463, end=1467)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=1467, end=1476)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=1476, end=1490)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=1490, end=1496)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=1496, end=1505)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=1505, end=1529)),
        TokenizationToken(text="▁", text_range=TextRange(start=1529, end=1530)),
        TokenizationToken(text="(", text_range=TextRange(start=1530, end=1531)),
        TokenizationToken(text="HAI", text_range=TextRange(start=1531, end=1534)),
        TokenizationToken(text=")", text_range=TextRange(start=1534, end=1535)),
        TokenizationToken(text="▁that", text_range=TextRange(start=1535, end=1540)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=1540, end=1553)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=1553, end=1565)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=1565, end=1577)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=1577, end=1587)),
        TokenizationToken(text=",", text_range=TextRange(start=1587, end=1588)),
        TokenizationToken(text="▁development", text_range=TextRange(start=1588, end=1600)),
        TokenizationToken(text=",", text_range=TextRange(start=1600, end=1601)),
        TokenizationToken(text="▁and", text_range=TextRange(start=1601, end=1605)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=1605, end=1619)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=1619, end=1630)),
        TokenizationToken(text="▁models", text_range=TextRange(start=1630, end=1637)),
        TokenizationToken(text=".", text_range=TextRange(start=1637, end=1638)),
        TokenizationToken(text="The", text_range=TextRange(start=1638, end=1641)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=1641, end=1652)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=1652, end=1664)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=1664, end=1675)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=1675, end=1682)),
        TokenizationToken(text="▁", text_range=TextRange(start=1682, end=1683)),
        TokenizationToken(text="(", text_range=TextRange(start=1683, end=1684)),
        TokenizationToken(text="CRF", text_range=TextRange(start=1684, end=1687)),
        TokenizationToken(text="M", text_range=TextRange(start=1687, end=1688)),
        TokenizationToken(text=")", text_range=TextRange(start=1688, end=1689)),
        TokenizationToken(text="▁is", text_range=TextRange(start=1689, end=1692)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=1692, end=1713)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=1713, end=1724)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=1724, end=1736)),
        TokenizationToken(text="▁the", text_range=TextRange(start=1736, end=1740)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=1740, end=1749)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=1749, end=1763)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=1763, end=1769)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=1769, end=1778)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=1778, end=1802)),
        TokenizationToken(text="▁", text_range=TextRange(start=1802, end=1803)),
        TokenizationToken(text="(", text_range=TextRange(start=1803, end=1804)),
        TokenizationToken(text="HAI", text_range=TextRange(start=1804, end=1807)),
        TokenizationToken(text=")", text_range=TextRange(start=1807, end=1808)),
        TokenizationToken(text="▁that", text_range=TextRange(start=1808, end=1813)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=1813, end=1826)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=1826, end=1838)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=1838, end=1850)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=1850, end=1860)),
        TokenizationToken(text=",", text_range=TextRange(start=1860, end=1861)),
        TokenizationToken(text="▁development", text_range=TextRange(start=1861, end=1873)),
        TokenizationToken(text=",", text_range=TextRange(start=1873, end=1874)),
        TokenizationToken(text="▁and", text_range=TextRange(start=1874, end=1878)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=1878, end=1892)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=1892, end=1903)),
        TokenizationToken(text="▁models", text_range=TextRange(start=1903, end=1910)),
        TokenizationToken(text=".", text_range=TextRange(start=1910, end=1911)),
        TokenizationToken(text="The", text_range=TextRange(start=1911, end=1914)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=1914, end=1925)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=1925, end=1937)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=1937, end=1948)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=1948, end=1955)),
        TokenizationToken(text="▁", text_range=TextRange(start=1955, end=1956)),
        TokenizationToken(text="(", text_range=TextRange(start=1956, end=1957)),
        TokenizationToken(text="CRF", text_range=TextRange(start=1957, end=1960)),
        TokenizationToken(text="M", text_range=TextRange(start=1960, end=1961)),
        TokenizationToken(text=")", text_range=TextRange(start=1961, end=1962)),
        TokenizationToken(text="▁is", text_range=TextRange(start=1962, end=1965)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=1965, end=1986)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=1986, end=1997)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=1997, end=2009)),
        TokenizationToken(text="▁the", text_range=TextRange(start=2009, end=2013)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=2013, end=2022)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=2022, end=2036)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=2036, end=2042)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=2042, end=2051)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=2051, end=2075)),
        TokenizationToken(text="▁", text_range=TextRange(start=2075, end=2076)),
        TokenizationToken(text="(", text_range=TextRange(start=2076, end=2077)),
        TokenizationToken(text="HAI", text_range=TextRange(start=2077, end=2080)),
        TokenizationToken(text=")", text_range=TextRange(start=2080, end=2081)),
        TokenizationToken(text="▁that", text_range=TextRange(start=2081, end=2086)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=2086, end=2099)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=2099, end=2111)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=2111, end=2123)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=2123, end=2133)),
        TokenizationToken(text=",", text_range=TextRange(start=2133, end=2134)),
        TokenizationToken(text="▁development", text_range=TextRange(start=2134, end=2146)),
        TokenizationToken(text=",", text_range=TextRange(start=2146, end=2147)),
        TokenizationToken(text="▁and", text_range=TextRange(start=2147, end=2151)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=2151, end=2165)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=2165, end=2176)),
        TokenizationToken(text="▁models", text_range=TextRange(start=2176, end=2183)),
        TokenizationToken(text=".", text_range=TextRange(start=2183, end=2184)),
        TokenizationToken(text="The", text_range=TextRange(start=2184, end=2187)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=2187, end=2198)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=2198, end=2210)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=2210, end=2221)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=2221, end=2228)),
        TokenizationToken(text="▁", text_range=TextRange(start=2228, end=2229)),
        TokenizationToken(text="(", text_range=TextRange(start=2229, end=2230)),
        TokenizationToken(text="CRF", text_range=TextRange(start=2230, end=2233)),
        TokenizationToken(text="M", text_range=TextRange(start=2233, end=2234)),
        TokenizationToken(text=")", text_range=TextRange(start=2234, end=2235)),
        TokenizationToken(text="▁is", text_range=TextRange(start=2235, end=2238)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=2238, end=2259)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=2259, end=2270)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=2270, end=2282)),
        TokenizationToken(text="▁the", text_range=TextRange(start=2282, end=2286)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=2286, end=2295)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=2295, end=2309)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=2309, end=2315)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=2315, end=2324)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=2324, end=2348)),
        TokenizationToken(text="▁", text_range=TextRange(start=2348, end=2349)),
        TokenizationToken(text="(", text_range=TextRange(start=2349, end=2350)),
        TokenizationToken(text="HAI", text_range=TextRange(start=2350, end=2353)),
        TokenizationToken(text=")", text_range=TextRange(start=2353, end=2354)),
        TokenizationToken(text="▁that", text_range=TextRange(start=2354, end=2359)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=2359, end=2372)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=2372, end=2384)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=2384, end=2396)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=2396, end=2406)),
        TokenizationToken(text=",", text_range=TextRange(start=2406, end=2407)),
        TokenizationToken(text="▁development", text_range=TextRange(start=2407, end=2419)),
        TokenizationToken(text=",", text_range=TextRange(start=2419, end=2420)),
        TokenizationToken(text="▁and", text_range=TextRange(start=2420, end=2424)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=2424, end=2438)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=2438, end=2449)),
        TokenizationToken(text="▁models", text_range=TextRange(start=2449, end=2456)),
        TokenizationToken(text=".", text_range=TextRange(start=2456, end=2457)),
        TokenizationToken(text="The", text_range=TextRange(start=2457, end=2460)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=2460, end=2471)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=2471, end=2483)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=2483, end=2494)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=2494, end=2501)),
        TokenizationToken(text="▁", text_range=TextRange(start=2501, end=2502)),
        TokenizationToken(text="(", text_range=TextRange(start=2502, end=2503)),
        TokenizationToken(text="CRF", text_range=TextRange(start=2503, end=2506)),
        TokenizationToken(text="M", text_range=TextRange(start=2506, end=2507)),
        TokenizationToken(text=")", text_range=TextRange(start=2507, end=2508)),
        TokenizationToken(text="▁is", text_range=TextRange(start=2508, end=2511)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=2511, end=2532)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=2532, end=2543)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=2543, end=2555)),
        TokenizationToken(text="▁the", text_range=TextRange(start=2555, end=2559)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=2559, end=2568)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=2568, end=2582)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=2582, end=2588)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=2588, end=2597)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=2597, end=2621)),
        TokenizationToken(text="▁", text_range=TextRange(start=2621, end=2622)),
        TokenizationToken(text="(", text_range=TextRange(start=2622, end=2623)),
        TokenizationToken(text="HAI", text_range=TextRange(start=2623, end=2626)),
        TokenizationToken(text=")", text_range=TextRange(start=2626, end=2627)),
        TokenizationToken(text="▁that", text_range=TextRange(start=2627, end=2632)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=2632, end=2645)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=2645, end=2657)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=2657, end=2669)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=2669, end=2679)),
        TokenizationToken(text=",", text_range=TextRange(start=2679, end=2680)),
        TokenizationToken(text="▁development", text_range=TextRange(start=2680, end=2692)),
        TokenizationToken(text=",", text_range=TextRange(start=2692, end=2693)),
        TokenizationToken(text="▁and", text_range=TextRange(start=2693, end=2697)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=2697, end=2711)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=2711, end=2722)),
        TokenizationToken(text="▁models", text_range=TextRange(start=2722, end=2729)),
        TokenizationToken(text=".", text_range=TextRange(start=2729, end=2730)),
        TokenizationToken(text="The", text_range=TextRange(start=2730, end=2733)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=2733, end=2744)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=2744, end=2756)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=2756, end=2767)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=2767, end=2774)),
        TokenizationToken(text="▁", text_range=TextRange(start=2774, end=2775)),
        TokenizationToken(text="(", text_range=TextRange(start=2775, end=2776)),
        TokenizationToken(text="CRF", text_range=TextRange(start=2776, end=2779)),
        TokenizationToken(text="M", text_range=TextRange(start=2779, end=2780)),
        TokenizationToken(text=")", text_range=TextRange(start=2780, end=2781)),
        TokenizationToken(text="▁is", text_range=TextRange(start=2781, end=2784)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=2784, end=2805)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=2805, end=2816)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=2816, end=2828)),
        TokenizationToken(text="▁the", text_range=TextRange(start=2828, end=2832)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=2832, end=2841)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=2841, end=2855)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=2855, end=2861)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=2861, end=2870)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=2870, end=2894)),
        TokenizationToken(text="▁", text_range=TextRange(start=2894, end=2895)),
        TokenizationToken(text="(", text_range=TextRange(start=2895, end=2896)),
        TokenizationToken(text="HAI", text_range=TextRange(start=2896, end=2899)),
        TokenizationToken(text=")", text_range=TextRange(start=2899, end=2900)),
        TokenizationToken(text="▁that", text_range=TextRange(start=2900, end=2905)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=2905, end=2918)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=2918, end=2930)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=2930, end=2942)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=2942, end=2952)),
        TokenizationToken(text=",", text_range=TextRange(start=2952, end=2953)),
        TokenizationToken(text="▁development", text_range=TextRange(start=2953, end=2965)),
        TokenizationToken(text=",", text_range=TextRange(start=2965, end=2966)),
        TokenizationToken(text="▁and", text_range=TextRange(start=2966, end=2970)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=2970, end=2984)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=2984, end=2995)),
        TokenizationToken(text="▁models", text_range=TextRange(start=2995, end=3002)),
        TokenizationToken(text=".", text_range=TextRange(start=3002, end=3003)),
        TokenizationToken(text="The", text_range=TextRange(start=3003, end=3006)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=3006, end=3017)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=3017, end=3029)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=3029, end=3040)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=3040, end=3047)),
        TokenizationToken(text="▁", text_range=TextRange(start=3047, end=3048)),
        TokenizationToken(text="(", text_range=TextRange(start=3048, end=3049)),
        TokenizationToken(text="CRF", text_range=TextRange(start=3049, end=3052)),
        TokenizationToken(text="M", text_range=TextRange(start=3052, end=3053)),
        TokenizationToken(text=")", text_range=TextRange(start=3053, end=3054)),
        TokenizationToken(text="▁is", text_range=TextRange(start=3054, end=3057)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=3057, end=3078)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=3078, end=3089)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=3089, end=3101)),
        TokenizationToken(text="▁the", text_range=TextRange(start=3101, end=3105)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=3105, end=3114)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=3114, end=3128)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=3128, end=3134)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=3134, end=3143)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=3143, end=3167)),
        TokenizationToken(text="▁", text_range=TextRange(start=3167, end=3168)),
        TokenizationToken(text="(", text_range=TextRange(start=3168, end=3169)),
        TokenizationToken(text="HAI", text_range=TextRange(start=3169, end=3172)),
        TokenizationToken(text=")", text_range=TextRange(start=3172, end=3173)),
        TokenizationToken(text="▁that", text_range=TextRange(start=3173, end=3178)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=3178, end=3191)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=3191, end=3203)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=3203, end=3215)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=3215, end=3225)),
        TokenizationToken(text=",", text_range=TextRange(start=3225, end=3226)),
        TokenizationToken(text="▁development", text_range=TextRange(start=3226, end=3238)),
        TokenizationToken(text=",", text_range=TextRange(start=3238, end=3239)),
        TokenizationToken(text="▁and", text_range=TextRange(start=3239, end=3243)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=3243, end=3257)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=3257, end=3268)),
        TokenizationToken(text="▁models", text_range=TextRange(start=3268, end=3275)),
        TokenizationToken(text=".", text_range=TextRange(start=3275, end=3276)),
        TokenizationToken(text="The", text_range=TextRange(start=3276, end=3279)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=3279, end=3290)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=3290, end=3302)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=3302, end=3313)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=3313, end=3320)),
        TokenizationToken(text="▁", text_range=TextRange(start=3320, end=3321)),
        TokenizationToken(text="(", text_range=TextRange(start=3321, end=3322)),
        TokenizationToken(text="CRF", text_range=TextRange(start=3322, end=3325)),
        TokenizationToken(text="M", text_range=TextRange(start=3325, end=3326)),
        TokenizationToken(text=")", text_range=TextRange(start=3326, end=3327)),
        TokenizationToken(text="▁is", text_range=TextRange(start=3327, end=3330)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=3330, end=3351)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=3351, end=3362)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=3362, end=3374)),
        TokenizationToken(text="▁the", text_range=TextRange(start=3374, end=3378)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=3378, end=3387)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=3387, end=3401)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=3401, end=3407)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=3407, end=3416)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=3416, end=3440)),
        TokenizationToken(text="▁", text_range=TextRange(start=3440, end=3441)),
        TokenizationToken(text="(", text_range=TextRange(start=3441, end=3442)),
        TokenizationToken(text="HAI", text_range=TextRange(start=3442, end=3445)),
        TokenizationToken(text=")", text_range=TextRange(start=3445, end=3446)),
        TokenizationToken(text="▁that", text_range=TextRange(start=3446, end=3451)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=3451, end=3464)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=3464, end=3476)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=3476, end=3488)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=3488, end=3498)),
        TokenizationToken(text=",", text_range=TextRange(start=3498, end=3499)),
        TokenizationToken(text="▁development", text_range=TextRange(start=3499, end=3511)),
        TokenizationToken(text=",", text_range=TextRange(start=3511, end=3512)),
        TokenizationToken(text="▁and", text_range=TextRange(start=3512, end=3516)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=3516, end=3530)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=3530, end=3541)),
        TokenizationToken(text="▁models", text_range=TextRange(start=3541, end=3548)),
        TokenizationToken(text=".", text_range=TextRange(start=3548, end=3549)),
        TokenizationToken(text="The", text_range=TextRange(start=3549, end=3552)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=3552, end=3563)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=3563, end=3575)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=3575, end=3586)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=3586, end=3593)),
        TokenizationToken(text="▁", text_range=TextRange(start=3593, end=3594)),
        TokenizationToken(text="(", text_range=TextRange(start=3594, end=3595)),
        TokenizationToken(text="CRF", text_range=TextRange(start=3595, end=3598)),
        TokenizationToken(text="M", text_range=TextRange(start=3598, end=3599)),
        TokenizationToken(text=")", text_range=TextRange(start=3599, end=3600)),
        TokenizationToken(text="▁is", text_range=TextRange(start=3600, end=3603)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=3603, end=3624)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=3624, end=3635)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=3635, end=3647)),
        TokenizationToken(text="▁the", text_range=TextRange(start=3647, end=3651)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=3651, end=3660)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=3660, end=3674)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=3674, end=3680)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=3680, end=3689)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=3689, end=3713)),
        TokenizationToken(text="▁", text_range=TextRange(start=3713, end=3714)),
        TokenizationToken(text="(", text_range=TextRange(start=3714, end=3715)),
        TokenizationToken(text="HAI", text_range=TextRange(start=3715, end=3718)),
        TokenizationToken(text=")", text_range=TextRange(start=3718, end=3719)),
        TokenizationToken(text="▁that", text_range=TextRange(start=3719, end=3724)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=3724, end=3737)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=3737, end=3749)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=3749, end=3761)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=3761, end=3771)),
        TokenizationToken(text=",", text_range=TextRange(start=3771, end=3772)),
        TokenizationToken(text="▁development", text_range=TextRange(start=3772, end=3784)),
        TokenizationToken(text=",", text_range=TextRange(start=3784, end=3785)),
        TokenizationToken(text="▁and", text_range=TextRange(start=3785, end=3789)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=3789, end=3803)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=3803, end=3814)),
        TokenizationToken(text="▁models", text_range=TextRange(start=3814, end=3821)),
        TokenizationToken(text=".", text_range=TextRange(start=3821, end=3822)),
        TokenizationToken(text="The", text_range=TextRange(start=3822, end=3825)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=3825, end=3836)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=3836, end=3848)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=3848, end=3859)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=3859, end=3866)),
        TokenizationToken(text="▁", text_range=TextRange(start=3866, end=3867)),
        TokenizationToken(text="(", text_range=TextRange(start=3867, end=3868)),
        TokenizationToken(text="CRF", text_range=TextRange(start=3868, end=3871)),
        TokenizationToken(text="M", text_range=TextRange(start=3871, end=3872)),
        TokenizationToken(text=")", text_range=TextRange(start=3872, end=3873)),
        TokenizationToken(text="▁is", text_range=TextRange(start=3873, end=3876)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=3876, end=3897)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=3897, end=3908)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=3908, end=3920)),
        TokenizationToken(text="▁the", text_range=TextRange(start=3920, end=3924)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=3924, end=3933)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=3933, end=3947)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=3947, end=3953)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=3953, end=3962)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=3962, end=3986)),
        TokenizationToken(text="▁", text_range=TextRange(start=3986, end=3987)),
        TokenizationToken(text="(", text_range=TextRange(start=3987, end=3988)),
        TokenizationToken(text="HAI", text_range=TextRange(start=3988, end=3991)),
        TokenizationToken(text=")", text_range=TextRange(start=3991, end=3992)),
        TokenizationToken(text="▁that", text_range=TextRange(start=3992, end=3997)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=3997, end=4010)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=4010, end=4022)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=4022, end=4034)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=4034, end=4044)),
        TokenizationToken(text=",", text_range=TextRange(start=4044, end=4045)),
        TokenizationToken(text="▁development", text_range=TextRange(start=4045, end=4057)),
        TokenizationToken(text=",", text_range=TextRange(start=4057, end=4058)),
        TokenizationToken(text="▁and", text_range=TextRange(start=4058, end=4062)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=4062, end=4076)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=4076, end=4087)),
        TokenizationToken(text="▁models", text_range=TextRange(start=4087, end=4094)),
        TokenizationToken(text=".", text_range=TextRange(start=4094, end=4095)),
        TokenizationToken(text="The", text_range=TextRange(start=4095, end=4098)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=4098, end=4109)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=4109, end=4121)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=4121, end=4132)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=4132, end=4139)),
        TokenizationToken(text="▁", text_range=TextRange(start=4139, end=4140)),
        TokenizationToken(text="(", text_range=TextRange(start=4140, end=4141)),
        TokenizationToken(text="CRF", text_range=TextRange(start=4141, end=4144)),
        TokenizationToken(text="M", text_range=TextRange(start=4144, end=4145)),
        TokenizationToken(text=")", text_range=TextRange(start=4145, end=4146)),
        TokenizationToken(text="▁is", text_range=TextRange(start=4146, end=4149)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=4149, end=4170)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=4170, end=4181)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=4181, end=4193)),
        TokenizationToken(text="▁the", text_range=TextRange(start=4193, end=4197)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=4197, end=4206)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=4206, end=4220)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=4220, end=4226)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=4226, end=4235)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=4235, end=4259)),
        TokenizationToken(text="▁", text_range=TextRange(start=4259, end=4260)),
        TokenizationToken(text="(", text_range=TextRange(start=4260, end=4261)),
        TokenizationToken(text="HAI", text_range=TextRange(start=4261, end=4264)),
        TokenizationToken(text=")", text_range=TextRange(start=4264, end=4265)),
        TokenizationToken(text="▁that", text_range=TextRange(start=4265, end=4270)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=4270, end=4283)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=4283, end=4295)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=4295, end=4307)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=4307, end=4317)),
        TokenizationToken(text=",", text_range=TextRange(start=4317, end=4318)),
        TokenizationToken(text="▁development", text_range=TextRange(start=4318, end=4330)),
        TokenizationToken(text=",", text_range=TextRange(start=4330, end=4331)),
        TokenizationToken(text="▁and", text_range=TextRange(start=4331, end=4335)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=4335, end=4349)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=4349, end=4360)),
        TokenizationToken(text="▁models", text_range=TextRange(start=4360, end=4367)),
        TokenizationToken(text=".", text_range=TextRange(start=4367, end=4368)),
        TokenizationToken(text="The", text_range=TextRange(start=4368, end=4371)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=4371, end=4382)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=4382, end=4394)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=4394, end=4405)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=4405, end=4412)),
        TokenizationToken(text="▁", text_range=TextRange(start=4412, end=4413)),
        TokenizationToken(text="(", text_range=TextRange(start=4413, end=4414)),
        TokenizationToken(text="CRF", text_range=TextRange(start=4414, end=4417)),
        TokenizationToken(text="M", text_range=TextRange(start=4417, end=4418)),
        TokenizationToken(text=")", text_range=TextRange(start=4418, end=4419)),
        TokenizationToken(text="▁is", text_range=TextRange(start=4419, end=4422)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=4422, end=4443)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=4443, end=4454)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=4454, end=4466)),
        TokenizationToken(text="▁the", text_range=TextRange(start=4466, end=4470)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=4470, end=4479)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=4479, end=4493)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=4493, end=4499)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=4499, end=4508)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=4508, end=4532)),
        TokenizationToken(text="▁", text_range=TextRange(start=4532, end=4533)),
        TokenizationToken(text="(", text_range=TextRange(start=4533, end=4534)),
        TokenizationToken(text="HAI", text_range=TextRange(start=4534, end=4537)),
        TokenizationToken(text=")", text_range=TextRange(start=4537, end=4538)),
        TokenizationToken(text="▁that", text_range=TextRange(start=4538, end=4543)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=4543, end=4556)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=4556, end=4568)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=4568, end=4580)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=4580, end=4590)),
        TokenizationToken(text=",", text_range=TextRange(start=4590, end=4591)),
        TokenizationToken(text="▁development", text_range=TextRange(start=4591, end=4603)),
        TokenizationToken(text=",", text_range=TextRange(start=4603, end=4604)),
        TokenizationToken(text="▁and", text_range=TextRange(start=4604, end=4608)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=4608, end=4622)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=4622, end=4633)),
        TokenizationToken(text="▁models", text_range=TextRange(start=4633, end=4640)),
        TokenizationToken(text=".", text_range=TextRange(start=4640, end=4641)),
        TokenizationToken(text="The", text_range=TextRange(start=4641, end=4644)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=4644, end=4655)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=4655, end=4667)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=4667, end=4678)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=4678, end=4685)),
        TokenizationToken(text="▁", text_range=TextRange(start=4685, end=4686)),
        TokenizationToken(text="(", text_range=TextRange(start=4686, end=4687)),
        TokenizationToken(text="CRF", text_range=TextRange(start=4687, end=4690)),
        TokenizationToken(text="M", text_range=TextRange(start=4690, end=4691)),
        TokenizationToken(text=")", text_range=TextRange(start=4691, end=4692)),
        TokenizationToken(text="▁is", text_range=TextRange(start=4692, end=4695)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=4695, end=4716)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=4716, end=4727)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=4727, end=4739)),
        TokenizationToken(text="▁the", text_range=TextRange(start=4739, end=4743)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=4743, end=4752)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=4752, end=4766)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=4766, end=4772)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=4772, end=4781)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=4781, end=4805)),
        TokenizationToken(text="▁", text_range=TextRange(start=4805, end=4806)),
        TokenizationToken(text="(", text_range=TextRange(start=4806, end=4807)),
        TokenizationToken(text="HAI", text_range=TextRange(start=4807, end=4810)),
        TokenizationToken(text=")", text_range=TextRange(start=4810, end=4811)),
        TokenizationToken(text="▁that", text_range=TextRange(start=4811, end=4816)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=4816, end=4829)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=4829, end=4841)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=4841, end=4853)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=4853, end=4863)),
        TokenizationToken(text=",", text_range=TextRange(start=4863, end=4864)),
        TokenizationToken(text="▁development", text_range=TextRange(start=4864, end=4876)),
        TokenizationToken(text=",", text_range=TextRange(start=4876, end=4877)),
        TokenizationToken(text="▁and", text_range=TextRange(start=4877, end=4881)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=4881, end=4895)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=4895, end=4906)),
        TokenizationToken(text="▁models", text_range=TextRange(start=4906, end=4913)),
        TokenizationToken(text=".", text_range=TextRange(start=4913, end=4914)),
        TokenizationToken(text="The", text_range=TextRange(start=4914, end=4917)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=4917, end=4928)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=4928, end=4940)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=4940, end=4951)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=4951, end=4958)),
        TokenizationToken(text="▁", text_range=TextRange(start=4958, end=4959)),
        TokenizationToken(text="(", text_range=TextRange(start=4959, end=4960)),
        TokenizationToken(text="CRF", text_range=TextRange(start=4960, end=4963)),
        TokenizationToken(text="M", text_range=TextRange(start=4963, end=4964)),
        TokenizationToken(text=")", text_range=TextRange(start=4964, end=4965)),
        TokenizationToken(text="▁is", text_range=TextRange(start=4965, end=4968)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=4968, end=4989)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=4989, end=5000)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=5000, end=5012)),
        TokenizationToken(text="▁the", text_range=TextRange(start=5012, end=5016)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=5016, end=5025)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=5025, end=5039)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=5039, end=5045)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=5045, end=5054)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=5054, end=5078)),
        TokenizationToken(text="▁", text_range=TextRange(start=5078, end=5079)),
        TokenizationToken(text="(", text_range=TextRange(start=5079, end=5080)),
        TokenizationToken(text="HAI", text_range=TextRange(start=5080, end=5083)),
        TokenizationToken(text=")", text_range=TextRange(start=5083, end=5084)),
        TokenizationToken(text="▁that", text_range=TextRange(start=5084, end=5089)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=5089, end=5102)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=5102, end=5114)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=5114, end=5126)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=5126, end=5136)),
        TokenizationToken(text=",", text_range=TextRange(start=5136, end=5137)),
        TokenizationToken(text="▁development", text_range=TextRange(start=5137, end=5149)),
        TokenizationToken(text=",", text_range=TextRange(start=5149, end=5150)),
        TokenizationToken(text="▁and", text_range=TextRange(start=5150, end=5154)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=5154, end=5168)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=5168, end=5179)),
        TokenizationToken(text="▁models", text_range=TextRange(start=5179, end=5186)),
        TokenizationToken(text=".", text_range=TextRange(start=5186, end=5187)),
        TokenizationToken(text="The", text_range=TextRange(start=5187, end=5190)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=5190, end=5201)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=5201, end=5213)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=5213, end=5224)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=5224, end=5231)),
        TokenizationToken(text="▁", text_range=TextRange(start=5231, end=5232)),
        TokenizationToken(text="(", text_range=TextRange(start=5232, end=5233)),
        TokenizationToken(text="CRF", text_range=TextRange(start=5233, end=5236)),
        TokenizationToken(text="M", text_range=TextRange(start=5236, end=5237)),
        TokenizationToken(text=")", text_range=TextRange(start=5237, end=5238)),
        TokenizationToken(text="▁is", text_range=TextRange(start=5238, end=5241)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=5241, end=5262)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=5262, end=5273)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=5273, end=5285)),
        TokenizationToken(text="▁the", text_range=TextRange(start=5285, end=5289)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=5289, end=5298)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=5298, end=5312)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=5312, end=5318)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=5318, end=5327)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=5327, end=5351)),
        TokenizationToken(text="▁", text_range=TextRange(start=5351, end=5352)),
        TokenizationToken(text="(", text_range=TextRange(start=5352, end=5353)),
        TokenizationToken(text="HAI", text_range=TextRange(start=5353, end=5356)),
        TokenizationToken(text=")", text_range=TextRange(start=5356, end=5357)),
        TokenizationToken(text="▁that", text_range=TextRange(start=5357, end=5362)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=5362, end=5375)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=5375, end=5387)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=5387, end=5399)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=5399, end=5409)),
        TokenizationToken(text=",", text_range=TextRange(start=5409, end=5410)),
        TokenizationToken(text="▁development", text_range=TextRange(start=5410, end=5422)),
        TokenizationToken(text=",", text_range=TextRange(start=5422, end=5423)),
        TokenizationToken(text="▁and", text_range=TextRange(start=5423, end=5427)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=5427, end=5441)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=5441, end=5452)),
        TokenizationToken(text="▁models", text_range=TextRange(start=5452, end=5459)),
        TokenizationToken(text=".", text_range=TextRange(start=5459, end=5460)),
        TokenizationToken(text="The", text_range=TextRange(start=5460, end=5463)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=5463, end=5474)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=5474, end=5486)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=5486, end=5497)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=5497, end=5504)),
        TokenizationToken(text="▁", text_range=TextRange(start=5504, end=5505)),
        TokenizationToken(text="(", text_range=TextRange(start=5505, end=5506)),
        TokenizationToken(text="CRF", text_range=TextRange(start=5506, end=5509)),
        TokenizationToken(text="M", text_range=TextRange(start=5509, end=5510)),
        TokenizationToken(text=")", text_range=TextRange(start=5510, end=5511)),
        TokenizationToken(text="▁is", text_range=TextRange(start=5511, end=5514)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=5514, end=5535)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=5535, end=5546)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=5546, end=5558)),
        TokenizationToken(text="▁the", text_range=TextRange(start=5558, end=5562)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=5562, end=5571)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=5571, end=5585)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=5585, end=5591)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=5591, end=5600)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=5600, end=5624)),
        TokenizationToken(text="▁", text_range=TextRange(start=5624, end=5625)),
        TokenizationToken(text="(", text_range=TextRange(start=5625, end=5626)),
        TokenizationToken(text="HAI", text_range=TextRange(start=5626, end=5629)),
        TokenizationToken(text=")", text_range=TextRange(start=5629, end=5630)),
        TokenizationToken(text="▁that", text_range=TextRange(start=5630, end=5635)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=5635, end=5648)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=5648, end=5660)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=5660, end=5672)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=5672, end=5682)),
        TokenizationToken(text=",", text_range=TextRange(start=5682, end=5683)),
        TokenizationToken(text="▁development", text_range=TextRange(start=5683, end=5695)),
        TokenizationToken(text=",", text_range=TextRange(start=5695, end=5696)),
        TokenizationToken(text="▁and", text_range=TextRange(start=5696, end=5700)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=5700, end=5714)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=5714, end=5725)),
        TokenizationToken(text="▁models", text_range=TextRange(start=5725, end=5732)),
        TokenizationToken(text=".", text_range=TextRange(start=5732, end=5733)),
        TokenizationToken(text="The", text_range=TextRange(start=5733, end=5736)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=5736, end=5747)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=5747, end=5759)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=5759, end=5770)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=5770, end=5777)),
        TokenizationToken(text="▁", text_range=TextRange(start=5777, end=5778)),
        TokenizationToken(text="(", text_range=TextRange(start=5778, end=5779)),
        TokenizationToken(text="CRF", text_range=TextRange(start=5779, end=5782)),
        TokenizationToken(text="M", text_range=TextRange(start=5782, end=5783)),
        TokenizationToken(text=")", text_range=TextRange(start=5783, end=5784)),
        TokenizationToken(text="▁is", text_range=TextRange(start=5784, end=5787)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=5787, end=5808)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=5808, end=5819)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=5819, end=5831)),
        TokenizationToken(text="▁the", text_range=TextRange(start=5831, end=5835)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=5835, end=5844)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=5844, end=5858)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=5858, end=5864)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=5864, end=5873)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=5873, end=5897)),
        TokenizationToken(text="▁", text_range=TextRange(start=5897, end=5898)),
        TokenizationToken(text="(", text_range=TextRange(start=5898, end=5899)),
        TokenizationToken(text="HAI", text_range=TextRange(start=5899, end=5902)),
        TokenizationToken(text=")", text_range=TextRange(start=5902, end=5903)),
        TokenizationToken(text="▁that", text_range=TextRange(start=5903, end=5908)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=5908, end=5921)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=5921, end=5933)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=5933, end=5945)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=5945, end=5955)),
        TokenizationToken(text=",", text_range=TextRange(start=5955, end=5956)),
        TokenizationToken(text="▁development", text_range=TextRange(start=5956, end=5968)),
        TokenizationToken(text=",", text_range=TextRange(start=5968, end=5969)),
        TokenizationToken(text="▁and", text_range=TextRange(start=5969, end=5973)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=5973, end=5987)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=5987, end=5998)),
        TokenizationToken(text="▁models", text_range=TextRange(start=5998, end=6005)),
        TokenizationToken(text=".", text_range=TextRange(start=6005, end=6006)),
        TokenizationToken(text="The", text_range=TextRange(start=6006, end=6009)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=6009, end=6020)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=6020, end=6032)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=6032, end=6043)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=6043, end=6050)),
        TokenizationToken(text="▁", text_range=TextRange(start=6050, end=6051)),
        TokenizationToken(text="(", text_range=TextRange(start=6051, end=6052)),
        TokenizationToken(text="CRF", text_range=TextRange(start=6052, end=6055)),
        TokenizationToken(text="M", text_range=TextRange(start=6055, end=6056)),
        TokenizationToken(text=")", text_range=TextRange(start=6056, end=6057)),
        TokenizationToken(text="▁is", text_range=TextRange(start=6057, end=6060)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=6060, end=6081)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=6081, end=6092)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=6092, end=6104)),
        TokenizationToken(text="▁the", text_range=TextRange(start=6104, end=6108)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=6108, end=6117)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=6117, end=6131)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=6131, end=6137)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=6137, end=6146)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=6146, end=6170)),
        TokenizationToken(text="▁", text_range=TextRange(start=6170, end=6171)),
        TokenizationToken(text="(", text_range=TextRange(start=6171, end=6172)),
        TokenizationToken(text="HAI", text_range=TextRange(start=6172, end=6175)),
        TokenizationToken(text=")", text_range=TextRange(start=6175, end=6176)),
        TokenizationToken(text="▁that", text_range=TextRange(start=6176, end=6181)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=6181, end=6194)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=6194, end=6206)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=6206, end=6218)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=6218, end=6228)),
        TokenizationToken(text=",", text_range=TextRange(start=6228, end=6229)),
        TokenizationToken(text="▁development", text_range=TextRange(start=6229, end=6241)),
        TokenizationToken(text=",", text_range=TextRange(start=6241, end=6242)),
        TokenizationToken(text="▁and", text_range=TextRange(start=6242, end=6246)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=6246, end=6260)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=6260, end=6271)),
        TokenizationToken(text="▁models", text_range=TextRange(start=6271, end=6278)),
        TokenizationToken(text=".", text_range=TextRange(start=6278, end=6279)),
        TokenizationToken(text="The", text_range=TextRange(start=6279, end=6282)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=6282, end=6293)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=6293, end=6305)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=6305, end=6316)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=6316, end=6323)),
        TokenizationToken(text="▁", text_range=TextRange(start=6323, end=6324)),
        TokenizationToken(text="(", text_range=TextRange(start=6324, end=6325)),
        TokenizationToken(text="CRF", text_range=TextRange(start=6325, end=6328)),
        TokenizationToken(text="M", text_range=TextRange(start=6328, end=6329)),
        TokenizationToken(text=")", text_range=TextRange(start=6329, end=6330)),
        TokenizationToken(text="▁is", text_range=TextRange(start=6330, end=6333)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=6333, end=6354)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=6354, end=6365)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=6365, end=6377)),
        TokenizationToken(text="▁the", text_range=TextRange(start=6377, end=6381)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=6381, end=6390)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=6390, end=6404)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=6404, end=6410)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=6410, end=6419)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=6419, end=6443)),
        TokenizationToken(text="▁", text_range=TextRange(start=6443, end=6444)),
        TokenizationToken(text="(", text_range=TextRange(start=6444, end=6445)),
        TokenizationToken(text="HAI", text_range=TextRange(start=6445, end=6448)),
        TokenizationToken(text=")", text_range=TextRange(start=6448, end=6449)),
        TokenizationToken(text="▁that", text_range=TextRange(start=6449, end=6454)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=6454, end=6467)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=6467, end=6479)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=6479, end=6491)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=6491, end=6501)),
        TokenizationToken(text=",", text_range=TextRange(start=6501, end=6502)),
        TokenizationToken(text="▁development", text_range=TextRange(start=6502, end=6514)),
        TokenizationToken(text=",", text_range=TextRange(start=6514, end=6515)),
        TokenizationToken(text="▁and", text_range=TextRange(start=6515, end=6519)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=6519, end=6533)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=6533, end=6544)),
        TokenizationToken(text="▁models", text_range=TextRange(start=6544, end=6551)),
        TokenizationToken(text=".", text_range=TextRange(start=6551, end=6552)),
        TokenizationToken(text="The", text_range=TextRange(start=6552, end=6555)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=6555, end=6566)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=6566, end=6578)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=6578, end=6589)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=6589, end=6596)),
        TokenizationToken(text="▁", text_range=TextRange(start=6596, end=6597)),
        TokenizationToken(text="(", text_range=TextRange(start=6597, end=6598)),
        TokenizationToken(text="CRF", text_range=TextRange(start=6598, end=6601)),
        TokenizationToken(text="M", text_range=TextRange(start=6601, end=6602)),
        TokenizationToken(text=")", text_range=TextRange(start=6602, end=6603)),
        TokenizationToken(text="▁is", text_range=TextRange(start=6603, end=6606)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=6606, end=6627)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=6627, end=6638)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=6638, end=6650)),
        TokenizationToken(text="▁the", text_range=TextRange(start=6650, end=6654)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=6654, end=6663)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=6663, end=6677)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=6677, end=6683)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=6683, end=6692)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=6692, end=6716)),
        TokenizationToken(text="▁", text_range=TextRange(start=6716, end=6717)),
        TokenizationToken(text="(", text_range=TextRange(start=6717, end=6718)),
        TokenizationToken(text="HAI", text_range=TextRange(start=6718, end=6721)),
        TokenizationToken(text=")", text_range=TextRange(start=6721, end=6722)),
        TokenizationToken(text="▁that", text_range=TextRange(start=6722, end=6727)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=6727, end=6740)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=6740, end=6752)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=6752, end=6764)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=6764, end=6774)),
        TokenizationToken(text=",", text_range=TextRange(start=6774, end=6775)),
        TokenizationToken(text="▁development", text_range=TextRange(start=6775, end=6787)),
        TokenizationToken(text=",", text_range=TextRange(start=6787, end=6788)),
        TokenizationToken(text="▁and", text_range=TextRange(start=6788, end=6792)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=6792, end=6806)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=6806, end=6817)),
        TokenizationToken(text="▁models", text_range=TextRange(start=6817, end=6824)),
        TokenizationToken(text=".", text_range=TextRange(start=6824, end=6825)),
        TokenizationToken(text="The", text_range=TextRange(start=6825, end=6828)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=6828, end=6839)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=6839, end=6851)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=6851, end=6862)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=6862, end=6869)),
        TokenizationToken(text="▁", text_range=TextRange(start=6869, end=6870)),
        TokenizationToken(text="(", text_range=TextRange(start=6870, end=6871)),
        TokenizationToken(text="CRF", text_range=TextRange(start=6871, end=6874)),
        TokenizationToken(text="M", text_range=TextRange(start=6874, end=6875)),
        TokenizationToken(text=")", text_range=TextRange(start=6875, end=6876)),
        TokenizationToken(text="▁is", text_range=TextRange(start=6876, end=6879)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=6879, end=6900)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=6900, end=6911)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=6911, end=6923)),
        TokenizationToken(text="▁the", text_range=TextRange(start=6923, end=6927)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=6927, end=6936)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=6936, end=6950)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=6950, end=6956)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=6956, end=6965)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=6965, end=6989)),
        TokenizationToken(text="▁", text_range=TextRange(start=6989, end=6990)),
        TokenizationToken(text="(", text_range=TextRange(start=6990, end=6991)),
        TokenizationToken(text="HAI", text_range=TextRange(start=6991, end=6994)),
        TokenizationToken(text=")", text_range=TextRange(start=6994, end=6995)),
        TokenizationToken(text="▁that", text_range=TextRange(start=6995, end=7000)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=7000, end=7013)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=7013, end=7025)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=7025, end=7037)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=7037, end=7047)),
        TokenizationToken(text=",", text_range=TextRange(start=7047, end=7048)),
        TokenizationToken(text="▁development", text_range=TextRange(start=7048, end=7060)),
        TokenizationToken(text=",", text_range=TextRange(start=7060, end=7061)),
        TokenizationToken(text="▁and", text_range=TextRange(start=7061, end=7065)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=7065, end=7079)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=7079, end=7090)),
        TokenizationToken(text="▁models", text_range=TextRange(start=7090, end=7097)),
        TokenizationToken(text=".", text_range=TextRange(start=7097, end=7098)),
        TokenizationToken(text="The", text_range=TextRange(start=7098, end=7101)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=7101, end=7112)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=7112, end=7124)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=7124, end=7135)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=7135, end=7142)),
        TokenizationToken(text="▁", text_range=TextRange(start=7142, end=7143)),
        TokenizationToken(text="(", text_range=TextRange(start=7143, end=7144)),
        TokenizationToken(text="CRF", text_range=TextRange(start=7144, end=7147)),
        TokenizationToken(text="M", text_range=TextRange(start=7147, end=7148)),
        TokenizationToken(text=")", text_range=TextRange(start=7148, end=7149)),
        TokenizationToken(text="▁is", text_range=TextRange(start=7149, end=7152)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=7152, end=7173)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=7173, end=7184)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=7184, end=7196)),
        TokenizationToken(text="▁the", text_range=TextRange(start=7196, end=7200)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=7200, end=7209)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=7209, end=7223)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=7223, end=7229)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=7229, end=7238)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=7238, end=7262)),
        TokenizationToken(text="▁", text_range=TextRange(start=7262, end=7263)),
        TokenizationToken(text="(", text_range=TextRange(start=7263, end=7264)),
        TokenizationToken(text="HAI", text_range=TextRange(start=7264, end=7267)),
        TokenizationToken(text=")", text_range=TextRange(start=7267, end=7268)),
        TokenizationToken(text="▁that", text_range=TextRange(start=7268, end=7273)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=7273, end=7286)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=7286, end=7298)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=7298, end=7310)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=7310, end=7320)),
        TokenizationToken(text=",", text_range=TextRange(start=7320, end=7321)),
        TokenizationToken(text="▁development", text_range=TextRange(start=7321, end=7333)),
        TokenizationToken(text=",", text_range=TextRange(start=7333, end=7334)),
        TokenizationToken(text="▁and", text_range=TextRange(start=7334, end=7338)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=7338, end=7352)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=7352, end=7363)),
        TokenizationToken(text="▁models", text_range=TextRange(start=7363, end=7370)),
        TokenizationToken(text=".", text_range=TextRange(start=7370, end=7371)),
        TokenizationToken(text="The", text_range=TextRange(start=7371, end=7374)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=7374, end=7385)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=7385, end=7397)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=7397, end=7408)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=7408, end=7415)),
        TokenizationToken(text="▁", text_range=TextRange(start=7415, end=7416)),
        TokenizationToken(text="(", text_range=TextRange(start=7416, end=7417)),
        TokenizationToken(text="CRF", text_range=TextRange(start=7417, end=7420)),
        TokenizationToken(text="M", text_range=TextRange(start=7420, end=7421)),
        TokenizationToken(text=")", text_range=TextRange(start=7421, end=7422)),
        TokenizationToken(text="▁is", text_range=TextRange(start=7422, end=7425)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=7425, end=7446)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=7446, end=7457)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=7457, end=7469)),
        TokenizationToken(text="▁the", text_range=TextRange(start=7469, end=7473)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=7473, end=7482)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=7482, end=7496)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=7496, end=7502)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=7502, end=7511)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=7511, end=7535)),
        TokenizationToken(text="▁", text_range=TextRange(start=7535, end=7536)),
        TokenizationToken(text="(", text_range=TextRange(start=7536, end=7537)),
        TokenizationToken(text="HAI", text_range=TextRange(start=7537, end=7540)),
        TokenizationToken(text=")", text_range=TextRange(start=7540, end=7541)),
        TokenizationToken(text="▁that", text_range=TextRange(start=7541, end=7546)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=7546, end=7559)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=7559, end=7571)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=7571, end=7583)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=7583, end=7593)),
        TokenizationToken(text=",", text_range=TextRange(start=7593, end=7594)),
        TokenizationToken(text="▁development", text_range=TextRange(start=7594, end=7606)),
        TokenizationToken(text=",", text_range=TextRange(start=7606, end=7607)),
        TokenizationToken(text="▁and", text_range=TextRange(start=7607, end=7611)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=7611, end=7625)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=7625, end=7636)),
        TokenizationToken(text="▁models", text_range=TextRange(start=7636, end=7643)),
        TokenizationToken(text=".", text_range=TextRange(start=7643, end=7644)),
        TokenizationToken(text="The", text_range=TextRange(start=7644, end=7647)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=7647, end=7658)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=7658, end=7670)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=7670, end=7681)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=7681, end=7688)),
        TokenizationToken(text="▁", text_range=TextRange(start=7688, end=7689)),
        TokenizationToken(text="(", text_range=TextRange(start=7689, end=7690)),
        TokenizationToken(text="CRF", text_range=TextRange(start=7690, end=7693)),
        TokenizationToken(text="M", text_range=TextRange(start=7693, end=7694)),
        TokenizationToken(text=")", text_range=TextRange(start=7694, end=7695)),
        TokenizationToken(text="▁is", text_range=TextRange(start=7695, end=7698)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=7698, end=7719)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=7719, end=7730)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=7730, end=7742)),
        TokenizationToken(text="▁the", text_range=TextRange(start=7742, end=7746)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=7746, end=7755)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=7755, end=7769)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=7769, end=7775)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=7775, end=7784)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=7784, end=7808)),
        TokenizationToken(text="▁", text_range=TextRange(start=7808, end=7809)),
        TokenizationToken(text="(", text_range=TextRange(start=7809, end=7810)),
        TokenizationToken(text="HAI", text_range=TextRange(start=7810, end=7813)),
        TokenizationToken(text=")", text_range=TextRange(start=7813, end=7814)),
        TokenizationToken(text="▁that", text_range=TextRange(start=7814, end=7819)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=7819, end=7832)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=7832, end=7844)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=7844, end=7856)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=7856, end=7866)),
        TokenizationToken(text=",", text_range=TextRange(start=7866, end=7867)),
        TokenizationToken(text="▁development", text_range=TextRange(start=7867, end=7879)),
        TokenizationToken(text=",", text_range=TextRange(start=7879, end=7880)),
        TokenizationToken(text="▁and", text_range=TextRange(start=7880, end=7884)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=7884, end=7898)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=7898, end=7909)),
        TokenizationToken(text="▁models", text_range=TextRange(start=7909, end=7916)),
        TokenizationToken(text=".", text_range=TextRange(start=7916, end=7917)),
        TokenizationToken(text="The", text_range=TextRange(start=7917, end=7920)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=7920, end=7931)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=7931, end=7943)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=7943, end=7954)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=7954, end=7961)),
        TokenizationToken(text="▁", text_range=TextRange(start=7961, end=7962)),
        TokenizationToken(text="(", text_range=TextRange(start=7962, end=7963)),
        TokenizationToken(text="CRF", text_range=TextRange(start=7963, end=7966)),
        TokenizationToken(text="M", text_range=TextRange(start=7966, end=7967)),
        TokenizationToken(text=")", text_range=TextRange(start=7967, end=7968)),
        TokenizationToken(text="▁is", text_range=TextRange(start=7968, end=7971)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=7971, end=7992)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=7992, end=8003)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=8003, end=8015)),
        TokenizationToken(text="▁the", text_range=TextRange(start=8015, end=8019)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=8019, end=8028)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=8028, end=8042)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=8042, end=8048)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=8048, end=8057)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=8057, end=8081)),
        TokenizationToken(text="▁", text_range=TextRange(start=8081, end=8082)),
        TokenizationToken(text="(", text_range=TextRange(start=8082, end=8083)),
        TokenizationToken(text="HAI", text_range=TextRange(start=8083, end=8086)),
        TokenizationToken(text=")", text_range=TextRange(start=8086, end=8087)),
        TokenizationToken(text="▁that", text_range=TextRange(start=8087, end=8092)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=8092, end=8105)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=8105, end=8117)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=8117, end=8129)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=8129, end=8139)),
        TokenizationToken(text=",", text_range=TextRange(start=8139, end=8140)),
        TokenizationToken(text="▁development", text_range=TextRange(start=8140, end=8152)),
        TokenizationToken(text=",", text_range=TextRange(start=8152, end=8153)),
        TokenizationToken(text="▁and", text_range=TextRange(start=8153, end=8157)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=8157, end=8171)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=8171, end=8182)),
        TokenizationToken(text="▁models", text_range=TextRange(start=8182, end=8189)),
        TokenizationToken(text=".", text_range=TextRange(start=8189, end=8190)),
        TokenizationToken(text="The", text_range=TextRange(start=8190, end=8193)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=8193, end=8204)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=8204, end=8216)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=8216, end=8227)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=8227, end=8234)),
        TokenizationToken(text="▁", text_range=TextRange(start=8234, end=8235)),
        TokenizationToken(text="(", text_range=TextRange(start=8235, end=8236)),
        TokenizationToken(text="CRF", text_range=TextRange(start=8236, end=8239)),
        TokenizationToken(text="M", text_range=TextRange(start=8239, end=8240)),
        TokenizationToken(text=")", text_range=TextRange(start=8240, end=8241)),
        TokenizationToken(text="▁is", text_range=TextRange(start=8241, end=8244)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=8244, end=8265)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=8265, end=8276)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=8276, end=8288)),
        TokenizationToken(text="▁the", text_range=TextRange(start=8288, end=8292)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=8292, end=8301)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=8301, end=8315)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=8315, end=8321)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=8321, end=8330)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=8330, end=8354)),
        TokenizationToken(text="▁", text_range=TextRange(start=8354, end=8355)),
        TokenizationToken(text="(", text_range=TextRange(start=8355, end=8356)),
        TokenizationToken(text="HAI", text_range=TextRange(start=8356, end=8359)),
        TokenizationToken(text=")", text_range=TextRange(start=8359, end=8360)),
        TokenizationToken(text="▁that", text_range=TextRange(start=8360, end=8365)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=8365, end=8378)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=8378, end=8390)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=8390, end=8402)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=8402, end=8412)),
        TokenizationToken(text=",", text_range=TextRange(start=8412, end=8413)),
        TokenizationToken(text="▁development", text_range=TextRange(start=8413, end=8425)),
        TokenizationToken(text=",", text_range=TextRange(start=8425, end=8426)),
        TokenizationToken(text="▁and", text_range=TextRange(start=8426, end=8430)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=8430, end=8444)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=8444, end=8455)),
        TokenizationToken(text="▁models", text_range=TextRange(start=8455, end=8462)),
        TokenizationToken(text=".", text_range=TextRange(start=8462, end=8463)),
        TokenizationToken(text="The", text_range=TextRange(start=8463, end=8466)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=8466, end=8477)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=8477, end=8489)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=8489, end=8500)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=8500, end=8507)),
        TokenizationToken(text="▁", text_range=TextRange(start=8507, end=8508)),
        TokenizationToken(text="(", text_range=TextRange(start=8508, end=8509)),
        TokenizationToken(text="CRF", text_range=TextRange(start=8509, end=8512)),
        TokenizationToken(text="M", text_range=TextRange(start=8512, end=8513)),
        TokenizationToken(text=")", text_range=TextRange(start=8513, end=8514)),
        TokenizationToken(text="▁is", text_range=TextRange(start=8514, end=8517)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=8517, end=8538)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=8538, end=8549)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=8549, end=8561)),
        TokenizationToken(text="▁the", text_range=TextRange(start=8561, end=8565)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=8565, end=8574)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=8574, end=8588)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=8588, end=8594)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=8594, end=8603)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=8603, end=8627)),
        TokenizationToken(text="▁", text_range=TextRange(start=8627, end=8628)),
        TokenizationToken(text="(", text_range=TextRange(start=8628, end=8629)),
        TokenizationToken(text="HAI", text_range=TextRange(start=8629, end=8632)),
        TokenizationToken(text=")", text_range=TextRange(start=8632, end=8633)),
        TokenizationToken(text="▁that", text_range=TextRange(start=8633, end=8638)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=8638, end=8651)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=8651, end=8663)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=8663, end=8675)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=8675, end=8685)),
        TokenizationToken(text=",", text_range=TextRange(start=8685, end=8686)),
        TokenizationToken(text="▁development", text_range=TextRange(start=8686, end=8698)),
        TokenizationToken(text=",", text_range=TextRange(start=8698, end=8699)),
        TokenizationToken(text="▁and", text_range=TextRange(start=8699, end=8703)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=8703, end=8717)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=8717, end=8728)),
        TokenizationToken(text="▁models", text_range=TextRange(start=8728, end=8735)),
        TokenizationToken(text=".", text_range=TextRange(start=8735, end=8736)),
        TokenizationToken(text="The", text_range=TextRange(start=8736, end=8739)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=8739, end=8750)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=8750, end=8762)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=8762, end=8773)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=8773, end=8780)),
        TokenizationToken(text="▁", text_range=TextRange(start=8780, end=8781)),
        TokenizationToken(text="(", text_range=TextRange(start=8781, end=8782)),
        TokenizationToken(text="CRF", text_range=TextRange(start=8782, end=8785)),
        TokenizationToken(text="M", text_range=TextRange(start=8785, end=8786)),
        TokenizationToken(text=")", text_range=TextRange(start=8786, end=8787)),
        TokenizationToken(text="▁is", text_range=TextRange(start=8787, end=8790)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=8790, end=8811)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=8811, end=8822)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=8822, end=8834)),
        TokenizationToken(text="▁the", text_range=TextRange(start=8834, end=8838)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=8838, end=8847)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=8847, end=8861)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=8861, end=8867)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=8867, end=8876)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=8876, end=8900)),
        TokenizationToken(text="▁", text_range=TextRange(start=8900, end=8901)),
        TokenizationToken(text="(", text_range=TextRange(start=8901, end=8902)),
        TokenizationToken(text="HAI", text_range=TextRange(start=8902, end=8905)),
        TokenizationToken(text=")", text_range=TextRange(start=8905, end=8906)),
        TokenizationToken(text="▁that", text_range=TextRange(start=8906, end=8911)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=8911, end=8924)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=8924, end=8936)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=8936, end=8948)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=8948, end=8958)),
        TokenizationToken(text=",", text_range=TextRange(start=8958, end=8959)),
        TokenizationToken(text="▁development", text_range=TextRange(start=8959, end=8971)),
        TokenizationToken(text=",", text_range=TextRange(start=8971, end=8972)),
        TokenizationToken(text="▁and", text_range=TextRange(start=8972, end=8976)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=8976, end=8990)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=8990, end=9001)),
        TokenizationToken(text="▁models", text_range=TextRange(start=9001, end=9008)),
        TokenizationToken(text=".", text_range=TextRange(start=9008, end=9009)),
        TokenizationToken(text="The", text_range=TextRange(start=9009, end=9012)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=9012, end=9023)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=9023, end=9035)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=9035, end=9046)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=9046, end=9053)),
        TokenizationToken(text="▁", text_range=TextRange(start=9053, end=9054)),
        TokenizationToken(text="(", text_range=TextRange(start=9054, end=9055)),
        TokenizationToken(text="CRF", text_range=TextRange(start=9055, end=9058)),
        TokenizationToken(text="M", text_range=TextRange(start=9058, end=9059)),
        TokenizationToken(text=")", text_range=TextRange(start=9059, end=9060)),
        TokenizationToken(text="▁is", text_range=TextRange(start=9060, end=9063)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=9063, end=9084)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=9084, end=9095)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=9095, end=9107)),
        TokenizationToken(text="▁the", text_range=TextRange(start=9107, end=9111)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=9111, end=9120)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=9120, end=9134)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=9134, end=9140)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=9140, end=9149)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=9149, end=9173)),
        TokenizationToken(text="▁", text_range=TextRange(start=9173, end=9174)),
        TokenizationToken(text="(", text_range=TextRange(start=9174, end=9175)),
        TokenizationToken(text="HAI", text_range=TextRange(start=9175, end=9178)),
        TokenizationToken(text=")", text_range=TextRange(start=9178, end=9179)),
        TokenizationToken(text="▁that", text_range=TextRange(start=9179, end=9184)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=9184, end=9197)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=9197, end=9209)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=9209, end=9221)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=9221, end=9231)),
        TokenizationToken(text=",", text_range=TextRange(start=9231, end=9232)),
        TokenizationToken(text="▁development", text_range=TextRange(start=9232, end=9244)),
        TokenizationToken(text=",", text_range=TextRange(start=9244, end=9245)),
        TokenizationToken(text="▁and", text_range=TextRange(start=9245, end=9249)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=9249, end=9263)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=9263, end=9274)),
        TokenizationToken(text="▁models", text_range=TextRange(start=9274, end=9281)),
        TokenizationToken(text=".", text_range=TextRange(start=9281, end=9282)),
        TokenizationToken(text="The", text_range=TextRange(start=9282, end=9285)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=9285, end=9296)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=9296, end=9308)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=9308, end=9319)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=9319, end=9326)),
        TokenizationToken(text="▁", text_range=TextRange(start=9326, end=9327)),
        TokenizationToken(text="(", text_range=TextRange(start=9327, end=9328)),
        TokenizationToken(text="CRF", text_range=TextRange(start=9328, end=9331)),
        TokenizationToken(text="M", text_range=TextRange(start=9331, end=9332)),
        TokenizationToken(text=")", text_range=TextRange(start=9332, end=9333)),
        TokenizationToken(text="▁is", text_range=TextRange(start=9333, end=9336)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=9336, end=9357)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=9357, end=9368)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=9368, end=9380)),
        TokenizationToken(text="▁the", text_range=TextRange(start=9380, end=9384)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=9384, end=9393)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=9393, end=9407)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=9407, end=9413)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=9413, end=9422)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=9422, end=9446)),
        TokenizationToken(text="▁", text_range=TextRange(start=9446, end=9447)),
        TokenizationToken(text="(", text_range=TextRange(start=9447, end=9448)),
        TokenizationToken(text="HAI", text_range=TextRange(start=9448, end=9451)),
        TokenizationToken(text=")", text_range=TextRange(start=9451, end=9452)),
        TokenizationToken(text="▁that", text_range=TextRange(start=9452, end=9457)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=9457, end=9470)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=9470, end=9482)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=9482, end=9494)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=9494, end=9504)),
        TokenizationToken(text=",", text_range=TextRange(start=9504, end=9505)),
        TokenizationToken(text="▁development", text_range=TextRange(start=9505, end=9517)),
        TokenizationToken(text=",", text_range=TextRange(start=9517, end=9518)),
        TokenizationToken(text="▁and", text_range=TextRange(start=9518, end=9522)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=9522, end=9536)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=9536, end=9547)),
        TokenizationToken(text="▁models", text_range=TextRange(start=9547, end=9554)),
        TokenizationToken(text=".", text_range=TextRange(start=9554, end=9555)),
        TokenizationToken(text="The", text_range=TextRange(start=9555, end=9558)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=9558, end=9569)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=9569, end=9581)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=9581, end=9592)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=9592, end=9599)),
        TokenizationToken(text="▁", text_range=TextRange(start=9599, end=9600)),
        TokenizationToken(text="(", text_range=TextRange(start=9600, end=9601)),
        TokenizationToken(text="CRF", text_range=TextRange(start=9601, end=9604)),
        TokenizationToken(text="M", text_range=TextRange(start=9604, end=9605)),
        TokenizationToken(text=")", text_range=TextRange(start=9605, end=9606)),
        TokenizationToken(text="▁is", text_range=TextRange(start=9606, end=9609)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=9609, end=9630)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=9630, end=9641)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=9641, end=9653)),
        TokenizationToken(text="▁the", text_range=TextRange(start=9653, end=9657)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=9657, end=9666)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=9666, end=9680)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=9680, end=9686)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=9686, end=9695)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=9695, end=9719)),
        TokenizationToken(text="▁", text_range=TextRange(start=9719, end=9720)),
        TokenizationToken(text="(", text_range=TextRange(start=9720, end=9721)),
        TokenizationToken(text="HAI", text_range=TextRange(start=9721, end=9724)),
        TokenizationToken(text=")", text_range=TextRange(start=9724, end=9725)),
        TokenizationToken(text="▁that", text_range=TextRange(start=9725, end=9730)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=9730, end=9743)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=9743, end=9755)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=9755, end=9767)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=9767, end=9777)),
        TokenizationToken(text=",", text_range=TextRange(start=9777, end=9778)),
        TokenizationToken(text="▁development", text_range=TextRange(start=9778, end=9790)),
        TokenizationToken(text=",", text_range=TextRange(start=9790, end=9791)),
        TokenizationToken(text="▁and", text_range=TextRange(start=9791, end=9795)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=9795, end=9809)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=9809, end=9820)),
        TokenizationToken(text="▁models", text_range=TextRange(start=9820, end=9827)),
        TokenizationToken(text=".", text_range=TextRange(start=9827, end=9828)),
        TokenizationToken(text="The", text_range=TextRange(start=9828, end=9831)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=9831, end=9842)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=9842, end=9854)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=9854, end=9865)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=9865, end=9872)),
        TokenizationToken(text="▁", text_range=TextRange(start=9872, end=9873)),
        TokenizationToken(text="(", text_range=TextRange(start=9873, end=9874)),
        TokenizationToken(text="CRF", text_range=TextRange(start=9874, end=9877)),
        TokenizationToken(text="M", text_range=TextRange(start=9877, end=9878)),
        TokenizationToken(text=")", text_range=TextRange(start=9878, end=9879)),
        TokenizationToken(text="▁is", text_range=TextRange(start=9879, end=9882)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=9882, end=9903)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=9903, end=9914)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=9914, end=9926)),
        TokenizationToken(text="▁the", text_range=TextRange(start=9926, end=9930)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=9930, end=9939)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=9939, end=9953)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=9953, end=9959)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=9959, end=9968)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=9968, end=9992)),
        TokenizationToken(text="▁", text_range=TextRange(start=9992, end=9993)),
        TokenizationToken(text="(", text_range=TextRange(start=9993, end=9994)),
        TokenizationToken(text="HAI", text_range=TextRange(start=9994, end=9997)),
        TokenizationToken(text=")", text_range=TextRange(start=9997, end=9998)),
        TokenizationToken(text="▁that", text_range=TextRange(start=9998, end=10003)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=10003, end=10016)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=10016, end=10028)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=10028, end=10040)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=10040, end=10050)),
        TokenizationToken(text=",", text_range=TextRange(start=10050, end=10051)),
        TokenizationToken(text="▁development", text_range=TextRange(start=10051, end=10063)),
        TokenizationToken(text=",", text_range=TextRange(start=10063, end=10064)),
        TokenizationToken(text="▁and", text_range=TextRange(start=10064, end=10068)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=10068, end=10082)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=10082, end=10093)),
        TokenizationToken(text="▁models", text_range=TextRange(start=10093, end=10100)),
        TokenizationToken(text=".", text_range=TextRange(start=10100, end=10101)),
        TokenizationToken(text="The", text_range=TextRange(start=10101, end=10104)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=10104, end=10115)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=10115, end=10127)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=10127, end=10138)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=10138, end=10145)),
        TokenizationToken(text="▁", text_range=TextRange(start=10145, end=10146)),
        TokenizationToken(text="(", text_range=TextRange(start=10146, end=10147)),
        TokenizationToken(text="CRF", text_range=TextRange(start=10147, end=10150)),
        TokenizationToken(text="M", text_range=TextRange(start=10150, end=10151)),
        TokenizationToken(text=")", text_range=TextRange(start=10151, end=10152)),
        TokenizationToken(text="▁is", text_range=TextRange(start=10152, end=10155)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=10155, end=10176)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=10176, end=10187)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=10187, end=10199)),
        TokenizationToken(text="▁the", text_range=TextRange(start=10199, end=10203)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=10203, end=10212)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=10212, end=10226)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=10226, end=10232)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=10232, end=10241)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=10241, end=10265)),
        TokenizationToken(text="▁", text_range=TextRange(start=10265, end=10266)),
        TokenizationToken(text="(", text_range=TextRange(start=10266, end=10267)),
        TokenizationToken(text="HAI", text_range=TextRange(start=10267, end=10270)),
        TokenizationToken(text=")", text_range=TextRange(start=10270, end=10271)),
        TokenizationToken(text="▁that", text_range=TextRange(start=10271, end=10276)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=10276, end=10289)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=10289, end=10301)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=10301, end=10313)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=10313, end=10323)),
        TokenizationToken(text=",", text_range=TextRange(start=10323, end=10324)),
        TokenizationToken(text="▁development", text_range=TextRange(start=10324, end=10336)),
        TokenizationToken(text=",", text_range=TextRange(start=10336, end=10337)),
        TokenizationToken(text="▁and", text_range=TextRange(start=10337, end=10341)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=10341, end=10355)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=10355, end=10366)),
        TokenizationToken(text="▁models", text_range=TextRange(start=10366, end=10373)),
        TokenizationToken(text=".", text_range=TextRange(start=10373, end=10374)),
        TokenizationToken(text="The", text_range=TextRange(start=10374, end=10377)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=10377, end=10388)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=10388, end=10400)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=10400, end=10411)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=10411, end=10418)),
        TokenizationToken(text="▁", text_range=TextRange(start=10418, end=10419)),
        TokenizationToken(text="(", text_range=TextRange(start=10419, end=10420)),
        TokenizationToken(text="CRF", text_range=TextRange(start=10420, end=10423)),
        TokenizationToken(text="M", text_range=TextRange(start=10423, end=10424)),
        TokenizationToken(text=")", text_range=TextRange(start=10424, end=10425)),
        TokenizationToken(text="▁is", text_range=TextRange(start=10425, end=10428)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=10428, end=10449)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=10449, end=10460)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=10460, end=10472)),
        TokenizationToken(text="▁the", text_range=TextRange(start=10472, end=10476)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=10476, end=10485)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=10485, end=10499)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=10499, end=10505)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=10505, end=10514)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=10514, end=10538)),
        TokenizationToken(text="▁", text_range=TextRange(start=10538, end=10539)),
        TokenizationToken(text="(", text_range=TextRange(start=10539, end=10540)),
        TokenizationToken(text="HAI", text_range=TextRange(start=10540, end=10543)),
        TokenizationToken(text=")", text_range=TextRange(start=10543, end=10544)),
        TokenizationToken(text="▁that", text_range=TextRange(start=10544, end=10549)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=10549, end=10562)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=10562, end=10574)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=10574, end=10586)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=10586, end=10596)),
        TokenizationToken(text=",", text_range=TextRange(start=10596, end=10597)),
        TokenizationToken(text="▁development", text_range=TextRange(start=10597, end=10609)),
        TokenizationToken(text=",", text_range=TextRange(start=10609, end=10610)),
        TokenizationToken(text="▁and", text_range=TextRange(start=10610, end=10614)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=10614, end=10628)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=10628, end=10639)),
        TokenizationToken(text="▁models", text_range=TextRange(start=10639, end=10646)),
        TokenizationToken(text=".", text_range=TextRange(start=10646, end=10647)),
        TokenizationToken(text="The", text_range=TextRange(start=10647, end=10650)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=10650, end=10661)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=10661, end=10673)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=10673, end=10684)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=10684, end=10691)),
        TokenizationToken(text="▁", text_range=TextRange(start=10691, end=10692)),
        TokenizationToken(text="(", text_range=TextRange(start=10692, end=10693)),
        TokenizationToken(text="CRF", text_range=TextRange(start=10693, end=10696)),
        TokenizationToken(text="M", text_range=TextRange(start=10696, end=10697)),
        TokenizationToken(text=")", text_range=TextRange(start=10697, end=10698)),
        TokenizationToken(text="▁is", text_range=TextRange(start=10698, end=10701)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=10701, end=10722)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=10722, end=10733)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=10733, end=10745)),
        TokenizationToken(text="▁the", text_range=TextRange(start=10745, end=10749)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=10749, end=10758)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=10758, end=10772)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=10772, end=10778)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=10778, end=10787)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=10787, end=10811)),
        TokenizationToken(text="▁", text_range=TextRange(start=10811, end=10812)),
        TokenizationToken(text="(", text_range=TextRange(start=10812, end=10813)),
        TokenizationToken(text="HAI", text_range=TextRange(start=10813, end=10816)),
        TokenizationToken(text=")", text_range=TextRange(start=10816, end=10817)),
        TokenizationToken(text="▁that", text_range=TextRange(start=10817, end=10822)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=10822, end=10835)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=10835, end=10847)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=10847, end=10859)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=10859, end=10869)),
        TokenizationToken(text=",", text_range=TextRange(start=10869, end=10870)),
        TokenizationToken(text="▁development", text_range=TextRange(start=10870, end=10882)),
        TokenizationToken(text=",", text_range=TextRange(start=10882, end=10883)),
        TokenizationToken(text="▁and", text_range=TextRange(start=10883, end=10887)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=10887, end=10901)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=10901, end=10912)),
        TokenizationToken(text="▁models", text_range=TextRange(start=10912, end=10919)),
        TokenizationToken(text=".", text_range=TextRange(start=10919, end=10920)),
        TokenizationToken(text="The", text_range=TextRange(start=10920, end=10923)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=10923, end=10934)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=10934, end=10946)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=10946, end=10957)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=10957, end=10964)),
        TokenizationToken(text="▁", text_range=TextRange(start=10964, end=10965)),
        TokenizationToken(text="(", text_range=TextRange(start=10965, end=10966)),
        TokenizationToken(text="CRF", text_range=TextRange(start=10966, end=10969)),
        TokenizationToken(text="M", text_range=TextRange(start=10969, end=10970)),
        TokenizationToken(text=")", text_range=TextRange(start=10970, end=10971)),
        TokenizationToken(text="▁is", text_range=TextRange(start=10971, end=10974)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=10974, end=10995)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=10995, end=11006)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=11006, end=11018)),
        TokenizationToken(text="▁the", text_range=TextRange(start=11018, end=11022)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=11022, end=11031)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=11031, end=11045)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=11045, end=11051)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=11051, end=11060)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=11060, end=11084)),
        TokenizationToken(text="▁", text_range=TextRange(start=11084, end=11085)),
        TokenizationToken(text="(", text_range=TextRange(start=11085, end=11086)),
        TokenizationToken(text="HAI", text_range=TextRange(start=11086, end=11089)),
        TokenizationToken(text=")", text_range=TextRange(start=11089, end=11090)),
        TokenizationToken(text="▁that", text_range=TextRange(start=11090, end=11095)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=11095, end=11108)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=11108, end=11120)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=11120, end=11132)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=11132, end=11142)),
        TokenizationToken(text=",", text_range=TextRange(start=11142, end=11143)),
        TokenizationToken(text="▁development", text_range=TextRange(start=11143, end=11155)),
        TokenizationToken(text=",", text_range=TextRange(start=11155, end=11156)),
        TokenizationToken(text="▁and", text_range=TextRange(start=11156, end=11160)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=11160, end=11174)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=11174, end=11185)),
        TokenizationToken(text="▁models", text_range=TextRange(start=11185, end=11192)),
        TokenizationToken(text=".", text_range=TextRange(start=11192, end=11193)),
        TokenizationToken(text="The", text_range=TextRange(start=11193, end=11196)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=11196, end=11207)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=11207, end=11219)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=11219, end=11230)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=11230, end=11237)),
        TokenizationToken(text="▁", text_range=TextRange(start=11237, end=11238)),
        TokenizationToken(text="(", text_range=TextRange(start=11238, end=11239)),
        TokenizationToken(text="CRF", text_range=TextRange(start=11239, end=11242)),
        TokenizationToken(text="M", text_range=TextRange(start=11242, end=11243)),
        TokenizationToken(text=")", text_range=TextRange(start=11243, end=11244)),
        TokenizationToken(text="▁is", text_range=TextRange(start=11244, end=11247)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=11247, end=11268)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=11268, end=11279)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=11279, end=11291)),
        TokenizationToken(text="▁the", text_range=TextRange(start=11291, end=11295)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=11295, end=11304)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=11304, end=11318)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=11318, end=11324)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=11324, end=11333)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=11333, end=11357)),
        TokenizationToken(text="▁", text_range=TextRange(start=11357, end=11358)),
        TokenizationToken(text="(", text_range=TextRange(start=11358, end=11359)),
        TokenizationToken(text="HAI", text_range=TextRange(start=11359, end=11362)),
        TokenizationToken(text=")", text_range=TextRange(start=11362, end=11363)),
        TokenizationToken(text="▁that", text_range=TextRange(start=11363, end=11368)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=11368, end=11381)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=11381, end=11393)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=11393, end=11405)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=11405, end=11415)),
        TokenizationToken(text=",", text_range=TextRange(start=11415, end=11416)),
        TokenizationToken(text="▁development", text_range=TextRange(start=11416, end=11428)),
        TokenizationToken(text=",", text_range=TextRange(start=11428, end=11429)),
        TokenizationToken(text="▁and", text_range=TextRange(start=11429, end=11433)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=11433, end=11447)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=11447, end=11458)),
        TokenizationToken(text="▁models", text_range=TextRange(start=11458, end=11465)),
        TokenizationToken(text=".", text_range=TextRange(start=11465, end=11466)),
        TokenizationToken(text="The", text_range=TextRange(start=11466, end=11469)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=11469, end=11480)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=11480, end=11492)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=11492, end=11503)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=11503, end=11510)),
        TokenizationToken(text="▁", text_range=TextRange(start=11510, end=11511)),
        TokenizationToken(text="(", text_range=TextRange(start=11511, end=11512)),
        TokenizationToken(text="CRF", text_range=TextRange(start=11512, end=11515)),
        TokenizationToken(text="M", text_range=TextRange(start=11515, end=11516)),
        TokenizationToken(text=")", text_range=TextRange(start=11516, end=11517)),
        TokenizationToken(text="▁is", text_range=TextRange(start=11517, end=11520)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=11520, end=11541)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=11541, end=11552)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=11552, end=11564)),
        TokenizationToken(text="▁the", text_range=TextRange(start=11564, end=11568)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=11568, end=11577)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=11577, end=11591)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=11591, end=11597)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=11597, end=11606)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=11606, end=11630)),
        TokenizationToken(text="▁", text_range=TextRange(start=11630, end=11631)),
        TokenizationToken(text="(", text_range=TextRange(start=11631, end=11632)),
        TokenizationToken(text="HAI", text_range=TextRange(start=11632, end=11635)),
        TokenizationToken(text=")", text_range=TextRange(start=11635, end=11636)),
        TokenizationToken(text="▁that", text_range=TextRange(start=11636, end=11641)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=11641, end=11654)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=11654, end=11666)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=11666, end=11678)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=11678, end=11688)),
        TokenizationToken(text=",", text_range=TextRange(start=11688, end=11689)),
        TokenizationToken(text="▁development", text_range=TextRange(start=11689, end=11701)),
        TokenizationToken(text=",", text_range=TextRange(start=11701, end=11702)),
        TokenizationToken(text="▁and", text_range=TextRange(start=11702, end=11706)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=11706, end=11720)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=11720, end=11731)),
        TokenizationToken(text="▁models", text_range=TextRange(start=11731, end=11738)),
        TokenizationToken(text=".", text_range=TextRange(start=11738, end=11739)),
        TokenizationToken(text="The", text_range=TextRange(start=11739, end=11742)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=11742, end=11753)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=11753, end=11765)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=11765, end=11776)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=11776, end=11783)),
        TokenizationToken(text="▁", text_range=TextRange(start=11783, end=11784)),
        TokenizationToken(text="(", text_range=TextRange(start=11784, end=11785)),
        TokenizationToken(text="CRF", text_range=TextRange(start=11785, end=11788)),
        TokenizationToken(text="M", text_range=TextRange(start=11788, end=11789)),
        TokenizationToken(text=")", text_range=TextRange(start=11789, end=11790)),
        TokenizationToken(text="▁is", text_range=TextRange(start=11790, end=11793)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=11793, end=11814)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=11814, end=11825)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=11825, end=11837)),
        TokenizationToken(text="▁the", text_range=TextRange(start=11837, end=11841)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=11841, end=11850)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=11850, end=11864)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=11864, end=11870)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=11870, end=11879)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=11879, end=11903)),
        TokenizationToken(text="▁", text_range=TextRange(start=11903, end=11904)),
        TokenizationToken(text="(", text_range=TextRange(start=11904, end=11905)),
        TokenizationToken(text="HAI", text_range=TextRange(start=11905, end=11908)),
        TokenizationToken(text=")", text_range=TextRange(start=11908, end=11909)),
        TokenizationToken(text="▁that", text_range=TextRange(start=11909, end=11914)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=11914, end=11927)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=11927, end=11939)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=11939, end=11951)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=11951, end=11961)),
        TokenizationToken(text=",", text_range=TextRange(start=11961, end=11962)),
        TokenizationToken(text="▁development", text_range=TextRange(start=11962, end=11974)),
        TokenizationToken(text=",", text_range=TextRange(start=11974, end=11975)),
        TokenizationToken(text="▁and", text_range=TextRange(start=11975, end=11979)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=11979, end=11993)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=11993, end=12004)),
        TokenizationToken(text="▁models", text_range=TextRange(start=12004, end=12011)),
        TokenizationToken(text=".", text_range=TextRange(start=12011, end=12012)),
        TokenizationToken(text="The", text_range=TextRange(start=12012, end=12015)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=12015, end=12026)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=12026, end=12038)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=12038, end=12049)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=12049, end=12056)),
        TokenizationToken(text="▁", text_range=TextRange(start=12056, end=12057)),
        TokenizationToken(text="(", text_range=TextRange(start=12057, end=12058)),
        TokenizationToken(text="CRF", text_range=TextRange(start=12058, end=12061)),
        TokenizationToken(text="M", text_range=TextRange(start=12061, end=12062)),
        TokenizationToken(text=")", text_range=TextRange(start=12062, end=12063)),
        TokenizationToken(text="▁is", text_range=TextRange(start=12063, end=12066)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=12066, end=12087)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=12087, end=12098)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=12098, end=12110)),
        TokenizationToken(text="▁the", text_range=TextRange(start=12110, end=12114)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=12114, end=12123)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=12123, end=12137)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=12137, end=12143)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=12143, end=12152)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=12152, end=12176)),
        TokenizationToken(text="▁", text_range=TextRange(start=12176, end=12177)),
        TokenizationToken(text="(", text_range=TextRange(start=12177, end=12178)),
        TokenizationToken(text="HAI", text_range=TextRange(start=12178, end=12181)),
        TokenizationToken(text=")", text_range=TextRange(start=12181, end=12182)),
        TokenizationToken(text="▁that", text_range=TextRange(start=12182, end=12187)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=12187, end=12200)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=12200, end=12212)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=12212, end=12224)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=12224, end=12234)),
        TokenizationToken(text=",", text_range=TextRange(start=12234, end=12235)),
        TokenizationToken(text="▁development", text_range=TextRange(start=12235, end=12247)),
        TokenizationToken(text=",", text_range=TextRange(start=12247, end=12248)),
        TokenizationToken(text="▁and", text_range=TextRange(start=12248, end=12252)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=12252, end=12266)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=12266, end=12277)),
        TokenizationToken(text="▁models", text_range=TextRange(start=12277, end=12284)),
        TokenizationToken(text=".", text_range=TextRange(start=12284, end=12285)),
        TokenizationToken(text="The", text_range=TextRange(start=12285, end=12288)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=12288, end=12299)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=12299, end=12311)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=12311, end=12322)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=12322, end=12329)),
        TokenizationToken(text="▁", text_range=TextRange(start=12329, end=12330)),
        TokenizationToken(text="(", text_range=TextRange(start=12330, end=12331)),
        TokenizationToken(text="CRF", text_range=TextRange(start=12331, end=12334)),
        TokenizationToken(text="M", text_range=TextRange(start=12334, end=12335)),
        TokenizationToken(text=")", text_range=TextRange(start=12335, end=12336)),
        TokenizationToken(text="▁is", text_range=TextRange(start=12336, end=12339)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=12339, end=12360)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=12360, end=12371)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=12371, end=12383)),
        TokenizationToken(text="▁the", text_range=TextRange(start=12383, end=12387)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=12387, end=12396)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=12396, end=12410)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=12410, end=12416)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=12416, end=12425)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=12425, end=12449)),
        TokenizationToken(text="▁", text_range=TextRange(start=12449, end=12450)),
        TokenizationToken(text="(", text_range=TextRange(start=12450, end=12451)),
        TokenizationToken(text="HAI", text_range=TextRange(start=12451, end=12454)),
        TokenizationToken(text=")", text_range=TextRange(start=12454, end=12455)),
        TokenizationToken(text="▁that", text_range=TextRange(start=12455, end=12460)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=12460, end=12473)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=12473, end=12485)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=12485, end=12497)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=12497, end=12507)),
        TokenizationToken(text=",", text_range=TextRange(start=12507, end=12508)),
        TokenizationToken(text="▁development", text_range=TextRange(start=12508, end=12520)),
        TokenizationToken(text=",", text_range=TextRange(start=12520, end=12521)),
        TokenizationToken(text="▁and", text_range=TextRange(start=12521, end=12525)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=12525, end=12539)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=12539, end=12550)),
        TokenizationToken(text="▁models", text_range=TextRange(start=12550, end=12557)),
        TokenizationToken(text=".", text_range=TextRange(start=12557, end=12558)),
        TokenizationToken(text="The", text_range=TextRange(start=12558, end=12561)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=12561, end=12572)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=12572, end=12584)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=12584, end=12595)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=12595, end=12602)),
        TokenizationToken(text="▁", text_range=TextRange(start=12602, end=12603)),
        TokenizationToken(text="(", text_range=TextRange(start=12603, end=12604)),
        TokenizationToken(text="CRF", text_range=TextRange(start=12604, end=12607)),
        TokenizationToken(text="M", text_range=TextRange(start=12607, end=12608)),
        TokenizationToken(text=")", text_range=TextRange(start=12608, end=12609)),
        TokenizationToken(text="▁is", text_range=TextRange(start=12609, end=12612)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=12612, end=12633)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=12633, end=12644)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=12644, end=12656)),
        TokenizationToken(text="▁the", text_range=TextRange(start=12656, end=12660)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=12660, end=12669)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=12669, end=12683)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=12683, end=12689)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=12689, end=12698)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=12698, end=12722)),
        TokenizationToken(text="▁", text_range=TextRange(start=12722, end=12723)),
        TokenizationToken(text="(", text_range=TextRange(start=12723, end=12724)),
        TokenizationToken(text="HAI", text_range=TextRange(start=12724, end=12727)),
        TokenizationToken(text=")", text_range=TextRange(start=12727, end=12728)),
        TokenizationToken(text="▁that", text_range=TextRange(start=12728, end=12733)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=12733, end=12746)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=12746, end=12758)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=12758, end=12770)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=12770, end=12780)),
        TokenizationToken(text=",", text_range=TextRange(start=12780, end=12781)),
        TokenizationToken(text="▁development", text_range=TextRange(start=12781, end=12793)),
        TokenizationToken(text=",", text_range=TextRange(start=12793, end=12794)),
        TokenizationToken(text="▁and", text_range=TextRange(start=12794, end=12798)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=12798, end=12812)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=12812, end=12823)),
        TokenizationToken(text="▁models", text_range=TextRange(start=12823, end=12830)),
        TokenizationToken(text=".", text_range=TextRange(start=12830, end=12831)),
        TokenizationToken(text="The", text_range=TextRange(start=12831, end=12834)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=12834, end=12845)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=12845, end=12857)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=12857, end=12868)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=12868, end=12875)),
        TokenizationToken(text="▁", text_range=TextRange(start=12875, end=12876)),
        TokenizationToken(text="(", text_range=TextRange(start=12876, end=12877)),
        TokenizationToken(text="CRF", text_range=TextRange(start=12877, end=12880)),
        TokenizationToken(text="M", text_range=TextRange(start=12880, end=12881)),
        TokenizationToken(text=")", text_range=TextRange(start=12881, end=12882)),
        TokenizationToken(text="▁is", text_range=TextRange(start=12882, end=12885)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=12885, end=12906)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=12906, end=12917)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=12917, end=12929)),
        TokenizationToken(text="▁the", text_range=TextRange(start=12929, end=12933)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=12933, end=12942)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=12942, end=12956)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=12956, end=12962)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=12962, end=12971)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=12971, end=12995)),
        TokenizationToken(text="▁", text_range=TextRange(start=12995, end=12996)),
        TokenizationToken(text="(", text_range=TextRange(start=12996, end=12997)),
        TokenizationToken(text="HAI", text_range=TextRange(start=12997, end=13000)),
        TokenizationToken(text=")", text_range=TextRange(start=13000, end=13001)),
        TokenizationToken(text="▁that", text_range=TextRange(start=13001, end=13006)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=13006, end=13019)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=13019, end=13031)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=13031, end=13043)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=13043, end=13053)),
        TokenizationToken(text=",", text_range=TextRange(start=13053, end=13054)),
        TokenizationToken(text="▁development", text_range=TextRange(start=13054, end=13066)),
        TokenizationToken(text=",", text_range=TextRange(start=13066, end=13067)),
        TokenizationToken(text="▁and", text_range=TextRange(start=13067, end=13071)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=13071, end=13085)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=13085, end=13096)),
        TokenizationToken(text="▁models", text_range=TextRange(start=13096, end=13103)),
        TokenizationToken(text=".", text_range=TextRange(start=13103, end=13104)),
        TokenizationToken(text="The", text_range=TextRange(start=13104, end=13107)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=13107, end=13118)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=13118, end=13130)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=13130, end=13141)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=13141, end=13148)),
        TokenizationToken(text="▁", text_range=TextRange(start=13148, end=13149)),
        TokenizationToken(text="(", text_range=TextRange(start=13149, end=13150)),
        TokenizationToken(text="CRF", text_range=TextRange(start=13150, end=13153)),
        TokenizationToken(text="M", text_range=TextRange(start=13153, end=13154)),
        TokenizationToken(text=")", text_range=TextRange(start=13154, end=13155)),
        TokenizationToken(text="▁is", text_range=TextRange(start=13155, end=13158)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=13158, end=13179)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=13179, end=13190)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=13190, end=13202)),
        TokenizationToken(text="▁the", text_range=TextRange(start=13202, end=13206)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=13206, end=13215)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=13215, end=13229)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=13229, end=13235)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=13235, end=13244)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=13244, end=13268)),
        TokenizationToken(text="▁", text_range=TextRange(start=13268, end=13269)),
        TokenizationToken(text="(", text_range=TextRange(start=13269, end=13270)),
        TokenizationToken(text="HAI", text_range=TextRange(start=13270, end=13273)),
        TokenizationToken(text=")", text_range=TextRange(start=13273, end=13274)),
        TokenizationToken(text="▁that", text_range=TextRange(start=13274, end=13279)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=13279, end=13292)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=13292, end=13304)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=13304, end=13316)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=13316, end=13326)),
        TokenizationToken(text=",", text_range=TextRange(start=13326, end=13327)),
        TokenizationToken(text="▁development", text_range=TextRange(start=13327, end=13339)),
        TokenizationToken(text=",", text_range=TextRange(start=13339, end=13340)),
        TokenizationToken(text="▁and", text_range=TextRange(start=13340, end=13344)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=13344, end=13358)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=13358, end=13369)),
        TokenizationToken(text="▁models", text_range=TextRange(start=13369, end=13376)),
        TokenizationToken(text=".", text_range=TextRange(start=13376, end=13377)),
        TokenizationToken(text="The", text_range=TextRange(start=13377, end=13380)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=13380, end=13391)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=13391, end=13403)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=13403, end=13414)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=13414, end=13421)),
        TokenizationToken(text="▁", text_range=TextRange(start=13421, end=13422)),
        TokenizationToken(text="(", text_range=TextRange(start=13422, end=13423)),
        TokenizationToken(text="CRF", text_range=TextRange(start=13423, end=13426)),
        TokenizationToken(text="M", text_range=TextRange(start=13426, end=13427)),
        TokenizationToken(text=")", text_range=TextRange(start=13427, end=13428)),
        TokenizationToken(text="▁is", text_range=TextRange(start=13428, end=13431)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=13431, end=13452)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=13452, end=13463)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=13463, end=13475)),
        TokenizationToken(text="▁the", text_range=TextRange(start=13475, end=13479)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=13479, end=13488)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=13488, end=13502)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=13502, end=13508)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=13508, end=13517)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=13517, end=13541)),
        TokenizationToken(text="▁", text_range=TextRange(start=13541, end=13542)),
        TokenizationToken(text="(", text_range=TextRange(start=13542, end=13543)),
        TokenizationToken(text="HAI", text_range=TextRange(start=13543, end=13546)),
        TokenizationToken(text=")", text_range=TextRange(start=13546, end=13547)),
        TokenizationToken(text="▁that", text_range=TextRange(start=13547, end=13552)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=13552, end=13565)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=13565, end=13577)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=13577, end=13589)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=13589, end=13599)),
        TokenizationToken(text=",", text_range=TextRange(start=13599, end=13600)),
        TokenizationToken(text="▁development", text_range=TextRange(start=13600, end=13612)),
        TokenizationToken(text=",", text_range=TextRange(start=13612, end=13613)),
        TokenizationToken(text="▁and", text_range=TextRange(start=13613, end=13617)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=13617, end=13631)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=13631, end=13642)),
        TokenizationToken(text="▁models", text_range=TextRange(start=13642, end=13649)),
        TokenizationToken(text=".", text_range=TextRange(start=13649, end=13650)),
        TokenizationToken(text="The", text_range=TextRange(start=13650, end=13653)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=13653, end=13664)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=13664, end=13676)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=13676, end=13687)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=13687, end=13694)),
        TokenizationToken(text="▁", text_range=TextRange(start=13694, end=13695)),
        TokenizationToken(text="(", text_range=TextRange(start=13695, end=13696)),
        TokenizationToken(text="CRF", text_range=TextRange(start=13696, end=13699)),
        TokenizationToken(text="M", text_range=TextRange(start=13699, end=13700)),
        TokenizationToken(text=")", text_range=TextRange(start=13700, end=13701)),
        TokenizationToken(text="▁is", text_range=TextRange(start=13701, end=13704)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=13704, end=13725)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=13725, end=13736)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=13736, end=13748)),
        TokenizationToken(text="▁the", text_range=TextRange(start=13748, end=13752)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=13752, end=13761)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=13761, end=13775)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=13775, end=13781)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=13781, end=13790)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=13790, end=13814)),
        TokenizationToken(text="▁", text_range=TextRange(start=13814, end=13815)),
        TokenizationToken(text="(", text_range=TextRange(start=13815, end=13816)),
        TokenizationToken(text="HAI", text_range=TextRange(start=13816, end=13819)),
        TokenizationToken(text=")", text_range=TextRange(start=13819, end=13820)),
        TokenizationToken(text="▁that", text_range=TextRange(start=13820, end=13825)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=13825, end=13838)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=13838, end=13850)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=13850, end=13862)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=13862, end=13872)),
        TokenizationToken(text=",", text_range=TextRange(start=13872, end=13873)),
        TokenizationToken(text="▁development", text_range=TextRange(start=13873, end=13885)),
        TokenizationToken(text=",", text_range=TextRange(start=13885, end=13886)),
        TokenizationToken(text="▁and", text_range=TextRange(start=13886, end=13890)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=13890, end=13904)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=13904, end=13915)),
        TokenizationToken(text="▁models", text_range=TextRange(start=13915, end=13922)),
        TokenizationToken(text=".", text_range=TextRange(start=13922, end=13923)),
        TokenizationToken(text="The", text_range=TextRange(start=13923, end=13926)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=13926, end=13937)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=13937, end=13949)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=13949, end=13960)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=13960, end=13967)),
        TokenizationToken(text="▁", text_range=TextRange(start=13967, end=13968)),
        TokenizationToken(text="(", text_range=TextRange(start=13968, end=13969)),
        TokenizationToken(text="CRF", text_range=TextRange(start=13969, end=13972)),
        TokenizationToken(text="M", text_range=TextRange(start=13972, end=13973)),
        TokenizationToken(text=")", text_range=TextRange(start=13973, end=13974)),
        TokenizationToken(text="▁is", text_range=TextRange(start=13974, end=13977)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=13977, end=13998)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=13998, end=14009)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=14009, end=14021)),
        TokenizationToken(text="▁the", text_range=TextRange(start=14021, end=14025)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=14025, end=14034)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=14034, end=14048)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=14048, end=14054)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=14054, end=14063)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=14063, end=14087)),
        TokenizationToken(text="▁", text_range=TextRange(start=14087, end=14088)),
        TokenizationToken(text="(", text_range=TextRange(start=14088, end=14089)),
        TokenizationToken(text="HAI", text_range=TextRange(start=14089, end=14092)),
        TokenizationToken(text=")", text_range=TextRange(start=14092, end=14093)),
        TokenizationToken(text="▁that", text_range=TextRange(start=14093, end=14098)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=14098, end=14111)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=14111, end=14123)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=14123, end=14135)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=14135, end=14145)),
        TokenizationToken(text=",", text_range=TextRange(start=14145, end=14146)),
        TokenizationToken(text="▁development", text_range=TextRange(start=14146, end=14158)),
        TokenizationToken(text=",", text_range=TextRange(start=14158, end=14159)),
        TokenizationToken(text="▁and", text_range=TextRange(start=14159, end=14163)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=14163, end=14177)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=14177, end=14188)),
        TokenizationToken(text="▁models", text_range=TextRange(start=14188, end=14195)),
        TokenizationToken(text=".", text_range=TextRange(start=14195, end=14196)),
        TokenizationToken(text="The", text_range=TextRange(start=14196, end=14199)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=14199, end=14210)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=14210, end=14222)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=14222, end=14233)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=14233, end=14240)),
        TokenizationToken(text="▁", text_range=TextRange(start=14240, end=14241)),
        TokenizationToken(text="(", text_range=TextRange(start=14241, end=14242)),
        TokenizationToken(text="CRF", text_range=TextRange(start=14242, end=14245)),
        TokenizationToken(text="M", text_range=TextRange(start=14245, end=14246)),
        TokenizationToken(text=")", text_range=TextRange(start=14246, end=14247)),
        TokenizationToken(text="▁is", text_range=TextRange(start=14247, end=14250)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=14250, end=14271)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=14271, end=14282)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=14282, end=14294)),
        TokenizationToken(text="▁the", text_range=TextRange(start=14294, end=14298)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=14298, end=14307)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=14307, end=14321)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=14321, end=14327)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=14327, end=14336)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=14336, end=14360)),
        TokenizationToken(text="▁", text_range=TextRange(start=14360, end=14361)),
        TokenizationToken(text="(", text_range=TextRange(start=14361, end=14362)),
        TokenizationToken(text="HAI", text_range=TextRange(start=14362, end=14365)),
        TokenizationToken(text=")", text_range=TextRange(start=14365, end=14366)),
        TokenizationToken(text="▁that", text_range=TextRange(start=14366, end=14371)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=14371, end=14384)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=14384, end=14396)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=14396, end=14408)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=14408, end=14418)),
        TokenizationToken(text=",", text_range=TextRange(start=14418, end=14419)),
        TokenizationToken(text="▁development", text_range=TextRange(start=14419, end=14431)),
        TokenizationToken(text=",", text_range=TextRange(start=14431, end=14432)),
        TokenizationToken(text="▁and", text_range=TextRange(start=14432, end=14436)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=14436, end=14450)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=14450, end=14461)),
        TokenizationToken(text="▁models", text_range=TextRange(start=14461, end=14468)),
        TokenizationToken(text=".", text_range=TextRange(start=14468, end=14469)),
        TokenizationToken(text="The", text_range=TextRange(start=14469, end=14472)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=14472, end=14483)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=14483, end=14495)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=14495, end=14506)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=14506, end=14513)),
        TokenizationToken(text="▁", text_range=TextRange(start=14513, end=14514)),
        TokenizationToken(text="(", text_range=TextRange(start=14514, end=14515)),
        TokenizationToken(text="CRF", text_range=TextRange(start=14515, end=14518)),
        TokenizationToken(text="M", text_range=TextRange(start=14518, end=14519)),
        TokenizationToken(text=")", text_range=TextRange(start=14519, end=14520)),
        TokenizationToken(text="▁is", text_range=TextRange(start=14520, end=14523)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=14523, end=14544)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=14544, end=14555)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=14555, end=14567)),
        TokenizationToken(text="▁the", text_range=TextRange(start=14567, end=14571)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=14571, end=14580)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=14580, end=14594)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=14594, end=14600)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=14600, end=14609)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=14609, end=14633)),
        TokenizationToken(text="▁", text_range=TextRange(start=14633, end=14634)),
        TokenizationToken(text="(", text_range=TextRange(start=14634, end=14635)),
        TokenizationToken(text="HAI", text_range=TextRange(start=14635, end=14638)),
        TokenizationToken(text=")", text_range=TextRange(start=14638, end=14639)),
        TokenizationToken(text="▁that", text_range=TextRange(start=14639, end=14644)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=14644, end=14657)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=14657, end=14669)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=14669, end=14681)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=14681, end=14691)),
        TokenizationToken(text=",", text_range=TextRange(start=14691, end=14692)),
        TokenizationToken(text="▁development", text_range=TextRange(start=14692, end=14704)),
        TokenizationToken(text=",", text_range=TextRange(start=14704, end=14705)),
        TokenizationToken(text="▁and", text_range=TextRange(start=14705, end=14709)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=14709, end=14723)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=14723, end=14734)),
        TokenizationToken(text="▁models", text_range=TextRange(start=14734, end=14741)),
        TokenizationToken(text=".", text_range=TextRange(start=14741, end=14742)),
        TokenizationToken(text="The", text_range=TextRange(start=14742, end=14745)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=14745, end=14756)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=14756, end=14768)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=14768, end=14779)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=14779, end=14786)),
        TokenizationToken(text="▁", text_range=TextRange(start=14786, end=14787)),
        TokenizationToken(text="(", text_range=TextRange(start=14787, end=14788)),
        TokenizationToken(text="CRF", text_range=TextRange(start=14788, end=14791)),
        TokenizationToken(text="M", text_range=TextRange(start=14791, end=14792)),
        TokenizationToken(text=")", text_range=TextRange(start=14792, end=14793)),
        TokenizationToken(text="▁is", text_range=TextRange(start=14793, end=14796)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=14796, end=14817)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=14817, end=14828)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=14828, end=14840)),
        TokenizationToken(text="▁the", text_range=TextRange(start=14840, end=14844)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=14844, end=14853)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=14853, end=14867)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=14867, end=14873)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=14873, end=14882)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=14882, end=14906)),
        TokenizationToken(text="▁", text_range=TextRange(start=14906, end=14907)),
        TokenizationToken(text="(", text_range=TextRange(start=14907, end=14908)),
        TokenizationToken(text="HAI", text_range=TextRange(start=14908, end=14911)),
        TokenizationToken(text=")", text_range=TextRange(start=14911, end=14912)),
        TokenizationToken(text="▁that", text_range=TextRange(start=14912, end=14917)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=14917, end=14930)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=14930, end=14942)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=14942, end=14954)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=14954, end=14964)),
        TokenizationToken(text=",", text_range=TextRange(start=14964, end=14965)),
        TokenizationToken(text="▁development", text_range=TextRange(start=14965, end=14977)),
        TokenizationToken(text=",", text_range=TextRange(start=14977, end=14978)),
        TokenizationToken(text="▁and", text_range=TextRange(start=14978, end=14982)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=14982, end=14996)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=14996, end=15007)),
        TokenizationToken(text="▁models", text_range=TextRange(start=15007, end=15014)),
        TokenizationToken(text=".", text_range=TextRange(start=15014, end=15015)),
        TokenizationToken(text="The", text_range=TextRange(start=15015, end=15018)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=15018, end=15029)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=15029, end=15041)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=15041, end=15052)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=15052, end=15059)),
        TokenizationToken(text="▁", text_range=TextRange(start=15059, end=15060)),
        TokenizationToken(text="(", text_range=TextRange(start=15060, end=15061)),
        TokenizationToken(text="CRF", text_range=TextRange(start=15061, end=15064)),
        TokenizationToken(text="M", text_range=TextRange(start=15064, end=15065)),
        TokenizationToken(text=")", text_range=TextRange(start=15065, end=15066)),
        TokenizationToken(text="▁is", text_range=TextRange(start=15066, end=15069)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=15069, end=15090)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=15090, end=15101)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=15101, end=15113)),
        TokenizationToken(text="▁the", text_range=TextRange(start=15113, end=15117)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=15117, end=15126)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=15126, end=15140)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=15140, end=15146)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=15146, end=15155)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=15155, end=15179)),
        TokenizationToken(text="▁", text_range=TextRange(start=15179, end=15180)),
        TokenizationToken(text="(", text_range=TextRange(start=15180, end=15181)),
        TokenizationToken(text="HAI", text_range=TextRange(start=15181, end=15184)),
        TokenizationToken(text=")", text_range=TextRange(start=15184, end=15185)),
        TokenizationToken(text="▁that", text_range=TextRange(start=15185, end=15190)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=15190, end=15203)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=15203, end=15215)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=15215, end=15227)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=15227, end=15237)),
        TokenizationToken(text=",", text_range=TextRange(start=15237, end=15238)),
        TokenizationToken(text="▁development", text_range=TextRange(start=15238, end=15250)),
        TokenizationToken(text=",", text_range=TextRange(start=15250, end=15251)),
        TokenizationToken(text="▁and", text_range=TextRange(start=15251, end=15255)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=15255, end=15269)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=15269, end=15280)),
        TokenizationToken(text="▁models", text_range=TextRange(start=15280, end=15287)),
        TokenizationToken(text=".", text_range=TextRange(start=15287, end=15288)),
        TokenizationToken(text="The", text_range=TextRange(start=15288, end=15291)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=15291, end=15302)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=15302, end=15314)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=15314, end=15325)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=15325, end=15332)),
        TokenizationToken(text="▁", text_range=TextRange(start=15332, end=15333)),
        TokenizationToken(text="(", text_range=TextRange(start=15333, end=15334)),
        TokenizationToken(text="CRF", text_range=TextRange(start=15334, end=15337)),
        TokenizationToken(text="M", text_range=TextRange(start=15337, end=15338)),
        TokenizationToken(text=")", text_range=TextRange(start=15338, end=15339)),
        TokenizationToken(text="▁is", text_range=TextRange(start=15339, end=15342)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=15342, end=15363)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=15363, end=15374)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=15374, end=15386)),
        TokenizationToken(text="▁the", text_range=TextRange(start=15386, end=15390)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=15390, end=15399)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=15399, end=15413)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=15413, end=15419)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=15419, end=15428)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=15428, end=15452)),
        TokenizationToken(text="▁", text_range=TextRange(start=15452, end=15453)),
        TokenizationToken(text="(", text_range=TextRange(start=15453, end=15454)),
        TokenizationToken(text="HAI", text_range=TextRange(start=15454, end=15457)),
        TokenizationToken(text=")", text_range=TextRange(start=15457, end=15458)),
        TokenizationToken(text="▁that", text_range=TextRange(start=15458, end=15463)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=15463, end=15476)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=15476, end=15488)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=15488, end=15500)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=15500, end=15510)),
        TokenizationToken(text=",", text_range=TextRange(start=15510, end=15511)),
        TokenizationToken(text="▁development", text_range=TextRange(start=15511, end=15523)),
        TokenizationToken(text=",", text_range=TextRange(start=15523, end=15524)),
        TokenizationToken(text="▁and", text_range=TextRange(start=15524, end=15528)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=15528, end=15542)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=15542, end=15553)),
        TokenizationToken(text="▁models", text_range=TextRange(start=15553, end=15560)),
        TokenizationToken(text=".", text_range=TextRange(start=15560, end=15561)),
    ],
)

TRUNCATED_REQUEST_RESULT = TokenizationRequestResult(
    cached=False,
    text="The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of the Stanford Institute for Human-Centered Artificial Intelligence (HAI) that aims to make fundamental advances in the study, development, and deployment of foundation models.The Center for Research on Foundation Models (CRFM) is an interdisciplinary initiative born out of",
    tokens=[
        TokenizationToken(text="▁The▁Center▁for", text_range=TextRange(start=0, end=14)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=14, end=26)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=26, end=37)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=37, end=44)),
        TokenizationToken(text="▁", text_range=TextRange(start=44, end=45)),
        TokenizationToken(text="(", text_range=TextRange(start=45, end=46)),
        TokenizationToken(text="CRF", text_range=TextRange(start=46, end=49)),
        TokenizationToken(text="M", text_range=TextRange(start=49, end=50)),
        TokenizationToken(text=")", text_range=TextRange(start=50, end=51)),
        TokenizationToken(text="▁is", text_range=TextRange(start=51, end=54)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=54, end=75)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=75, end=86)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=86, end=98)),
        TokenizationToken(text="▁the", text_range=TextRange(start=98, end=102)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=102, end=111)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=111, end=125)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=125, end=131)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=131, end=140)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=140, end=164)),
        TokenizationToken(text="▁", text_range=TextRange(start=164, end=165)),
        TokenizationToken(text="(", text_range=TextRange(start=165, end=166)),
        TokenizationToken(text="HAI", text_range=TextRange(start=166, end=169)),
        TokenizationToken(text=")", text_range=TextRange(start=169, end=170)),
        TokenizationToken(text="▁that", text_range=TextRange(start=170, end=175)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=175, end=188)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=188, end=200)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=200, end=212)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=212, end=222)),
        TokenizationToken(text=",", text_range=TextRange(start=222, end=223)),
        TokenizationToken(text="▁development", text_range=TextRange(start=223, end=235)),
        TokenizationToken(text=",", text_range=TextRange(start=235, end=236)),
        TokenizationToken(text="▁and", text_range=TextRange(start=236, end=240)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=240, end=254)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=254, end=265)),
        TokenizationToken(text="▁models", text_range=TextRange(start=265, end=272)),
        TokenizationToken(text=".", text_range=TextRange(start=272, end=273)),
        TokenizationToken(text="The", text_range=TextRange(start=273, end=276)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=276, end=287)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=287, end=299)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=299, end=310)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=310, end=317)),
        TokenizationToken(text="▁", text_range=TextRange(start=317, end=318)),
        TokenizationToken(text="(", text_range=TextRange(start=318, end=319)),
        TokenizationToken(text="CRF", text_range=TextRange(start=319, end=322)),
        TokenizationToken(text="M", text_range=TextRange(start=322, end=323)),
        TokenizationToken(text=")", text_range=TextRange(start=323, end=324)),
        TokenizationToken(text="▁is", text_range=TextRange(start=324, end=327)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=327, end=348)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=348, end=359)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=359, end=371)),
        TokenizationToken(text="▁the", text_range=TextRange(start=371, end=375)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=375, end=384)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=384, end=398)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=398, end=404)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=404, end=413)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=413, end=437)),
        TokenizationToken(text="▁", text_range=TextRange(start=437, end=438)),
        TokenizationToken(text="(", text_range=TextRange(start=438, end=439)),
        TokenizationToken(text="HAI", text_range=TextRange(start=439, end=442)),
        TokenizationToken(text=")", text_range=TextRange(start=442, end=443)),
        TokenizationToken(text="▁that", text_range=TextRange(start=443, end=448)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=448, end=461)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=461, end=473)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=473, end=485)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=485, end=495)),
        TokenizationToken(text=",", text_range=TextRange(start=495, end=496)),
        TokenizationToken(text="▁development", text_range=TextRange(start=496, end=508)),
        TokenizationToken(text=",", text_range=TextRange(start=508, end=509)),
        TokenizationToken(text="▁and", text_range=TextRange(start=509, end=513)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=513, end=527)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=527, end=538)),
        TokenizationToken(text="▁models", text_range=TextRange(start=538, end=545)),
        TokenizationToken(text=".", text_range=TextRange(start=545, end=546)),
        TokenizationToken(text="The", text_range=TextRange(start=546, end=549)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=549, end=560)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=560, end=572)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=572, end=583)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=583, end=590)),
        TokenizationToken(text="▁", text_range=TextRange(start=590, end=591)),
        TokenizationToken(text="(", text_range=TextRange(start=591, end=592)),
        TokenizationToken(text="CRF", text_range=TextRange(start=592, end=595)),
        TokenizationToken(text="M", text_range=TextRange(start=595, end=596)),
        TokenizationToken(text=")", text_range=TextRange(start=596, end=597)),
        TokenizationToken(text="▁is", text_range=TextRange(start=597, end=600)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=600, end=621)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=621, end=632)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=632, end=644)),
        TokenizationToken(text="▁the", text_range=TextRange(start=644, end=648)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=648, end=657)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=657, end=671)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=671, end=677)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=677, end=686)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=686, end=710)),
        TokenizationToken(text="▁", text_range=TextRange(start=710, end=711)),
        TokenizationToken(text="(", text_range=TextRange(start=711, end=712)),
        TokenizationToken(text="HAI", text_range=TextRange(start=712, end=715)),
        TokenizationToken(text=")", text_range=TextRange(start=715, end=716)),
        TokenizationToken(text="▁that", text_range=TextRange(start=716, end=721)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=721, end=734)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=734, end=746)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=746, end=758)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=758, end=768)),
        TokenizationToken(text=",", text_range=TextRange(start=768, end=769)),
        TokenizationToken(text="▁development", text_range=TextRange(start=769, end=781)),
        TokenizationToken(text=",", text_range=TextRange(start=781, end=782)),
        TokenizationToken(text="▁and", text_range=TextRange(start=782, end=786)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=786, end=800)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=800, end=811)),
        TokenizationToken(text="▁models", text_range=TextRange(start=811, end=818)),
        TokenizationToken(text=".", text_range=TextRange(start=818, end=819)),
        TokenizationToken(text="The", text_range=TextRange(start=819, end=822)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=822, end=833)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=833, end=845)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=845, end=856)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=856, end=863)),
        TokenizationToken(text="▁", text_range=TextRange(start=863, end=864)),
        TokenizationToken(text="(", text_range=TextRange(start=864, end=865)),
        TokenizationToken(text="CRF", text_range=TextRange(start=865, end=868)),
        TokenizationToken(text="M", text_range=TextRange(start=868, end=869)),
        TokenizationToken(text=")", text_range=TextRange(start=869, end=870)),
        TokenizationToken(text="▁is", text_range=TextRange(start=870, end=873)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=873, end=894)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=894, end=905)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=905, end=917)),
        TokenizationToken(text="▁the", text_range=TextRange(start=917, end=921)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=921, end=930)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=930, end=944)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=944, end=950)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=950, end=959)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=959, end=983)),
        TokenizationToken(text="▁", text_range=TextRange(start=983, end=984)),
        TokenizationToken(text="(", text_range=TextRange(start=984, end=985)),
        TokenizationToken(text="HAI", text_range=TextRange(start=985, end=988)),
        TokenizationToken(text=")", text_range=TextRange(start=988, end=989)),
        TokenizationToken(text="▁that", text_range=TextRange(start=989, end=994)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=994, end=1007)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=1007, end=1019)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=1019, end=1031)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=1031, end=1041)),
        TokenizationToken(text=",", text_range=TextRange(start=1041, end=1042)),
        TokenizationToken(text="▁development", text_range=TextRange(start=1042, end=1054)),
        TokenizationToken(text=",", text_range=TextRange(start=1054, end=1055)),
        TokenizationToken(text="▁and", text_range=TextRange(start=1055, end=1059)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=1059, end=1073)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=1073, end=1084)),
        TokenizationToken(text="▁models", text_range=TextRange(start=1084, end=1091)),
        TokenizationToken(text=".", text_range=TextRange(start=1091, end=1092)),
        TokenizationToken(text="The", text_range=TextRange(start=1092, end=1095)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=1095, end=1106)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=1106, end=1118)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=1118, end=1129)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=1129, end=1136)),
        TokenizationToken(text="▁", text_range=TextRange(start=1136, end=1137)),
        TokenizationToken(text="(", text_range=TextRange(start=1137, end=1138)),
        TokenizationToken(text="CRF", text_range=TextRange(start=1138, end=1141)),
        TokenizationToken(text="M", text_range=TextRange(start=1141, end=1142)),
        TokenizationToken(text=")", text_range=TextRange(start=1142, end=1143)),
        TokenizationToken(text="▁is", text_range=TextRange(start=1143, end=1146)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=1146, end=1167)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=1167, end=1178)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=1178, end=1190)),
        TokenizationToken(text="▁the", text_range=TextRange(start=1190, end=1194)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=1194, end=1203)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=1203, end=1217)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=1217, end=1223)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=1223, end=1232)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=1232, end=1256)),
        TokenizationToken(text="▁", text_range=TextRange(start=1256, end=1257)),
        TokenizationToken(text="(", text_range=TextRange(start=1257, end=1258)),
        TokenizationToken(text="HAI", text_range=TextRange(start=1258, end=1261)),
        TokenizationToken(text=")", text_range=TextRange(start=1261, end=1262)),
        TokenizationToken(text="▁that", text_range=TextRange(start=1262, end=1267)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=1267, end=1280)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=1280, end=1292)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=1292, end=1304)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=1304, end=1314)),
        TokenizationToken(text=",", text_range=TextRange(start=1314, end=1315)),
        TokenizationToken(text="▁development", text_range=TextRange(start=1315, end=1327)),
        TokenizationToken(text=",", text_range=TextRange(start=1327, end=1328)),
        TokenizationToken(text="▁and", text_range=TextRange(start=1328, end=1332)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=1332, end=1346)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=1346, end=1357)),
        TokenizationToken(text="▁models", text_range=TextRange(start=1357, end=1364)),
        TokenizationToken(text=".", text_range=TextRange(start=1364, end=1365)),
        TokenizationToken(text="The", text_range=TextRange(start=1365, end=1368)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=1368, end=1379)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=1379, end=1391)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=1391, end=1402)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=1402, end=1409)),
        TokenizationToken(text="▁", text_range=TextRange(start=1409, end=1410)),
        TokenizationToken(text="(", text_range=TextRange(start=1410, end=1411)),
        TokenizationToken(text="CRF", text_range=TextRange(start=1411, end=1414)),
        TokenizationToken(text="M", text_range=TextRange(start=1414, end=1415)),
        TokenizationToken(text=")", text_range=TextRange(start=1415, end=1416)),
        TokenizationToken(text="▁is", text_range=TextRange(start=1416, end=1419)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=1419, end=1440)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=1440, end=1451)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=1451, end=1463)),
        TokenizationToken(text="▁the", text_range=TextRange(start=1463, end=1467)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=1467, end=1476)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=1476, end=1490)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=1490, end=1496)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=1496, end=1505)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=1505, end=1529)),
        TokenizationToken(text="▁", text_range=TextRange(start=1529, end=1530)),
        TokenizationToken(text="(", text_range=TextRange(start=1530, end=1531)),
        TokenizationToken(text="HAI", text_range=TextRange(start=1531, end=1534)),
        TokenizationToken(text=")", text_range=TextRange(start=1534, end=1535)),
        TokenizationToken(text="▁that", text_range=TextRange(start=1535, end=1540)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=1540, end=1553)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=1553, end=1565)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=1565, end=1577)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=1577, end=1587)),
        TokenizationToken(text=",", text_range=TextRange(start=1587, end=1588)),
        TokenizationToken(text="▁development", text_range=TextRange(start=1588, end=1600)),
        TokenizationToken(text=",", text_range=TextRange(start=1600, end=1601)),
        TokenizationToken(text="▁and", text_range=TextRange(start=1601, end=1605)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=1605, end=1619)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=1619, end=1630)),
        TokenizationToken(text="▁models", text_range=TextRange(start=1630, end=1637)),
        TokenizationToken(text=".", text_range=TextRange(start=1637, end=1638)),
        TokenizationToken(text="The", text_range=TextRange(start=1638, end=1641)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=1641, end=1652)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=1652, end=1664)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=1664, end=1675)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=1675, end=1682)),
        TokenizationToken(text="▁", text_range=TextRange(start=1682, end=1683)),
        TokenizationToken(text="(", text_range=TextRange(start=1683, end=1684)),
        TokenizationToken(text="CRF", text_range=TextRange(start=1684, end=1687)),
        TokenizationToken(text="M", text_range=TextRange(start=1687, end=1688)),
        TokenizationToken(text=")", text_range=TextRange(start=1688, end=1689)),
        TokenizationToken(text="▁is", text_range=TextRange(start=1689, end=1692)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=1692, end=1713)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=1713, end=1724)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=1724, end=1736)),
        TokenizationToken(text="▁the", text_range=TextRange(start=1736, end=1740)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=1740, end=1749)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=1749, end=1763)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=1763, end=1769)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=1769, end=1778)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=1778, end=1802)),
        TokenizationToken(text="▁", text_range=TextRange(start=1802, end=1803)),
        TokenizationToken(text="(", text_range=TextRange(start=1803, end=1804)),
        TokenizationToken(text="HAI", text_range=TextRange(start=1804, end=1807)),
        TokenizationToken(text=")", text_range=TextRange(start=1807, end=1808)),
        TokenizationToken(text="▁that", text_range=TextRange(start=1808, end=1813)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=1813, end=1826)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=1826, end=1838)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=1838, end=1850)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=1850, end=1860)),
        TokenizationToken(text=",", text_range=TextRange(start=1860, end=1861)),
        TokenizationToken(text="▁development", text_range=TextRange(start=1861, end=1873)),
        TokenizationToken(text=",", text_range=TextRange(start=1873, end=1874)),
        TokenizationToken(text="▁and", text_range=TextRange(start=1874, end=1878)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=1878, end=1892)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=1892, end=1903)),
        TokenizationToken(text="▁models", text_range=TextRange(start=1903, end=1910)),
        TokenizationToken(text=".", text_range=TextRange(start=1910, end=1911)),
        TokenizationToken(text="The", text_range=TextRange(start=1911, end=1914)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=1914, end=1925)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=1925, end=1937)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=1937, end=1948)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=1948, end=1955)),
        TokenizationToken(text="▁", text_range=TextRange(start=1955, end=1956)),
        TokenizationToken(text="(", text_range=TextRange(start=1956, end=1957)),
        TokenizationToken(text="CRF", text_range=TextRange(start=1957, end=1960)),
        TokenizationToken(text="M", text_range=TextRange(start=1960, end=1961)),
        TokenizationToken(text=")", text_range=TextRange(start=1961, end=1962)),
        TokenizationToken(text="▁is", text_range=TextRange(start=1962, end=1965)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=1965, end=1986)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=1986, end=1997)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=1997, end=2009)),
        TokenizationToken(text="▁the", text_range=TextRange(start=2009, end=2013)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=2013, end=2022)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=2022, end=2036)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=2036, end=2042)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=2042, end=2051)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=2051, end=2075)),
        TokenizationToken(text="▁", text_range=TextRange(start=2075, end=2076)),
        TokenizationToken(text="(", text_range=TextRange(start=2076, end=2077)),
        TokenizationToken(text="HAI", text_range=TextRange(start=2077, end=2080)),
        TokenizationToken(text=")", text_range=TextRange(start=2080, end=2081)),
        TokenizationToken(text="▁that", text_range=TextRange(start=2081, end=2086)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=2086, end=2099)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=2099, end=2111)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=2111, end=2123)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=2123, end=2133)),
        TokenizationToken(text=",", text_range=TextRange(start=2133, end=2134)),
        TokenizationToken(text="▁development", text_range=TextRange(start=2134, end=2146)),
        TokenizationToken(text=",", text_range=TextRange(start=2146, end=2147)),
        TokenizationToken(text="▁and", text_range=TextRange(start=2147, end=2151)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=2151, end=2165)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=2165, end=2176)),
        TokenizationToken(text="▁models", text_range=TextRange(start=2176, end=2183)),
        TokenizationToken(text=".", text_range=TextRange(start=2183, end=2184)),
        TokenizationToken(text="The", text_range=TextRange(start=2184, end=2187)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=2187, end=2198)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=2198, end=2210)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=2210, end=2221)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=2221, end=2228)),
        TokenizationToken(text="▁", text_range=TextRange(start=2228, end=2229)),
        TokenizationToken(text="(", text_range=TextRange(start=2229, end=2230)),
        TokenizationToken(text="CRF", text_range=TextRange(start=2230, end=2233)),
        TokenizationToken(text="M", text_range=TextRange(start=2233, end=2234)),
        TokenizationToken(text=")", text_range=TextRange(start=2234, end=2235)),
        TokenizationToken(text="▁is", text_range=TextRange(start=2235, end=2238)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=2238, end=2259)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=2259, end=2270)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=2270, end=2282)),
        TokenizationToken(text="▁the", text_range=TextRange(start=2282, end=2286)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=2286, end=2295)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=2295, end=2309)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=2309, end=2315)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=2315, end=2324)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=2324, end=2348)),
        TokenizationToken(text="▁", text_range=TextRange(start=2348, end=2349)),
        TokenizationToken(text="(", text_range=TextRange(start=2349, end=2350)),
        TokenizationToken(text="HAI", text_range=TextRange(start=2350, end=2353)),
        TokenizationToken(text=")", text_range=TextRange(start=2353, end=2354)),
        TokenizationToken(text="▁that", text_range=TextRange(start=2354, end=2359)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=2359, end=2372)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=2372, end=2384)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=2384, end=2396)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=2396, end=2406)),
        TokenizationToken(text=",", text_range=TextRange(start=2406, end=2407)),
        TokenizationToken(text="▁development", text_range=TextRange(start=2407, end=2419)),
        TokenizationToken(text=",", text_range=TextRange(start=2419, end=2420)),
        TokenizationToken(text="▁and", text_range=TextRange(start=2420, end=2424)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=2424, end=2438)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=2438, end=2449)),
        TokenizationToken(text="▁models", text_range=TextRange(start=2449, end=2456)),
        TokenizationToken(text=".", text_range=TextRange(start=2456, end=2457)),
        TokenizationToken(text="The", text_range=TextRange(start=2457, end=2460)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=2460, end=2471)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=2471, end=2483)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=2483, end=2494)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=2494, end=2501)),
        TokenizationToken(text="▁", text_range=TextRange(start=2501, end=2502)),
        TokenizationToken(text="(", text_range=TextRange(start=2502, end=2503)),
        TokenizationToken(text="CRF", text_range=TextRange(start=2503, end=2506)),
        TokenizationToken(text="M", text_range=TextRange(start=2506, end=2507)),
        TokenizationToken(text=")", text_range=TextRange(start=2507, end=2508)),
        TokenizationToken(text="▁is", text_range=TextRange(start=2508, end=2511)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=2511, end=2532)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=2532, end=2543)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=2543, end=2555)),
        TokenizationToken(text="▁the", text_range=TextRange(start=2555, end=2559)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=2559, end=2568)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=2568, end=2582)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=2582, end=2588)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=2588, end=2597)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=2597, end=2621)),
        TokenizationToken(text="▁", text_range=TextRange(start=2621, end=2622)),
        TokenizationToken(text="(", text_range=TextRange(start=2622, end=2623)),
        TokenizationToken(text="HAI", text_range=TextRange(start=2623, end=2626)),
        TokenizationToken(text=")", text_range=TextRange(start=2626, end=2627)),
        TokenizationToken(text="▁that", text_range=TextRange(start=2627, end=2632)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=2632, end=2645)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=2645, end=2657)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=2657, end=2669)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=2669, end=2679)),
        TokenizationToken(text=",", text_range=TextRange(start=2679, end=2680)),
        TokenizationToken(text="▁development", text_range=TextRange(start=2680, end=2692)),
        TokenizationToken(text=",", text_range=TextRange(start=2692, end=2693)),
        TokenizationToken(text="▁and", text_range=TextRange(start=2693, end=2697)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=2697, end=2711)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=2711, end=2722)),
        TokenizationToken(text="▁models", text_range=TextRange(start=2722, end=2729)),
        TokenizationToken(text=".", text_range=TextRange(start=2729, end=2730)),
        TokenizationToken(text="The", text_range=TextRange(start=2730, end=2733)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=2733, end=2744)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=2744, end=2756)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=2756, end=2767)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=2767, end=2774)),
        TokenizationToken(text="▁", text_range=TextRange(start=2774, end=2775)),
        TokenizationToken(text="(", text_range=TextRange(start=2775, end=2776)),
        TokenizationToken(text="CRF", text_range=TextRange(start=2776, end=2779)),
        TokenizationToken(text="M", text_range=TextRange(start=2779, end=2780)),
        TokenizationToken(text=")", text_range=TextRange(start=2780, end=2781)),
        TokenizationToken(text="▁is", text_range=TextRange(start=2781, end=2784)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=2784, end=2805)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=2805, end=2816)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=2816, end=2828)),
        TokenizationToken(text="▁the", text_range=TextRange(start=2828, end=2832)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=2832, end=2841)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=2841, end=2855)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=2855, end=2861)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=2861, end=2870)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=2870, end=2894)),
        TokenizationToken(text="▁", text_range=TextRange(start=2894, end=2895)),
        TokenizationToken(text="(", text_range=TextRange(start=2895, end=2896)),
        TokenizationToken(text="HAI", text_range=TextRange(start=2896, end=2899)),
        TokenizationToken(text=")", text_range=TextRange(start=2899, end=2900)),
        TokenizationToken(text="▁that", text_range=TextRange(start=2900, end=2905)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=2905, end=2918)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=2918, end=2930)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=2930, end=2942)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=2942, end=2952)),
        TokenizationToken(text=",", text_range=TextRange(start=2952, end=2953)),
        TokenizationToken(text="▁development", text_range=TextRange(start=2953, end=2965)),
        TokenizationToken(text=",", text_range=TextRange(start=2965, end=2966)),
        TokenizationToken(text="▁and", text_range=TextRange(start=2966, end=2970)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=2970, end=2984)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=2984, end=2995)),
        TokenizationToken(text="▁models", text_range=TextRange(start=2995, end=3002)),
        TokenizationToken(text=".", text_range=TextRange(start=3002, end=3003)),
        TokenizationToken(text="The", text_range=TextRange(start=3003, end=3006)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=3006, end=3017)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=3017, end=3029)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=3029, end=3040)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=3040, end=3047)),
        TokenizationToken(text="▁", text_range=TextRange(start=3047, end=3048)),
        TokenizationToken(text="(", text_range=TextRange(start=3048, end=3049)),
        TokenizationToken(text="CRF", text_range=TextRange(start=3049, end=3052)),
        TokenizationToken(text="M", text_range=TextRange(start=3052, end=3053)),
        TokenizationToken(text=")", text_range=TextRange(start=3053, end=3054)),
        TokenizationToken(text="▁is", text_range=TextRange(start=3054, end=3057)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=3057, end=3078)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=3078, end=3089)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=3089, end=3101)),
        TokenizationToken(text="▁the", text_range=TextRange(start=3101, end=3105)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=3105, end=3114)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=3114, end=3128)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=3128, end=3134)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=3134, end=3143)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=3143, end=3167)),
        TokenizationToken(text="▁", text_range=TextRange(start=3167, end=3168)),
        TokenizationToken(text="(", text_range=TextRange(start=3168, end=3169)),
        TokenizationToken(text="HAI", text_range=TextRange(start=3169, end=3172)),
        TokenizationToken(text=")", text_range=TextRange(start=3172, end=3173)),
        TokenizationToken(text="▁that", text_range=TextRange(start=3173, end=3178)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=3178, end=3191)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=3191, end=3203)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=3203, end=3215)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=3215, end=3225)),
        TokenizationToken(text=",", text_range=TextRange(start=3225, end=3226)),
        TokenizationToken(text="▁development", text_range=TextRange(start=3226, end=3238)),
        TokenizationToken(text=",", text_range=TextRange(start=3238, end=3239)),
        TokenizationToken(text="▁and", text_range=TextRange(start=3239, end=3243)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=3243, end=3257)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=3257, end=3268)),
        TokenizationToken(text="▁models", text_range=TextRange(start=3268, end=3275)),
        TokenizationToken(text=".", text_range=TextRange(start=3275, end=3276)),
        TokenizationToken(text="The", text_range=TextRange(start=3276, end=3279)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=3279, end=3290)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=3290, end=3302)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=3302, end=3313)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=3313, end=3320)),
        TokenizationToken(text="▁", text_range=TextRange(start=3320, end=3321)),
        TokenizationToken(text="(", text_range=TextRange(start=3321, end=3322)),
        TokenizationToken(text="CRF", text_range=TextRange(start=3322, end=3325)),
        TokenizationToken(text="M", text_range=TextRange(start=3325, end=3326)),
        TokenizationToken(text=")", text_range=TextRange(start=3326, end=3327)),
        TokenizationToken(text="▁is", text_range=TextRange(start=3327, end=3330)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=3330, end=3351)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=3351, end=3362)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=3362, end=3374)),
        TokenizationToken(text="▁the", text_range=TextRange(start=3374, end=3378)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=3378, end=3387)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=3387, end=3401)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=3401, end=3407)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=3407, end=3416)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=3416, end=3440)),
        TokenizationToken(text="▁", text_range=TextRange(start=3440, end=3441)),
        TokenizationToken(text="(", text_range=TextRange(start=3441, end=3442)),
        TokenizationToken(text="HAI", text_range=TextRange(start=3442, end=3445)),
        TokenizationToken(text=")", text_range=TextRange(start=3445, end=3446)),
        TokenizationToken(text="▁that", text_range=TextRange(start=3446, end=3451)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=3451, end=3464)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=3464, end=3476)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=3476, end=3488)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=3488, end=3498)),
        TokenizationToken(text=",", text_range=TextRange(start=3498, end=3499)),
        TokenizationToken(text="▁development", text_range=TextRange(start=3499, end=3511)),
        TokenizationToken(text=",", text_range=TextRange(start=3511, end=3512)),
        TokenizationToken(text="▁and", text_range=TextRange(start=3512, end=3516)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=3516, end=3530)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=3530, end=3541)),
        TokenizationToken(text="▁models", text_range=TextRange(start=3541, end=3548)),
        TokenizationToken(text=".", text_range=TextRange(start=3548, end=3549)),
        TokenizationToken(text="The", text_range=TextRange(start=3549, end=3552)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=3552, end=3563)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=3563, end=3575)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=3575, end=3586)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=3586, end=3593)),
        TokenizationToken(text="▁", text_range=TextRange(start=3593, end=3594)),
        TokenizationToken(text="(", text_range=TextRange(start=3594, end=3595)),
        TokenizationToken(text="CRF", text_range=TextRange(start=3595, end=3598)),
        TokenizationToken(text="M", text_range=TextRange(start=3598, end=3599)),
        TokenizationToken(text=")", text_range=TextRange(start=3599, end=3600)),
        TokenizationToken(text="▁is", text_range=TextRange(start=3600, end=3603)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=3603, end=3624)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=3624, end=3635)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=3635, end=3647)),
        TokenizationToken(text="▁the", text_range=TextRange(start=3647, end=3651)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=3651, end=3660)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=3660, end=3674)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=3674, end=3680)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=3680, end=3689)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=3689, end=3713)),
        TokenizationToken(text="▁", text_range=TextRange(start=3713, end=3714)),
        TokenizationToken(text="(", text_range=TextRange(start=3714, end=3715)),
        TokenizationToken(text="HAI", text_range=TextRange(start=3715, end=3718)),
        TokenizationToken(text=")", text_range=TextRange(start=3718, end=3719)),
        TokenizationToken(text="▁that", text_range=TextRange(start=3719, end=3724)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=3724, end=3737)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=3737, end=3749)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=3749, end=3761)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=3761, end=3771)),
        TokenizationToken(text=",", text_range=TextRange(start=3771, end=3772)),
        TokenizationToken(text="▁development", text_range=TextRange(start=3772, end=3784)),
        TokenizationToken(text=",", text_range=TextRange(start=3784, end=3785)),
        TokenizationToken(text="▁and", text_range=TextRange(start=3785, end=3789)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=3789, end=3803)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=3803, end=3814)),
        TokenizationToken(text="▁models", text_range=TextRange(start=3814, end=3821)),
        TokenizationToken(text=".", text_range=TextRange(start=3821, end=3822)),
        TokenizationToken(text="The", text_range=TextRange(start=3822, end=3825)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=3825, end=3836)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=3836, end=3848)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=3848, end=3859)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=3859, end=3866)),
        TokenizationToken(text="▁", text_range=TextRange(start=3866, end=3867)),
        TokenizationToken(text="(", text_range=TextRange(start=3867, end=3868)),
        TokenizationToken(text="CRF", text_range=TextRange(start=3868, end=3871)),
        TokenizationToken(text="M", text_range=TextRange(start=3871, end=3872)),
        TokenizationToken(text=")", text_range=TextRange(start=3872, end=3873)),
        TokenizationToken(text="▁is", text_range=TextRange(start=3873, end=3876)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=3876, end=3897)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=3897, end=3908)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=3908, end=3920)),
        TokenizationToken(text="▁the", text_range=TextRange(start=3920, end=3924)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=3924, end=3933)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=3933, end=3947)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=3947, end=3953)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=3953, end=3962)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=3962, end=3986)),
        TokenizationToken(text="▁", text_range=TextRange(start=3986, end=3987)),
        TokenizationToken(text="(", text_range=TextRange(start=3987, end=3988)),
        TokenizationToken(text="HAI", text_range=TextRange(start=3988, end=3991)),
        TokenizationToken(text=")", text_range=TextRange(start=3991, end=3992)),
        TokenizationToken(text="▁that", text_range=TextRange(start=3992, end=3997)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=3997, end=4010)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=4010, end=4022)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=4022, end=4034)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=4034, end=4044)),
        TokenizationToken(text=",", text_range=TextRange(start=4044, end=4045)),
        TokenizationToken(text="▁development", text_range=TextRange(start=4045, end=4057)),
        TokenizationToken(text=",", text_range=TextRange(start=4057, end=4058)),
        TokenizationToken(text="▁and", text_range=TextRange(start=4058, end=4062)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=4062, end=4076)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=4076, end=4087)),
        TokenizationToken(text="▁models", text_range=TextRange(start=4087, end=4094)),
        TokenizationToken(text=".", text_range=TextRange(start=4094, end=4095)),
        TokenizationToken(text="The", text_range=TextRange(start=4095, end=4098)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=4098, end=4109)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=4109, end=4121)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=4121, end=4132)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=4132, end=4139)),
        TokenizationToken(text="▁", text_range=TextRange(start=4139, end=4140)),
        TokenizationToken(text="(", text_range=TextRange(start=4140, end=4141)),
        TokenizationToken(text="CRF", text_range=TextRange(start=4141, end=4144)),
        TokenizationToken(text="M", text_range=TextRange(start=4144, end=4145)),
        TokenizationToken(text=")", text_range=TextRange(start=4145, end=4146)),
        TokenizationToken(text="▁is", text_range=TextRange(start=4146, end=4149)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=4149, end=4170)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=4170, end=4181)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=4181, end=4193)),
        TokenizationToken(text="▁the", text_range=TextRange(start=4193, end=4197)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=4197, end=4206)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=4206, end=4220)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=4220, end=4226)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=4226, end=4235)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=4235, end=4259)),
        TokenizationToken(text="▁", text_range=TextRange(start=4259, end=4260)),
        TokenizationToken(text="(", text_range=TextRange(start=4260, end=4261)),
        TokenizationToken(text="HAI", text_range=TextRange(start=4261, end=4264)),
        TokenizationToken(text=")", text_range=TextRange(start=4264, end=4265)),
        TokenizationToken(text="▁that", text_range=TextRange(start=4265, end=4270)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=4270, end=4283)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=4283, end=4295)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=4295, end=4307)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=4307, end=4317)),
        TokenizationToken(text=",", text_range=TextRange(start=4317, end=4318)),
        TokenizationToken(text="▁development", text_range=TextRange(start=4318, end=4330)),
        TokenizationToken(text=",", text_range=TextRange(start=4330, end=4331)),
        TokenizationToken(text="▁and", text_range=TextRange(start=4331, end=4335)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=4335, end=4349)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=4349, end=4360)),
        TokenizationToken(text="▁models", text_range=TextRange(start=4360, end=4367)),
        TokenizationToken(text=".", text_range=TextRange(start=4367, end=4368)),
        TokenizationToken(text="The", text_range=TextRange(start=4368, end=4371)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=4371, end=4382)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=4382, end=4394)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=4394, end=4405)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=4405, end=4412)),
        TokenizationToken(text="▁", text_range=TextRange(start=4412, end=4413)),
        TokenizationToken(text="(", text_range=TextRange(start=4413, end=4414)),
        TokenizationToken(text="CRF", text_range=TextRange(start=4414, end=4417)),
        TokenizationToken(text="M", text_range=TextRange(start=4417, end=4418)),
        TokenizationToken(text=")", text_range=TextRange(start=4418, end=4419)),
        TokenizationToken(text="▁is", text_range=TextRange(start=4419, end=4422)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=4422, end=4443)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=4443, end=4454)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=4454, end=4466)),
        TokenizationToken(text="▁the", text_range=TextRange(start=4466, end=4470)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=4470, end=4479)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=4479, end=4493)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=4493, end=4499)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=4499, end=4508)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=4508, end=4532)),
        TokenizationToken(text="▁", text_range=TextRange(start=4532, end=4533)),
        TokenizationToken(text="(", text_range=TextRange(start=4533, end=4534)),
        TokenizationToken(text="HAI", text_range=TextRange(start=4534, end=4537)),
        TokenizationToken(text=")", text_range=TextRange(start=4537, end=4538)),
        TokenizationToken(text="▁that", text_range=TextRange(start=4538, end=4543)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=4543, end=4556)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=4556, end=4568)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=4568, end=4580)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=4580, end=4590)),
        TokenizationToken(text=",", text_range=TextRange(start=4590, end=4591)),
        TokenizationToken(text="▁development", text_range=TextRange(start=4591, end=4603)),
        TokenizationToken(text=",", text_range=TextRange(start=4603, end=4604)),
        TokenizationToken(text="▁and", text_range=TextRange(start=4604, end=4608)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=4608, end=4622)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=4622, end=4633)),
        TokenizationToken(text="▁models", text_range=TextRange(start=4633, end=4640)),
        TokenizationToken(text=".", text_range=TextRange(start=4640, end=4641)),
        TokenizationToken(text="The", text_range=TextRange(start=4641, end=4644)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=4644, end=4655)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=4655, end=4667)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=4667, end=4678)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=4678, end=4685)),
        TokenizationToken(text="▁", text_range=TextRange(start=4685, end=4686)),
        TokenizationToken(text="(", text_range=TextRange(start=4686, end=4687)),
        TokenizationToken(text="CRF", text_range=TextRange(start=4687, end=4690)),
        TokenizationToken(text="M", text_range=TextRange(start=4690, end=4691)),
        TokenizationToken(text=")", text_range=TextRange(start=4691, end=4692)),
        TokenizationToken(text="▁is", text_range=TextRange(start=4692, end=4695)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=4695, end=4716)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=4716, end=4727)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=4727, end=4739)),
        TokenizationToken(text="▁the", text_range=TextRange(start=4739, end=4743)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=4743, end=4752)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=4752, end=4766)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=4766, end=4772)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=4772, end=4781)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=4781, end=4805)),
        TokenizationToken(text="▁", text_range=TextRange(start=4805, end=4806)),
        TokenizationToken(text="(", text_range=TextRange(start=4806, end=4807)),
        TokenizationToken(text="HAI", text_range=TextRange(start=4807, end=4810)),
        TokenizationToken(text=")", text_range=TextRange(start=4810, end=4811)),
        TokenizationToken(text="▁that", text_range=TextRange(start=4811, end=4816)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=4816, end=4829)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=4829, end=4841)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=4841, end=4853)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=4853, end=4863)),
        TokenizationToken(text=",", text_range=TextRange(start=4863, end=4864)),
        TokenizationToken(text="▁development", text_range=TextRange(start=4864, end=4876)),
        TokenizationToken(text=",", text_range=TextRange(start=4876, end=4877)),
        TokenizationToken(text="▁and", text_range=TextRange(start=4877, end=4881)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=4881, end=4895)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=4895, end=4906)),
        TokenizationToken(text="▁models", text_range=TextRange(start=4906, end=4913)),
        TokenizationToken(text=".", text_range=TextRange(start=4913, end=4914)),
        TokenizationToken(text="The", text_range=TextRange(start=4914, end=4917)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=4917, end=4928)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=4928, end=4940)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=4940, end=4951)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=4951, end=4958)),
        TokenizationToken(text="▁", text_range=TextRange(start=4958, end=4959)),
        TokenizationToken(text="(", text_range=TextRange(start=4959, end=4960)),
        TokenizationToken(text="CRF", text_range=TextRange(start=4960, end=4963)),
        TokenizationToken(text="M", text_range=TextRange(start=4963, end=4964)),
        TokenizationToken(text=")", text_range=TextRange(start=4964, end=4965)),
        TokenizationToken(text="▁is", text_range=TextRange(start=4965, end=4968)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=4968, end=4989)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=4989, end=5000)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=5000, end=5012)),
        TokenizationToken(text="▁the", text_range=TextRange(start=5012, end=5016)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=5016, end=5025)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=5025, end=5039)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=5039, end=5045)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=5045, end=5054)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=5054, end=5078)),
        TokenizationToken(text="▁", text_range=TextRange(start=5078, end=5079)),
        TokenizationToken(text="(", text_range=TextRange(start=5079, end=5080)),
        TokenizationToken(text="HAI", text_range=TextRange(start=5080, end=5083)),
        TokenizationToken(text=")", text_range=TextRange(start=5083, end=5084)),
        TokenizationToken(text="▁that", text_range=TextRange(start=5084, end=5089)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=5089, end=5102)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=5102, end=5114)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=5114, end=5126)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=5126, end=5136)),
        TokenizationToken(text=",", text_range=TextRange(start=5136, end=5137)),
        TokenizationToken(text="▁development", text_range=TextRange(start=5137, end=5149)),
        TokenizationToken(text=",", text_range=TextRange(start=5149, end=5150)),
        TokenizationToken(text="▁and", text_range=TextRange(start=5150, end=5154)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=5154, end=5168)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=5168, end=5179)),
        TokenizationToken(text="▁models", text_range=TextRange(start=5179, end=5186)),
        TokenizationToken(text=".", text_range=TextRange(start=5186, end=5187)),
        TokenizationToken(text="The", text_range=TextRange(start=5187, end=5190)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=5190, end=5201)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=5201, end=5213)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=5213, end=5224)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=5224, end=5231)),
        TokenizationToken(text="▁", text_range=TextRange(start=5231, end=5232)),
        TokenizationToken(text="(", text_range=TextRange(start=5232, end=5233)),
        TokenizationToken(text="CRF", text_range=TextRange(start=5233, end=5236)),
        TokenizationToken(text="M", text_range=TextRange(start=5236, end=5237)),
        TokenizationToken(text=")", text_range=TextRange(start=5237, end=5238)),
        TokenizationToken(text="▁is", text_range=TextRange(start=5238, end=5241)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=5241, end=5262)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=5262, end=5273)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=5273, end=5285)),
        TokenizationToken(text="▁the", text_range=TextRange(start=5285, end=5289)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=5289, end=5298)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=5298, end=5312)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=5312, end=5318)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=5318, end=5327)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=5327, end=5351)),
        TokenizationToken(text="▁", text_range=TextRange(start=5351, end=5352)),
        TokenizationToken(text="(", text_range=TextRange(start=5352, end=5353)),
        TokenizationToken(text="HAI", text_range=TextRange(start=5353, end=5356)),
        TokenizationToken(text=")", text_range=TextRange(start=5356, end=5357)),
        TokenizationToken(text="▁that", text_range=TextRange(start=5357, end=5362)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=5362, end=5375)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=5375, end=5387)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=5387, end=5399)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=5399, end=5409)),
        TokenizationToken(text=",", text_range=TextRange(start=5409, end=5410)),
        TokenizationToken(text="▁development", text_range=TextRange(start=5410, end=5422)),
        TokenizationToken(text=",", text_range=TextRange(start=5422, end=5423)),
        TokenizationToken(text="▁and", text_range=TextRange(start=5423, end=5427)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=5427, end=5441)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=5441, end=5452)),
        TokenizationToken(text="▁models", text_range=TextRange(start=5452, end=5459)),
        TokenizationToken(text=".", text_range=TextRange(start=5459, end=5460)),
        TokenizationToken(text="The", text_range=TextRange(start=5460, end=5463)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=5463, end=5474)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=5474, end=5486)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=5486, end=5497)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=5497, end=5504)),
        TokenizationToken(text="▁", text_range=TextRange(start=5504, end=5505)),
        TokenizationToken(text="(", text_range=TextRange(start=5505, end=5506)),
        TokenizationToken(text="CRF", text_range=TextRange(start=5506, end=5509)),
        TokenizationToken(text="M", text_range=TextRange(start=5509, end=5510)),
        TokenizationToken(text=")", text_range=TextRange(start=5510, end=5511)),
        TokenizationToken(text="▁is", text_range=TextRange(start=5511, end=5514)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=5514, end=5535)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=5535, end=5546)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=5546, end=5558)),
        TokenizationToken(text="▁the", text_range=TextRange(start=5558, end=5562)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=5562, end=5571)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=5571, end=5585)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=5585, end=5591)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=5591, end=5600)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=5600, end=5624)),
        TokenizationToken(text="▁", text_range=TextRange(start=5624, end=5625)),
        TokenizationToken(text="(", text_range=TextRange(start=5625, end=5626)),
        TokenizationToken(text="HAI", text_range=TextRange(start=5626, end=5629)),
        TokenizationToken(text=")", text_range=TextRange(start=5629, end=5630)),
        TokenizationToken(text="▁that", text_range=TextRange(start=5630, end=5635)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=5635, end=5648)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=5648, end=5660)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=5660, end=5672)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=5672, end=5682)),
        TokenizationToken(text=",", text_range=TextRange(start=5682, end=5683)),
        TokenizationToken(text="▁development", text_range=TextRange(start=5683, end=5695)),
        TokenizationToken(text=",", text_range=TextRange(start=5695, end=5696)),
        TokenizationToken(text="▁and", text_range=TextRange(start=5696, end=5700)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=5700, end=5714)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=5714, end=5725)),
        TokenizationToken(text="▁models", text_range=TextRange(start=5725, end=5732)),
        TokenizationToken(text=".", text_range=TextRange(start=5732, end=5733)),
        TokenizationToken(text="The", text_range=TextRange(start=5733, end=5736)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=5736, end=5747)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=5747, end=5759)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=5759, end=5770)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=5770, end=5777)),
        TokenizationToken(text="▁", text_range=TextRange(start=5777, end=5778)),
        TokenizationToken(text="(", text_range=TextRange(start=5778, end=5779)),
        TokenizationToken(text="CRF", text_range=TextRange(start=5779, end=5782)),
        TokenizationToken(text="M", text_range=TextRange(start=5782, end=5783)),
        TokenizationToken(text=")", text_range=TextRange(start=5783, end=5784)),
        TokenizationToken(text="▁is", text_range=TextRange(start=5784, end=5787)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=5787, end=5808)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=5808, end=5819)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=5819, end=5831)),
        TokenizationToken(text="▁the", text_range=TextRange(start=5831, end=5835)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=5835, end=5844)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=5844, end=5858)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=5858, end=5864)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=5864, end=5873)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=5873, end=5897)),
        TokenizationToken(text="▁", text_range=TextRange(start=5897, end=5898)),
        TokenizationToken(text="(", text_range=TextRange(start=5898, end=5899)),
        TokenizationToken(text="HAI", text_range=TextRange(start=5899, end=5902)),
        TokenizationToken(text=")", text_range=TextRange(start=5902, end=5903)),
        TokenizationToken(text="▁that", text_range=TextRange(start=5903, end=5908)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=5908, end=5921)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=5921, end=5933)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=5933, end=5945)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=5945, end=5955)),
        TokenizationToken(text=",", text_range=TextRange(start=5955, end=5956)),
        TokenizationToken(text="▁development", text_range=TextRange(start=5956, end=5968)),
        TokenizationToken(text=",", text_range=TextRange(start=5968, end=5969)),
        TokenizationToken(text="▁and", text_range=TextRange(start=5969, end=5973)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=5973, end=5987)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=5987, end=5998)),
        TokenizationToken(text="▁models", text_range=TextRange(start=5998, end=6005)),
        TokenizationToken(text=".", text_range=TextRange(start=6005, end=6006)),
        TokenizationToken(text="The", text_range=TextRange(start=6006, end=6009)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=6009, end=6020)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=6020, end=6032)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=6032, end=6043)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=6043, end=6050)),
        TokenizationToken(text="▁", text_range=TextRange(start=6050, end=6051)),
        TokenizationToken(text="(", text_range=TextRange(start=6051, end=6052)),
        TokenizationToken(text="CRF", text_range=TextRange(start=6052, end=6055)),
        TokenizationToken(text="M", text_range=TextRange(start=6055, end=6056)),
        TokenizationToken(text=")", text_range=TextRange(start=6056, end=6057)),
        TokenizationToken(text="▁is", text_range=TextRange(start=6057, end=6060)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=6060, end=6081)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=6081, end=6092)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=6092, end=6104)),
        TokenizationToken(text="▁the", text_range=TextRange(start=6104, end=6108)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=6108, end=6117)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=6117, end=6131)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=6131, end=6137)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=6137, end=6146)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=6146, end=6170)),
        TokenizationToken(text="▁", text_range=TextRange(start=6170, end=6171)),
        TokenizationToken(text="(", text_range=TextRange(start=6171, end=6172)),
        TokenizationToken(text="HAI", text_range=TextRange(start=6172, end=6175)),
        TokenizationToken(text=")", text_range=TextRange(start=6175, end=6176)),
        TokenizationToken(text="▁that", text_range=TextRange(start=6176, end=6181)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=6181, end=6194)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=6194, end=6206)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=6206, end=6218)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=6218, end=6228)),
        TokenizationToken(text=",", text_range=TextRange(start=6228, end=6229)),
        TokenizationToken(text="▁development", text_range=TextRange(start=6229, end=6241)),
        TokenizationToken(text=",", text_range=TextRange(start=6241, end=6242)),
        TokenizationToken(text="▁and", text_range=TextRange(start=6242, end=6246)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=6246, end=6260)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=6260, end=6271)),
        TokenizationToken(text="▁models", text_range=TextRange(start=6271, end=6278)),
        TokenizationToken(text=".", text_range=TextRange(start=6278, end=6279)),
        TokenizationToken(text="The", text_range=TextRange(start=6279, end=6282)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=6282, end=6293)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=6293, end=6305)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=6305, end=6316)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=6316, end=6323)),
        TokenizationToken(text="▁", text_range=TextRange(start=6323, end=6324)),
        TokenizationToken(text="(", text_range=TextRange(start=6324, end=6325)),
        TokenizationToken(text="CRF", text_range=TextRange(start=6325, end=6328)),
        TokenizationToken(text="M", text_range=TextRange(start=6328, end=6329)),
        TokenizationToken(text=")", text_range=TextRange(start=6329, end=6330)),
        TokenizationToken(text="▁is", text_range=TextRange(start=6330, end=6333)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=6333, end=6354)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=6354, end=6365)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=6365, end=6377)),
        TokenizationToken(text="▁the", text_range=TextRange(start=6377, end=6381)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=6381, end=6390)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=6390, end=6404)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=6404, end=6410)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=6410, end=6419)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=6419, end=6443)),
        TokenizationToken(text="▁", text_range=TextRange(start=6443, end=6444)),
        TokenizationToken(text="(", text_range=TextRange(start=6444, end=6445)),
        TokenizationToken(text="HAI", text_range=TextRange(start=6445, end=6448)),
        TokenizationToken(text=")", text_range=TextRange(start=6448, end=6449)),
        TokenizationToken(text="▁that", text_range=TextRange(start=6449, end=6454)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=6454, end=6467)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=6467, end=6479)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=6479, end=6491)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=6491, end=6501)),
        TokenizationToken(text=",", text_range=TextRange(start=6501, end=6502)),
        TokenizationToken(text="▁development", text_range=TextRange(start=6502, end=6514)),
        TokenizationToken(text=",", text_range=TextRange(start=6514, end=6515)),
        TokenizationToken(text="▁and", text_range=TextRange(start=6515, end=6519)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=6519, end=6533)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=6533, end=6544)),
        TokenizationToken(text="▁models", text_range=TextRange(start=6544, end=6551)),
        TokenizationToken(text=".", text_range=TextRange(start=6551, end=6552)),
        TokenizationToken(text="The", text_range=TextRange(start=6552, end=6555)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=6555, end=6566)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=6566, end=6578)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=6578, end=6589)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=6589, end=6596)),
        TokenizationToken(text="▁", text_range=TextRange(start=6596, end=6597)),
        TokenizationToken(text="(", text_range=TextRange(start=6597, end=6598)),
        TokenizationToken(text="CRF", text_range=TextRange(start=6598, end=6601)),
        TokenizationToken(text="M", text_range=TextRange(start=6601, end=6602)),
        TokenizationToken(text=")", text_range=TextRange(start=6602, end=6603)),
        TokenizationToken(text="▁is", text_range=TextRange(start=6603, end=6606)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=6606, end=6627)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=6627, end=6638)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=6638, end=6650)),
        TokenizationToken(text="▁the", text_range=TextRange(start=6650, end=6654)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=6654, end=6663)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=6663, end=6677)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=6677, end=6683)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=6683, end=6692)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=6692, end=6716)),
        TokenizationToken(text="▁", text_range=TextRange(start=6716, end=6717)),
        TokenizationToken(text="(", text_range=TextRange(start=6717, end=6718)),
        TokenizationToken(text="HAI", text_range=TextRange(start=6718, end=6721)),
        TokenizationToken(text=")", text_range=TextRange(start=6721, end=6722)),
        TokenizationToken(text="▁that", text_range=TextRange(start=6722, end=6727)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=6727, end=6740)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=6740, end=6752)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=6752, end=6764)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=6764, end=6774)),
        TokenizationToken(text=",", text_range=TextRange(start=6774, end=6775)),
        TokenizationToken(text="▁development", text_range=TextRange(start=6775, end=6787)),
        TokenizationToken(text=",", text_range=TextRange(start=6787, end=6788)),
        TokenizationToken(text="▁and", text_range=TextRange(start=6788, end=6792)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=6792, end=6806)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=6806, end=6817)),
        TokenizationToken(text="▁models", text_range=TextRange(start=6817, end=6824)),
        TokenizationToken(text=".", text_range=TextRange(start=6824, end=6825)),
        TokenizationToken(text="The", text_range=TextRange(start=6825, end=6828)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=6828, end=6839)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=6839, end=6851)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=6851, end=6862)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=6862, end=6869)),
        TokenizationToken(text="▁", text_range=TextRange(start=6869, end=6870)),
        TokenizationToken(text="(", text_range=TextRange(start=6870, end=6871)),
        TokenizationToken(text="CRF", text_range=TextRange(start=6871, end=6874)),
        TokenizationToken(text="M", text_range=TextRange(start=6874, end=6875)),
        TokenizationToken(text=")", text_range=TextRange(start=6875, end=6876)),
        TokenizationToken(text="▁is", text_range=TextRange(start=6876, end=6879)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=6879, end=6900)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=6900, end=6911)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=6911, end=6923)),
        TokenizationToken(text="▁the", text_range=TextRange(start=6923, end=6927)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=6927, end=6936)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=6936, end=6950)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=6950, end=6956)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=6956, end=6965)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=6965, end=6989)),
        TokenizationToken(text="▁", text_range=TextRange(start=6989, end=6990)),
        TokenizationToken(text="(", text_range=TextRange(start=6990, end=6991)),
        TokenizationToken(text="HAI", text_range=TextRange(start=6991, end=6994)),
        TokenizationToken(text=")", text_range=TextRange(start=6994, end=6995)),
        TokenizationToken(text="▁that", text_range=TextRange(start=6995, end=7000)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=7000, end=7013)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=7013, end=7025)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=7025, end=7037)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=7037, end=7047)),
        TokenizationToken(text=",", text_range=TextRange(start=7047, end=7048)),
        TokenizationToken(text="▁development", text_range=TextRange(start=7048, end=7060)),
        TokenizationToken(text=",", text_range=TextRange(start=7060, end=7061)),
        TokenizationToken(text="▁and", text_range=TextRange(start=7061, end=7065)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=7065, end=7079)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=7079, end=7090)),
        TokenizationToken(text="▁models", text_range=TextRange(start=7090, end=7097)),
        TokenizationToken(text=".", text_range=TextRange(start=7097, end=7098)),
        TokenizationToken(text="The", text_range=TextRange(start=7098, end=7101)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=7101, end=7112)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=7112, end=7124)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=7124, end=7135)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=7135, end=7142)),
        TokenizationToken(text="▁", text_range=TextRange(start=7142, end=7143)),
        TokenizationToken(text="(", text_range=TextRange(start=7143, end=7144)),
        TokenizationToken(text="CRF", text_range=TextRange(start=7144, end=7147)),
        TokenizationToken(text="M", text_range=TextRange(start=7147, end=7148)),
        TokenizationToken(text=")", text_range=TextRange(start=7148, end=7149)),
        TokenizationToken(text="▁is", text_range=TextRange(start=7149, end=7152)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=7152, end=7173)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=7173, end=7184)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=7184, end=7196)),
        TokenizationToken(text="▁the", text_range=TextRange(start=7196, end=7200)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=7200, end=7209)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=7209, end=7223)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=7223, end=7229)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=7229, end=7238)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=7238, end=7262)),
        TokenizationToken(text="▁", text_range=TextRange(start=7262, end=7263)),
        TokenizationToken(text="(", text_range=TextRange(start=7263, end=7264)),
        TokenizationToken(text="HAI", text_range=TextRange(start=7264, end=7267)),
        TokenizationToken(text=")", text_range=TextRange(start=7267, end=7268)),
        TokenizationToken(text="▁that", text_range=TextRange(start=7268, end=7273)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=7273, end=7286)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=7286, end=7298)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=7298, end=7310)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=7310, end=7320)),
        TokenizationToken(text=",", text_range=TextRange(start=7320, end=7321)),
        TokenizationToken(text="▁development", text_range=TextRange(start=7321, end=7333)),
        TokenizationToken(text=",", text_range=TextRange(start=7333, end=7334)),
        TokenizationToken(text="▁and", text_range=TextRange(start=7334, end=7338)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=7338, end=7352)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=7352, end=7363)),
        TokenizationToken(text="▁models", text_range=TextRange(start=7363, end=7370)),
        TokenizationToken(text=".", text_range=TextRange(start=7370, end=7371)),
        TokenizationToken(text="The", text_range=TextRange(start=7371, end=7374)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=7374, end=7385)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=7385, end=7397)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=7397, end=7408)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=7408, end=7415)),
        TokenizationToken(text="▁", text_range=TextRange(start=7415, end=7416)),
        TokenizationToken(text="(", text_range=TextRange(start=7416, end=7417)),
        TokenizationToken(text="CRF", text_range=TextRange(start=7417, end=7420)),
        TokenizationToken(text="M", text_range=TextRange(start=7420, end=7421)),
        TokenizationToken(text=")", text_range=TextRange(start=7421, end=7422)),
        TokenizationToken(text="▁is", text_range=TextRange(start=7422, end=7425)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=7425, end=7446)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=7446, end=7457)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=7457, end=7469)),
        TokenizationToken(text="▁the", text_range=TextRange(start=7469, end=7473)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=7473, end=7482)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=7482, end=7496)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=7496, end=7502)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=7502, end=7511)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=7511, end=7535)),
        TokenizationToken(text="▁", text_range=TextRange(start=7535, end=7536)),
        TokenizationToken(text="(", text_range=TextRange(start=7536, end=7537)),
        TokenizationToken(text="HAI", text_range=TextRange(start=7537, end=7540)),
        TokenizationToken(text=")", text_range=TextRange(start=7540, end=7541)),
        TokenizationToken(text="▁that", text_range=TextRange(start=7541, end=7546)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=7546, end=7559)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=7559, end=7571)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=7571, end=7583)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=7583, end=7593)),
        TokenizationToken(text=",", text_range=TextRange(start=7593, end=7594)),
        TokenizationToken(text="▁development", text_range=TextRange(start=7594, end=7606)),
        TokenizationToken(text=",", text_range=TextRange(start=7606, end=7607)),
        TokenizationToken(text="▁and", text_range=TextRange(start=7607, end=7611)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=7611, end=7625)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=7625, end=7636)),
        TokenizationToken(text="▁models", text_range=TextRange(start=7636, end=7643)),
        TokenizationToken(text=".", text_range=TextRange(start=7643, end=7644)),
        TokenizationToken(text="The", text_range=TextRange(start=7644, end=7647)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=7647, end=7658)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=7658, end=7670)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=7670, end=7681)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=7681, end=7688)),
        TokenizationToken(text="▁", text_range=TextRange(start=7688, end=7689)),
        TokenizationToken(text="(", text_range=TextRange(start=7689, end=7690)),
        TokenizationToken(text="CRF", text_range=TextRange(start=7690, end=7693)),
        TokenizationToken(text="M", text_range=TextRange(start=7693, end=7694)),
        TokenizationToken(text=")", text_range=TextRange(start=7694, end=7695)),
        TokenizationToken(text="▁is", text_range=TextRange(start=7695, end=7698)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=7698, end=7719)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=7719, end=7730)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=7730, end=7742)),
        TokenizationToken(text="▁the", text_range=TextRange(start=7742, end=7746)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=7746, end=7755)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=7755, end=7769)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=7769, end=7775)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=7775, end=7784)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=7784, end=7808)),
        TokenizationToken(text="▁", text_range=TextRange(start=7808, end=7809)),
        TokenizationToken(text="(", text_range=TextRange(start=7809, end=7810)),
        TokenizationToken(text="HAI", text_range=TextRange(start=7810, end=7813)),
        TokenizationToken(text=")", text_range=TextRange(start=7813, end=7814)),
        TokenizationToken(text="▁that", text_range=TextRange(start=7814, end=7819)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=7819, end=7832)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=7832, end=7844)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=7844, end=7856)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=7856, end=7866)),
        TokenizationToken(text=",", text_range=TextRange(start=7866, end=7867)),
        TokenizationToken(text="▁development", text_range=TextRange(start=7867, end=7879)),
        TokenizationToken(text=",", text_range=TextRange(start=7879, end=7880)),
        TokenizationToken(text="▁and", text_range=TextRange(start=7880, end=7884)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=7884, end=7898)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=7898, end=7909)),
        TokenizationToken(text="▁models", text_range=TextRange(start=7909, end=7916)),
        TokenizationToken(text=".", text_range=TextRange(start=7916, end=7917)),
        TokenizationToken(text="The", text_range=TextRange(start=7917, end=7920)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=7920, end=7931)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=7931, end=7943)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=7943, end=7954)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=7954, end=7961)),
        TokenizationToken(text="▁", text_range=TextRange(start=7961, end=7962)),
        TokenizationToken(text="(", text_range=TextRange(start=7962, end=7963)),
        TokenizationToken(text="CRF", text_range=TextRange(start=7963, end=7966)),
        TokenizationToken(text="M", text_range=TextRange(start=7966, end=7967)),
        TokenizationToken(text=")", text_range=TextRange(start=7967, end=7968)),
        TokenizationToken(text="▁is", text_range=TextRange(start=7968, end=7971)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=7971, end=7992)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=7992, end=8003)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=8003, end=8015)),
        TokenizationToken(text="▁the", text_range=TextRange(start=8015, end=8019)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=8019, end=8028)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=8028, end=8042)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=8042, end=8048)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=8048, end=8057)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=8057, end=8081)),
        TokenizationToken(text="▁", text_range=TextRange(start=8081, end=8082)),
        TokenizationToken(text="(", text_range=TextRange(start=8082, end=8083)),
        TokenizationToken(text="HAI", text_range=TextRange(start=8083, end=8086)),
        TokenizationToken(text=")", text_range=TextRange(start=8086, end=8087)),
        TokenizationToken(text="▁that", text_range=TextRange(start=8087, end=8092)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=8092, end=8105)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=8105, end=8117)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=8117, end=8129)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=8129, end=8139)),
        TokenizationToken(text=",", text_range=TextRange(start=8139, end=8140)),
        TokenizationToken(text="▁development", text_range=TextRange(start=8140, end=8152)),
        TokenizationToken(text=",", text_range=TextRange(start=8152, end=8153)),
        TokenizationToken(text="▁and", text_range=TextRange(start=8153, end=8157)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=8157, end=8171)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=8171, end=8182)),
        TokenizationToken(text="▁models", text_range=TextRange(start=8182, end=8189)),
        TokenizationToken(text=".", text_range=TextRange(start=8189, end=8190)),
        TokenizationToken(text="The", text_range=TextRange(start=8190, end=8193)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=8193, end=8204)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=8204, end=8216)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=8216, end=8227)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=8227, end=8234)),
        TokenizationToken(text="▁", text_range=TextRange(start=8234, end=8235)),
        TokenizationToken(text="(", text_range=TextRange(start=8235, end=8236)),
        TokenizationToken(text="CRF", text_range=TextRange(start=8236, end=8239)),
        TokenizationToken(text="M", text_range=TextRange(start=8239, end=8240)),
        TokenizationToken(text=")", text_range=TextRange(start=8240, end=8241)),
        TokenizationToken(text="▁is", text_range=TextRange(start=8241, end=8244)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=8244, end=8265)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=8265, end=8276)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=8276, end=8288)),
        TokenizationToken(text="▁the", text_range=TextRange(start=8288, end=8292)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=8292, end=8301)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=8301, end=8315)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=8315, end=8321)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=8321, end=8330)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=8330, end=8354)),
        TokenizationToken(text="▁", text_range=TextRange(start=8354, end=8355)),
        TokenizationToken(text="(", text_range=TextRange(start=8355, end=8356)),
        TokenizationToken(text="HAI", text_range=TextRange(start=8356, end=8359)),
        TokenizationToken(text=")", text_range=TextRange(start=8359, end=8360)),
        TokenizationToken(text="▁that", text_range=TextRange(start=8360, end=8365)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=8365, end=8378)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=8378, end=8390)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=8390, end=8402)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=8402, end=8412)),
        TokenizationToken(text=",", text_range=TextRange(start=8412, end=8413)),
        TokenizationToken(text="▁development", text_range=TextRange(start=8413, end=8425)),
        TokenizationToken(text=",", text_range=TextRange(start=8425, end=8426)),
        TokenizationToken(text="▁and", text_range=TextRange(start=8426, end=8430)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=8430, end=8444)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=8444, end=8455)),
        TokenizationToken(text="▁models", text_range=TextRange(start=8455, end=8462)),
        TokenizationToken(text=".", text_range=TextRange(start=8462, end=8463)),
        TokenizationToken(text="The", text_range=TextRange(start=8463, end=8466)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=8466, end=8477)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=8477, end=8489)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=8489, end=8500)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=8500, end=8507)),
        TokenizationToken(text="▁", text_range=TextRange(start=8507, end=8508)),
        TokenizationToken(text="(", text_range=TextRange(start=8508, end=8509)),
        TokenizationToken(text="CRF", text_range=TextRange(start=8509, end=8512)),
        TokenizationToken(text="M", text_range=TextRange(start=8512, end=8513)),
        TokenizationToken(text=")", text_range=TextRange(start=8513, end=8514)),
        TokenizationToken(text="▁is", text_range=TextRange(start=8514, end=8517)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=8517, end=8538)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=8538, end=8549)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=8549, end=8561)),
        TokenizationToken(text="▁the", text_range=TextRange(start=8561, end=8565)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=8565, end=8574)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=8574, end=8588)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=8588, end=8594)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=8594, end=8603)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=8603, end=8627)),
        TokenizationToken(text="▁", text_range=TextRange(start=8627, end=8628)),
        TokenizationToken(text="(", text_range=TextRange(start=8628, end=8629)),
        TokenizationToken(text="HAI", text_range=TextRange(start=8629, end=8632)),
        TokenizationToken(text=")", text_range=TextRange(start=8632, end=8633)),
        TokenizationToken(text="▁that", text_range=TextRange(start=8633, end=8638)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=8638, end=8651)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=8651, end=8663)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=8663, end=8675)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=8675, end=8685)),
        TokenizationToken(text=",", text_range=TextRange(start=8685, end=8686)),
        TokenizationToken(text="▁development", text_range=TextRange(start=8686, end=8698)),
        TokenizationToken(text=",", text_range=TextRange(start=8698, end=8699)),
        TokenizationToken(text="▁and", text_range=TextRange(start=8699, end=8703)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=8703, end=8717)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=8717, end=8728)),
        TokenizationToken(text="▁models", text_range=TextRange(start=8728, end=8735)),
        TokenizationToken(text=".", text_range=TextRange(start=8735, end=8736)),
        TokenizationToken(text="The", text_range=TextRange(start=8736, end=8739)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=8739, end=8750)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=8750, end=8762)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=8762, end=8773)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=8773, end=8780)),
        TokenizationToken(text="▁", text_range=TextRange(start=8780, end=8781)),
        TokenizationToken(text="(", text_range=TextRange(start=8781, end=8782)),
        TokenizationToken(text="CRF", text_range=TextRange(start=8782, end=8785)),
        TokenizationToken(text="M", text_range=TextRange(start=8785, end=8786)),
        TokenizationToken(text=")", text_range=TextRange(start=8786, end=8787)),
        TokenizationToken(text="▁is", text_range=TextRange(start=8787, end=8790)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=8790, end=8811)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=8811, end=8822)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=8822, end=8834)),
        TokenizationToken(text="▁the", text_range=TextRange(start=8834, end=8838)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=8838, end=8847)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=8847, end=8861)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=8861, end=8867)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=8867, end=8876)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=8876, end=8900)),
        TokenizationToken(text="▁", text_range=TextRange(start=8900, end=8901)),
        TokenizationToken(text="(", text_range=TextRange(start=8901, end=8902)),
        TokenizationToken(text="HAI", text_range=TextRange(start=8902, end=8905)),
        TokenizationToken(text=")", text_range=TextRange(start=8905, end=8906)),
        TokenizationToken(text="▁that", text_range=TextRange(start=8906, end=8911)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=8911, end=8924)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=8924, end=8936)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=8936, end=8948)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=8948, end=8958)),
        TokenizationToken(text=",", text_range=TextRange(start=8958, end=8959)),
        TokenizationToken(text="▁development", text_range=TextRange(start=8959, end=8971)),
        TokenizationToken(text=",", text_range=TextRange(start=8971, end=8972)),
        TokenizationToken(text="▁and", text_range=TextRange(start=8972, end=8976)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=8976, end=8990)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=8990, end=9001)),
        TokenizationToken(text="▁models", text_range=TextRange(start=9001, end=9008)),
        TokenizationToken(text=".", text_range=TextRange(start=9008, end=9009)),
        TokenizationToken(text="The", text_range=TextRange(start=9009, end=9012)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=9012, end=9023)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=9023, end=9035)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=9035, end=9046)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=9046, end=9053)),
        TokenizationToken(text="▁", text_range=TextRange(start=9053, end=9054)),
        TokenizationToken(text="(", text_range=TextRange(start=9054, end=9055)),
        TokenizationToken(text="CRF", text_range=TextRange(start=9055, end=9058)),
        TokenizationToken(text="M", text_range=TextRange(start=9058, end=9059)),
        TokenizationToken(text=")", text_range=TextRange(start=9059, end=9060)),
        TokenizationToken(text="▁is", text_range=TextRange(start=9060, end=9063)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=9063, end=9084)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=9084, end=9095)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=9095, end=9107)),
        TokenizationToken(text="▁the", text_range=TextRange(start=9107, end=9111)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=9111, end=9120)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=9120, end=9134)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=9134, end=9140)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=9140, end=9149)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=9149, end=9173)),
        TokenizationToken(text="▁", text_range=TextRange(start=9173, end=9174)),
        TokenizationToken(text="(", text_range=TextRange(start=9174, end=9175)),
        TokenizationToken(text="HAI", text_range=TextRange(start=9175, end=9178)),
        TokenizationToken(text=")", text_range=TextRange(start=9178, end=9179)),
        TokenizationToken(text="▁that", text_range=TextRange(start=9179, end=9184)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=9184, end=9197)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=9197, end=9209)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=9209, end=9221)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=9221, end=9231)),
        TokenizationToken(text=",", text_range=TextRange(start=9231, end=9232)),
        TokenizationToken(text="▁development", text_range=TextRange(start=9232, end=9244)),
        TokenizationToken(text=",", text_range=TextRange(start=9244, end=9245)),
        TokenizationToken(text="▁and", text_range=TextRange(start=9245, end=9249)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=9249, end=9263)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=9263, end=9274)),
        TokenizationToken(text="▁models", text_range=TextRange(start=9274, end=9281)),
        TokenizationToken(text=".", text_range=TextRange(start=9281, end=9282)),
        TokenizationToken(text="The", text_range=TextRange(start=9282, end=9285)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=9285, end=9296)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=9296, end=9308)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=9308, end=9319)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=9319, end=9326)),
        TokenizationToken(text="▁", text_range=TextRange(start=9326, end=9327)),
        TokenizationToken(text="(", text_range=TextRange(start=9327, end=9328)),
        TokenizationToken(text="CRF", text_range=TextRange(start=9328, end=9331)),
        TokenizationToken(text="M", text_range=TextRange(start=9331, end=9332)),
        TokenizationToken(text=")", text_range=TextRange(start=9332, end=9333)),
        TokenizationToken(text="▁is", text_range=TextRange(start=9333, end=9336)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=9336, end=9357)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=9357, end=9368)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=9368, end=9380)),
        TokenizationToken(text="▁the", text_range=TextRange(start=9380, end=9384)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=9384, end=9393)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=9393, end=9407)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=9407, end=9413)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=9413, end=9422)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=9422, end=9446)),
        TokenizationToken(text="▁", text_range=TextRange(start=9446, end=9447)),
        TokenizationToken(text="(", text_range=TextRange(start=9447, end=9448)),
        TokenizationToken(text="HAI", text_range=TextRange(start=9448, end=9451)),
        TokenizationToken(text=")", text_range=TextRange(start=9451, end=9452)),
        TokenizationToken(text="▁that", text_range=TextRange(start=9452, end=9457)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=9457, end=9470)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=9470, end=9482)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=9482, end=9494)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=9494, end=9504)),
        TokenizationToken(text=",", text_range=TextRange(start=9504, end=9505)),
        TokenizationToken(text="▁development", text_range=TextRange(start=9505, end=9517)),
        TokenizationToken(text=",", text_range=TextRange(start=9517, end=9518)),
        TokenizationToken(text="▁and", text_range=TextRange(start=9518, end=9522)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=9522, end=9536)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=9536, end=9547)),
        TokenizationToken(text="▁models", text_range=TextRange(start=9547, end=9554)),
        TokenizationToken(text=".", text_range=TextRange(start=9554, end=9555)),
        TokenizationToken(text="The", text_range=TextRange(start=9555, end=9558)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=9558, end=9569)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=9569, end=9581)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=9581, end=9592)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=9592, end=9599)),
        TokenizationToken(text="▁", text_range=TextRange(start=9599, end=9600)),
        TokenizationToken(text="(", text_range=TextRange(start=9600, end=9601)),
        TokenizationToken(text="CRF", text_range=TextRange(start=9601, end=9604)),
        TokenizationToken(text="M", text_range=TextRange(start=9604, end=9605)),
        TokenizationToken(text=")", text_range=TextRange(start=9605, end=9606)),
        TokenizationToken(text="▁is", text_range=TextRange(start=9606, end=9609)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=9609, end=9630)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=9630, end=9641)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=9641, end=9653)),
        TokenizationToken(text="▁the", text_range=TextRange(start=9653, end=9657)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=9657, end=9666)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=9666, end=9680)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=9680, end=9686)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=9686, end=9695)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=9695, end=9719)),
        TokenizationToken(text="▁", text_range=TextRange(start=9719, end=9720)),
        TokenizationToken(text="(", text_range=TextRange(start=9720, end=9721)),
        TokenizationToken(text="HAI", text_range=TextRange(start=9721, end=9724)),
        TokenizationToken(text=")", text_range=TextRange(start=9724, end=9725)),
        TokenizationToken(text="▁that", text_range=TextRange(start=9725, end=9730)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=9730, end=9743)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=9743, end=9755)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=9755, end=9767)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=9767, end=9777)),
        TokenizationToken(text=",", text_range=TextRange(start=9777, end=9778)),
        TokenizationToken(text="▁development", text_range=TextRange(start=9778, end=9790)),
        TokenizationToken(text=",", text_range=TextRange(start=9790, end=9791)),
        TokenizationToken(text="▁and", text_range=TextRange(start=9791, end=9795)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=9795, end=9809)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=9809, end=9820)),
        TokenizationToken(text="▁models", text_range=TextRange(start=9820, end=9827)),
        TokenizationToken(text=".", text_range=TextRange(start=9827, end=9828)),
        TokenizationToken(text="The", text_range=TextRange(start=9828, end=9831)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=9831, end=9842)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=9842, end=9854)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=9854, end=9865)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=9865, end=9872)),
        TokenizationToken(text="▁", text_range=TextRange(start=9872, end=9873)),
        TokenizationToken(text="(", text_range=TextRange(start=9873, end=9874)),
        TokenizationToken(text="CRF", text_range=TextRange(start=9874, end=9877)),
        TokenizationToken(text="M", text_range=TextRange(start=9877, end=9878)),
        TokenizationToken(text=")", text_range=TextRange(start=9878, end=9879)),
        TokenizationToken(text="▁is", text_range=TextRange(start=9879, end=9882)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=9882, end=9903)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=9903, end=9914)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=9914, end=9926)),
        TokenizationToken(text="▁the", text_range=TextRange(start=9926, end=9930)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=9930, end=9939)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=9939, end=9953)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=9953, end=9959)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=9959, end=9968)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=9968, end=9992)),
        TokenizationToken(text="▁", text_range=TextRange(start=9992, end=9993)),
        TokenizationToken(text="(", text_range=TextRange(start=9993, end=9994)),
        TokenizationToken(text="HAI", text_range=TextRange(start=9994, end=9997)),
        TokenizationToken(text=")", text_range=TextRange(start=9997, end=9998)),
        TokenizationToken(text="▁that", text_range=TextRange(start=9998, end=10003)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=10003, end=10016)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=10016, end=10028)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=10028, end=10040)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=10040, end=10050)),
        TokenizationToken(text=",", text_range=TextRange(start=10050, end=10051)),
        TokenizationToken(text="▁development", text_range=TextRange(start=10051, end=10063)),
        TokenizationToken(text=",", text_range=TextRange(start=10063, end=10064)),
        TokenizationToken(text="▁and", text_range=TextRange(start=10064, end=10068)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=10068, end=10082)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=10082, end=10093)),
        TokenizationToken(text="▁models", text_range=TextRange(start=10093, end=10100)),
        TokenizationToken(text=".", text_range=TextRange(start=10100, end=10101)),
        TokenizationToken(text="The", text_range=TextRange(start=10101, end=10104)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=10104, end=10115)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=10115, end=10127)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=10127, end=10138)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=10138, end=10145)),
        TokenizationToken(text="▁", text_range=TextRange(start=10145, end=10146)),
        TokenizationToken(text="(", text_range=TextRange(start=10146, end=10147)),
        TokenizationToken(text="CRF", text_range=TextRange(start=10147, end=10150)),
        TokenizationToken(text="M", text_range=TextRange(start=10150, end=10151)),
        TokenizationToken(text=")", text_range=TextRange(start=10151, end=10152)),
        TokenizationToken(text="▁is", text_range=TextRange(start=10152, end=10155)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=10155, end=10176)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=10176, end=10187)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=10187, end=10199)),
        TokenizationToken(text="▁the", text_range=TextRange(start=10199, end=10203)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=10203, end=10212)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=10212, end=10226)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=10226, end=10232)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=10232, end=10241)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=10241, end=10265)),
        TokenizationToken(text="▁", text_range=TextRange(start=10265, end=10266)),
        TokenizationToken(text="(", text_range=TextRange(start=10266, end=10267)),
        TokenizationToken(text="HAI", text_range=TextRange(start=10267, end=10270)),
        TokenizationToken(text=")", text_range=TextRange(start=10270, end=10271)),
        TokenizationToken(text="▁that", text_range=TextRange(start=10271, end=10276)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=10276, end=10289)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=10289, end=10301)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=10301, end=10313)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=10313, end=10323)),
        TokenizationToken(text=",", text_range=TextRange(start=10323, end=10324)),
        TokenizationToken(text="▁development", text_range=TextRange(start=10324, end=10336)),
        TokenizationToken(text=",", text_range=TextRange(start=10336, end=10337)),
        TokenizationToken(text="▁and", text_range=TextRange(start=10337, end=10341)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=10341, end=10355)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=10355, end=10366)),
        TokenizationToken(text="▁models", text_range=TextRange(start=10366, end=10373)),
        TokenizationToken(text=".", text_range=TextRange(start=10373, end=10374)),
        TokenizationToken(text="The", text_range=TextRange(start=10374, end=10377)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=10377, end=10388)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=10388, end=10400)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=10400, end=10411)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=10411, end=10418)),
        TokenizationToken(text="▁", text_range=TextRange(start=10418, end=10419)),
        TokenizationToken(text="(", text_range=TextRange(start=10419, end=10420)),
        TokenizationToken(text="CRF", text_range=TextRange(start=10420, end=10423)),
        TokenizationToken(text="M", text_range=TextRange(start=10423, end=10424)),
        TokenizationToken(text=")", text_range=TextRange(start=10424, end=10425)),
        TokenizationToken(text="▁is", text_range=TextRange(start=10425, end=10428)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=10428, end=10449)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=10449, end=10460)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=10460, end=10472)),
        TokenizationToken(text="▁the", text_range=TextRange(start=10472, end=10476)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=10476, end=10485)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=10485, end=10499)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=10499, end=10505)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=10505, end=10514)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=10514, end=10538)),
        TokenizationToken(text="▁", text_range=TextRange(start=10538, end=10539)),
        TokenizationToken(text="(", text_range=TextRange(start=10539, end=10540)),
        TokenizationToken(text="HAI", text_range=TextRange(start=10540, end=10543)),
        TokenizationToken(text=")", text_range=TextRange(start=10543, end=10544)),
        TokenizationToken(text="▁that", text_range=TextRange(start=10544, end=10549)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=10549, end=10562)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=10562, end=10574)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=10574, end=10586)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=10586, end=10596)),
        TokenizationToken(text=",", text_range=TextRange(start=10596, end=10597)),
        TokenizationToken(text="▁development", text_range=TextRange(start=10597, end=10609)),
        TokenizationToken(text=",", text_range=TextRange(start=10609, end=10610)),
        TokenizationToken(text="▁and", text_range=TextRange(start=10610, end=10614)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=10614, end=10628)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=10628, end=10639)),
        TokenizationToken(text="▁models", text_range=TextRange(start=10639, end=10646)),
        TokenizationToken(text=".", text_range=TextRange(start=10646, end=10647)),
        TokenizationToken(text="The", text_range=TextRange(start=10647, end=10650)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=10650, end=10661)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=10661, end=10673)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=10673, end=10684)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=10684, end=10691)),
        TokenizationToken(text="▁", text_range=TextRange(start=10691, end=10692)),
        TokenizationToken(text="(", text_range=TextRange(start=10692, end=10693)),
        TokenizationToken(text="CRF", text_range=TextRange(start=10693, end=10696)),
        TokenizationToken(text="M", text_range=TextRange(start=10696, end=10697)),
        TokenizationToken(text=")", text_range=TextRange(start=10697, end=10698)),
        TokenizationToken(text="▁is", text_range=TextRange(start=10698, end=10701)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=10701, end=10722)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=10722, end=10733)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=10733, end=10745)),
        TokenizationToken(text="▁the", text_range=TextRange(start=10745, end=10749)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=10749, end=10758)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=10758, end=10772)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=10772, end=10778)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=10778, end=10787)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=10787, end=10811)),
        TokenizationToken(text="▁", text_range=TextRange(start=10811, end=10812)),
        TokenizationToken(text="(", text_range=TextRange(start=10812, end=10813)),
        TokenizationToken(text="HAI", text_range=TextRange(start=10813, end=10816)),
        TokenizationToken(text=")", text_range=TextRange(start=10816, end=10817)),
        TokenizationToken(text="▁that", text_range=TextRange(start=10817, end=10822)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=10822, end=10835)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=10835, end=10847)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=10847, end=10859)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=10859, end=10869)),
        TokenizationToken(text=",", text_range=TextRange(start=10869, end=10870)),
        TokenizationToken(text="▁development", text_range=TextRange(start=10870, end=10882)),
        TokenizationToken(text=",", text_range=TextRange(start=10882, end=10883)),
        TokenizationToken(text="▁and", text_range=TextRange(start=10883, end=10887)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=10887, end=10901)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=10901, end=10912)),
        TokenizationToken(text="▁models", text_range=TextRange(start=10912, end=10919)),
        TokenizationToken(text=".", text_range=TextRange(start=10919, end=10920)),
        TokenizationToken(text="The", text_range=TextRange(start=10920, end=10923)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=10923, end=10934)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=10934, end=10946)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=10946, end=10957)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=10957, end=10964)),
        TokenizationToken(text="▁", text_range=TextRange(start=10964, end=10965)),
        TokenizationToken(text="(", text_range=TextRange(start=10965, end=10966)),
        TokenizationToken(text="CRF", text_range=TextRange(start=10966, end=10969)),
        TokenizationToken(text="M", text_range=TextRange(start=10969, end=10970)),
        TokenizationToken(text=")", text_range=TextRange(start=10970, end=10971)),
        TokenizationToken(text="▁is", text_range=TextRange(start=10971, end=10974)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=10974, end=10995)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=10995, end=11006)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=11006, end=11018)),
        TokenizationToken(text="▁the", text_range=TextRange(start=11018, end=11022)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=11022, end=11031)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=11031, end=11045)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=11045, end=11051)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=11051, end=11060)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=11060, end=11084)),
        TokenizationToken(text="▁", text_range=TextRange(start=11084, end=11085)),
        TokenizationToken(text="(", text_range=TextRange(start=11085, end=11086)),
        TokenizationToken(text="HAI", text_range=TextRange(start=11086, end=11089)),
        TokenizationToken(text=")", text_range=TextRange(start=11089, end=11090)),
        TokenizationToken(text="▁that", text_range=TextRange(start=11090, end=11095)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=11095, end=11108)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=11108, end=11120)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=11120, end=11132)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=11132, end=11142)),
        TokenizationToken(text=",", text_range=TextRange(start=11142, end=11143)),
        TokenizationToken(text="▁development", text_range=TextRange(start=11143, end=11155)),
        TokenizationToken(text=",", text_range=TextRange(start=11155, end=11156)),
        TokenizationToken(text="▁and", text_range=TextRange(start=11156, end=11160)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=11160, end=11174)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=11174, end=11185)),
        TokenizationToken(text="▁models", text_range=TextRange(start=11185, end=11192)),
        TokenizationToken(text=".", text_range=TextRange(start=11192, end=11193)),
        TokenizationToken(text="The", text_range=TextRange(start=11193, end=11196)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=11196, end=11207)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=11207, end=11219)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=11219, end=11230)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=11230, end=11237)),
        TokenizationToken(text="▁", text_range=TextRange(start=11237, end=11238)),
        TokenizationToken(text="(", text_range=TextRange(start=11238, end=11239)),
        TokenizationToken(text="CRF", text_range=TextRange(start=11239, end=11242)),
        TokenizationToken(text="M", text_range=TextRange(start=11242, end=11243)),
        TokenizationToken(text=")", text_range=TextRange(start=11243, end=11244)),
        TokenizationToken(text="▁is", text_range=TextRange(start=11244, end=11247)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=11247, end=11268)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=11268, end=11279)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=11279, end=11291)),
        TokenizationToken(text="▁the", text_range=TextRange(start=11291, end=11295)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=11295, end=11304)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=11304, end=11318)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=11318, end=11324)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=11324, end=11333)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=11333, end=11357)),
        TokenizationToken(text="▁", text_range=TextRange(start=11357, end=11358)),
        TokenizationToken(text="(", text_range=TextRange(start=11358, end=11359)),
        TokenizationToken(text="HAI", text_range=TextRange(start=11359, end=11362)),
        TokenizationToken(text=")", text_range=TextRange(start=11362, end=11363)),
        TokenizationToken(text="▁that", text_range=TextRange(start=11363, end=11368)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=11368, end=11381)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=11381, end=11393)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=11393, end=11405)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=11405, end=11415)),
        TokenizationToken(text=",", text_range=TextRange(start=11415, end=11416)),
        TokenizationToken(text="▁development", text_range=TextRange(start=11416, end=11428)),
        TokenizationToken(text=",", text_range=TextRange(start=11428, end=11429)),
        TokenizationToken(text="▁and", text_range=TextRange(start=11429, end=11433)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=11433, end=11447)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=11447, end=11458)),
        TokenizationToken(text="▁models", text_range=TextRange(start=11458, end=11465)),
        TokenizationToken(text=".", text_range=TextRange(start=11465, end=11466)),
        TokenizationToken(text="The", text_range=TextRange(start=11466, end=11469)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=11469, end=11480)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=11480, end=11492)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=11492, end=11503)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=11503, end=11510)),
        TokenizationToken(text="▁", text_range=TextRange(start=11510, end=11511)),
        TokenizationToken(text="(", text_range=TextRange(start=11511, end=11512)),
        TokenizationToken(text="CRF", text_range=TextRange(start=11512, end=11515)),
        TokenizationToken(text="M", text_range=TextRange(start=11515, end=11516)),
        TokenizationToken(text=")", text_range=TextRange(start=11516, end=11517)),
        TokenizationToken(text="▁is", text_range=TextRange(start=11517, end=11520)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=11520, end=11541)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=11541, end=11552)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=11552, end=11564)),
        TokenizationToken(text="▁the", text_range=TextRange(start=11564, end=11568)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=11568, end=11577)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=11577, end=11591)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=11591, end=11597)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=11597, end=11606)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=11606, end=11630)),
        TokenizationToken(text="▁", text_range=TextRange(start=11630, end=11631)),
        TokenizationToken(text="(", text_range=TextRange(start=11631, end=11632)),
        TokenizationToken(text="HAI", text_range=TextRange(start=11632, end=11635)),
        TokenizationToken(text=")", text_range=TextRange(start=11635, end=11636)),
        TokenizationToken(text="▁that", text_range=TextRange(start=11636, end=11641)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=11641, end=11654)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=11654, end=11666)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=11666, end=11678)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=11678, end=11688)),
        TokenizationToken(text=",", text_range=TextRange(start=11688, end=11689)),
        TokenizationToken(text="▁development", text_range=TextRange(start=11689, end=11701)),
        TokenizationToken(text=",", text_range=TextRange(start=11701, end=11702)),
        TokenizationToken(text="▁and", text_range=TextRange(start=11702, end=11706)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=11706, end=11720)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=11720, end=11731)),
        TokenizationToken(text="▁models", text_range=TextRange(start=11731, end=11738)),
        TokenizationToken(text=".", text_range=TextRange(start=11738, end=11739)),
        TokenizationToken(text="The", text_range=TextRange(start=11739, end=11742)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=11742, end=11753)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=11753, end=11765)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=11765, end=11776)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=11776, end=11783)),
        TokenizationToken(text="▁", text_range=TextRange(start=11783, end=11784)),
        TokenizationToken(text="(", text_range=TextRange(start=11784, end=11785)),
        TokenizationToken(text="CRF", text_range=TextRange(start=11785, end=11788)),
        TokenizationToken(text="M", text_range=TextRange(start=11788, end=11789)),
        TokenizationToken(text=")", text_range=TextRange(start=11789, end=11790)),
        TokenizationToken(text="▁is", text_range=TextRange(start=11790, end=11793)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=11793, end=11814)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=11814, end=11825)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=11825, end=11837)),
        TokenizationToken(text="▁the", text_range=TextRange(start=11837, end=11841)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=11841, end=11850)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=11850, end=11864)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=11864, end=11870)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=11870, end=11879)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=11879, end=11903)),
        TokenizationToken(text="▁", text_range=TextRange(start=11903, end=11904)),
        TokenizationToken(text="(", text_range=TextRange(start=11904, end=11905)),
        TokenizationToken(text="HAI", text_range=TextRange(start=11905, end=11908)),
        TokenizationToken(text=")", text_range=TextRange(start=11908, end=11909)),
        TokenizationToken(text="▁that", text_range=TextRange(start=11909, end=11914)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=11914, end=11927)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=11927, end=11939)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=11939, end=11951)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=11951, end=11961)),
        TokenizationToken(text=",", text_range=TextRange(start=11961, end=11962)),
        TokenizationToken(text="▁development", text_range=TextRange(start=11962, end=11974)),
        TokenizationToken(text=",", text_range=TextRange(start=11974, end=11975)),
        TokenizationToken(text="▁and", text_range=TextRange(start=11975, end=11979)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=11979, end=11993)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=11993, end=12004)),
        TokenizationToken(text="▁models", text_range=TextRange(start=12004, end=12011)),
        TokenizationToken(text=".", text_range=TextRange(start=12011, end=12012)),
        TokenizationToken(text="The", text_range=TextRange(start=12012, end=12015)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=12015, end=12026)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=12026, end=12038)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=12038, end=12049)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=12049, end=12056)),
        TokenizationToken(text="▁", text_range=TextRange(start=12056, end=12057)),
        TokenizationToken(text="(", text_range=TextRange(start=12057, end=12058)),
        TokenizationToken(text="CRF", text_range=TextRange(start=12058, end=12061)),
        TokenizationToken(text="M", text_range=TextRange(start=12061, end=12062)),
        TokenizationToken(text=")", text_range=TextRange(start=12062, end=12063)),
        TokenizationToken(text="▁is", text_range=TextRange(start=12063, end=12066)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=12066, end=12087)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=12087, end=12098)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=12098, end=12110)),
        TokenizationToken(text="▁the", text_range=TextRange(start=12110, end=12114)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=12114, end=12123)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=12123, end=12137)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=12137, end=12143)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=12143, end=12152)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=12152, end=12176)),
        TokenizationToken(text="▁", text_range=TextRange(start=12176, end=12177)),
        TokenizationToken(text="(", text_range=TextRange(start=12177, end=12178)),
        TokenizationToken(text="HAI", text_range=TextRange(start=12178, end=12181)),
        TokenizationToken(text=")", text_range=TextRange(start=12181, end=12182)),
        TokenizationToken(text="▁that", text_range=TextRange(start=12182, end=12187)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=12187, end=12200)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=12200, end=12212)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=12212, end=12224)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=12224, end=12234)),
        TokenizationToken(text=",", text_range=TextRange(start=12234, end=12235)),
        TokenizationToken(text="▁development", text_range=TextRange(start=12235, end=12247)),
        TokenizationToken(text=",", text_range=TextRange(start=12247, end=12248)),
        TokenizationToken(text="▁and", text_range=TextRange(start=12248, end=12252)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=12252, end=12266)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=12266, end=12277)),
        TokenizationToken(text="▁models", text_range=TextRange(start=12277, end=12284)),
        TokenizationToken(text=".", text_range=TextRange(start=12284, end=12285)),
        TokenizationToken(text="The", text_range=TextRange(start=12285, end=12288)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=12288, end=12299)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=12299, end=12311)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=12311, end=12322)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=12322, end=12329)),
        TokenizationToken(text="▁", text_range=TextRange(start=12329, end=12330)),
        TokenizationToken(text="(", text_range=TextRange(start=12330, end=12331)),
        TokenizationToken(text="CRF", text_range=TextRange(start=12331, end=12334)),
        TokenizationToken(text="M", text_range=TextRange(start=12334, end=12335)),
        TokenizationToken(text=")", text_range=TextRange(start=12335, end=12336)),
        TokenizationToken(text="▁is", text_range=TextRange(start=12336, end=12339)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=12339, end=12360)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=12360, end=12371)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=12371, end=12383)),
        TokenizationToken(text="▁the", text_range=TextRange(start=12383, end=12387)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=12387, end=12396)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=12396, end=12410)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=12410, end=12416)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=12416, end=12425)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=12425, end=12449)),
        TokenizationToken(text="▁", text_range=TextRange(start=12449, end=12450)),
        TokenizationToken(text="(", text_range=TextRange(start=12450, end=12451)),
        TokenizationToken(text="HAI", text_range=TextRange(start=12451, end=12454)),
        TokenizationToken(text=")", text_range=TextRange(start=12454, end=12455)),
        TokenizationToken(text="▁that", text_range=TextRange(start=12455, end=12460)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=12460, end=12473)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=12473, end=12485)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=12485, end=12497)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=12497, end=12507)),
        TokenizationToken(text=",", text_range=TextRange(start=12507, end=12508)),
        TokenizationToken(text="▁development", text_range=TextRange(start=12508, end=12520)),
        TokenizationToken(text=",", text_range=TextRange(start=12520, end=12521)),
        TokenizationToken(text="▁and", text_range=TextRange(start=12521, end=12525)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=12525, end=12539)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=12539, end=12550)),
        TokenizationToken(text="▁models", text_range=TextRange(start=12550, end=12557)),
        TokenizationToken(text=".", text_range=TextRange(start=12557, end=12558)),
        TokenizationToken(text="The", text_range=TextRange(start=12558, end=12561)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=12561, end=12572)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=12572, end=12584)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=12584, end=12595)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=12595, end=12602)),
        TokenizationToken(text="▁", text_range=TextRange(start=12602, end=12603)),
        TokenizationToken(text="(", text_range=TextRange(start=12603, end=12604)),
        TokenizationToken(text="CRF", text_range=TextRange(start=12604, end=12607)),
        TokenizationToken(text="M", text_range=TextRange(start=12607, end=12608)),
        TokenizationToken(text=")", text_range=TextRange(start=12608, end=12609)),
        TokenizationToken(text="▁is", text_range=TextRange(start=12609, end=12612)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=12612, end=12633)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=12633, end=12644)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=12644, end=12656)),
        TokenizationToken(text="▁the", text_range=TextRange(start=12656, end=12660)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=12660, end=12669)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=12669, end=12683)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=12683, end=12689)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=12689, end=12698)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=12698, end=12722)),
        TokenizationToken(text="▁", text_range=TextRange(start=12722, end=12723)),
        TokenizationToken(text="(", text_range=TextRange(start=12723, end=12724)),
        TokenizationToken(text="HAI", text_range=TextRange(start=12724, end=12727)),
        TokenizationToken(text=")", text_range=TextRange(start=12727, end=12728)),
        TokenizationToken(text="▁that", text_range=TextRange(start=12728, end=12733)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=12733, end=12746)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=12746, end=12758)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=12758, end=12770)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=12770, end=12780)),
        TokenizationToken(text=",", text_range=TextRange(start=12780, end=12781)),
        TokenizationToken(text="▁development", text_range=TextRange(start=12781, end=12793)),
        TokenizationToken(text=",", text_range=TextRange(start=12793, end=12794)),
        TokenizationToken(text="▁and", text_range=TextRange(start=12794, end=12798)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=12798, end=12812)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=12812, end=12823)),
        TokenizationToken(text="▁models", text_range=TextRange(start=12823, end=12830)),
        TokenizationToken(text=".", text_range=TextRange(start=12830, end=12831)),
        TokenizationToken(text="The", text_range=TextRange(start=12831, end=12834)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=12834, end=12845)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=12845, end=12857)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=12857, end=12868)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=12868, end=12875)),
        TokenizationToken(text="▁", text_range=TextRange(start=12875, end=12876)),
        TokenizationToken(text="(", text_range=TextRange(start=12876, end=12877)),
        TokenizationToken(text="CRF", text_range=TextRange(start=12877, end=12880)),
        TokenizationToken(text="M", text_range=TextRange(start=12880, end=12881)),
        TokenizationToken(text=")", text_range=TextRange(start=12881, end=12882)),
        TokenizationToken(text="▁is", text_range=TextRange(start=12882, end=12885)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=12885, end=12906)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=12906, end=12917)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=12917, end=12929)),
        TokenizationToken(text="▁the", text_range=TextRange(start=12929, end=12933)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=12933, end=12942)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=12942, end=12956)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=12956, end=12962)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=12962, end=12971)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=12971, end=12995)),
        TokenizationToken(text="▁", text_range=TextRange(start=12995, end=12996)),
        TokenizationToken(text="(", text_range=TextRange(start=12996, end=12997)),
        TokenizationToken(text="HAI", text_range=TextRange(start=12997, end=13000)),
        TokenizationToken(text=")", text_range=TextRange(start=13000, end=13001)),
        TokenizationToken(text="▁that", text_range=TextRange(start=13001, end=13006)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=13006, end=13019)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=13019, end=13031)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=13031, end=13043)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=13043, end=13053)),
        TokenizationToken(text=",", text_range=TextRange(start=13053, end=13054)),
        TokenizationToken(text="▁development", text_range=TextRange(start=13054, end=13066)),
        TokenizationToken(text=",", text_range=TextRange(start=13066, end=13067)),
        TokenizationToken(text="▁and", text_range=TextRange(start=13067, end=13071)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=13071, end=13085)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=13085, end=13096)),
        TokenizationToken(text="▁models", text_range=TextRange(start=13096, end=13103)),
        TokenizationToken(text=".", text_range=TextRange(start=13103, end=13104)),
        TokenizationToken(text="The", text_range=TextRange(start=13104, end=13107)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=13107, end=13118)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=13118, end=13130)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=13130, end=13141)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=13141, end=13148)),
        TokenizationToken(text="▁", text_range=TextRange(start=13148, end=13149)),
        TokenizationToken(text="(", text_range=TextRange(start=13149, end=13150)),
        TokenizationToken(text="CRF", text_range=TextRange(start=13150, end=13153)),
        TokenizationToken(text="M", text_range=TextRange(start=13153, end=13154)),
        TokenizationToken(text=")", text_range=TextRange(start=13154, end=13155)),
        TokenizationToken(text="▁is", text_range=TextRange(start=13155, end=13158)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=13158, end=13179)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=13179, end=13190)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=13190, end=13202)),
        TokenizationToken(text="▁the", text_range=TextRange(start=13202, end=13206)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=13206, end=13215)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=13215, end=13229)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=13229, end=13235)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=13235, end=13244)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=13244, end=13268)),
        TokenizationToken(text="▁", text_range=TextRange(start=13268, end=13269)),
        TokenizationToken(text="(", text_range=TextRange(start=13269, end=13270)),
        TokenizationToken(text="HAI", text_range=TextRange(start=13270, end=13273)),
        TokenizationToken(text=")", text_range=TextRange(start=13273, end=13274)),
        TokenizationToken(text="▁that", text_range=TextRange(start=13274, end=13279)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=13279, end=13292)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=13292, end=13304)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=13304, end=13316)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=13316, end=13326)),
        TokenizationToken(text=",", text_range=TextRange(start=13326, end=13327)),
        TokenizationToken(text="▁development", text_range=TextRange(start=13327, end=13339)),
        TokenizationToken(text=",", text_range=TextRange(start=13339, end=13340)),
        TokenizationToken(text="▁and", text_range=TextRange(start=13340, end=13344)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=13344, end=13358)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=13358, end=13369)),
        TokenizationToken(text="▁models", text_range=TextRange(start=13369, end=13376)),
        TokenizationToken(text=".", text_range=TextRange(start=13376, end=13377)),
        TokenizationToken(text="The", text_range=TextRange(start=13377, end=13380)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=13380, end=13391)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=13391, end=13403)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=13403, end=13414)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=13414, end=13421)),
        TokenizationToken(text="▁", text_range=TextRange(start=13421, end=13422)),
        TokenizationToken(text="(", text_range=TextRange(start=13422, end=13423)),
        TokenizationToken(text="CRF", text_range=TextRange(start=13423, end=13426)),
        TokenizationToken(text="M", text_range=TextRange(start=13426, end=13427)),
        TokenizationToken(text=")", text_range=TextRange(start=13427, end=13428)),
        TokenizationToken(text="▁is", text_range=TextRange(start=13428, end=13431)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=13431, end=13452)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=13452, end=13463)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=13463, end=13475)),
        TokenizationToken(text="▁the", text_range=TextRange(start=13475, end=13479)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=13479, end=13488)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=13488, end=13502)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=13502, end=13508)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=13508, end=13517)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=13517, end=13541)),
        TokenizationToken(text="▁", text_range=TextRange(start=13541, end=13542)),
        TokenizationToken(text="(", text_range=TextRange(start=13542, end=13543)),
        TokenizationToken(text="HAI", text_range=TextRange(start=13543, end=13546)),
        TokenizationToken(text=")", text_range=TextRange(start=13546, end=13547)),
        TokenizationToken(text="▁that", text_range=TextRange(start=13547, end=13552)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=13552, end=13565)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=13565, end=13577)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=13577, end=13589)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=13589, end=13599)),
        TokenizationToken(text=",", text_range=TextRange(start=13599, end=13600)),
        TokenizationToken(text="▁development", text_range=TextRange(start=13600, end=13612)),
        TokenizationToken(text=",", text_range=TextRange(start=13612, end=13613)),
        TokenizationToken(text="▁and", text_range=TextRange(start=13613, end=13617)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=13617, end=13631)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=13631, end=13642)),
        TokenizationToken(text="▁models", text_range=TextRange(start=13642, end=13649)),
        TokenizationToken(text=".", text_range=TextRange(start=13649, end=13650)),
        TokenizationToken(text="The", text_range=TextRange(start=13650, end=13653)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=13653, end=13664)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=13664, end=13676)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=13676, end=13687)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=13687, end=13694)),
        TokenizationToken(text="▁", text_range=TextRange(start=13694, end=13695)),
        TokenizationToken(text="(", text_range=TextRange(start=13695, end=13696)),
        TokenizationToken(text="CRF", text_range=TextRange(start=13696, end=13699)),
        TokenizationToken(text="M", text_range=TextRange(start=13699, end=13700)),
        TokenizationToken(text=")", text_range=TextRange(start=13700, end=13701)),
        TokenizationToken(text="▁is", text_range=TextRange(start=13701, end=13704)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=13704, end=13725)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=13725, end=13736)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=13736, end=13748)),
        TokenizationToken(text="▁the", text_range=TextRange(start=13748, end=13752)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=13752, end=13761)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=13761, end=13775)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=13775, end=13781)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=13781, end=13790)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=13790, end=13814)),
        TokenizationToken(text="▁", text_range=TextRange(start=13814, end=13815)),
        TokenizationToken(text="(", text_range=TextRange(start=13815, end=13816)),
        TokenizationToken(text="HAI", text_range=TextRange(start=13816, end=13819)),
        TokenizationToken(text=")", text_range=TextRange(start=13819, end=13820)),
        TokenizationToken(text="▁that", text_range=TextRange(start=13820, end=13825)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=13825, end=13838)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=13838, end=13850)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=13850, end=13862)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=13862, end=13872)),
        TokenizationToken(text=",", text_range=TextRange(start=13872, end=13873)),
        TokenizationToken(text="▁development", text_range=TextRange(start=13873, end=13885)),
        TokenizationToken(text=",", text_range=TextRange(start=13885, end=13886)),
        TokenizationToken(text="▁and", text_range=TextRange(start=13886, end=13890)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=13890, end=13904)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=13904, end=13915)),
        TokenizationToken(text="▁models", text_range=TextRange(start=13915, end=13922)),
        TokenizationToken(text=".", text_range=TextRange(start=13922, end=13923)),
        TokenizationToken(text="The", text_range=TextRange(start=13923, end=13926)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=13926, end=13937)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=13937, end=13949)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=13949, end=13960)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=13960, end=13967)),
        TokenizationToken(text="▁", text_range=TextRange(start=13967, end=13968)),
        TokenizationToken(text="(", text_range=TextRange(start=13968, end=13969)),
        TokenizationToken(text="CRF", text_range=TextRange(start=13969, end=13972)),
        TokenizationToken(text="M", text_range=TextRange(start=13972, end=13973)),
        TokenizationToken(text=")", text_range=TextRange(start=13973, end=13974)),
        TokenizationToken(text="▁is", text_range=TextRange(start=13974, end=13977)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=13977, end=13998)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=13998, end=14009)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=14009, end=14021)),
        TokenizationToken(text="▁the", text_range=TextRange(start=14021, end=14025)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=14025, end=14034)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=14034, end=14048)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=14048, end=14054)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=14054, end=14063)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=14063, end=14087)),
        TokenizationToken(text="▁", text_range=TextRange(start=14087, end=14088)),
        TokenizationToken(text="(", text_range=TextRange(start=14088, end=14089)),
        TokenizationToken(text="HAI", text_range=TextRange(start=14089, end=14092)),
        TokenizationToken(text=")", text_range=TextRange(start=14092, end=14093)),
        TokenizationToken(text="▁that", text_range=TextRange(start=14093, end=14098)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=14098, end=14111)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=14111, end=14123)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=14123, end=14135)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=14135, end=14145)),
        TokenizationToken(text=",", text_range=TextRange(start=14145, end=14146)),
        TokenizationToken(text="▁development", text_range=TextRange(start=14146, end=14158)),
        TokenizationToken(text=",", text_range=TextRange(start=14158, end=14159)),
        TokenizationToken(text="▁and", text_range=TextRange(start=14159, end=14163)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=14163, end=14177)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=14177, end=14188)),
        TokenizationToken(text="▁models", text_range=TextRange(start=14188, end=14195)),
        TokenizationToken(text=".", text_range=TextRange(start=14195, end=14196)),
        TokenizationToken(text="The", text_range=TextRange(start=14196, end=14199)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=14199, end=14210)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=14210, end=14222)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=14222, end=14233)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=14233, end=14240)),
        TokenizationToken(text="▁", text_range=TextRange(start=14240, end=14241)),
        TokenizationToken(text="(", text_range=TextRange(start=14241, end=14242)),
        TokenizationToken(text="CRF", text_range=TextRange(start=14242, end=14245)),
        TokenizationToken(text="M", text_range=TextRange(start=14245, end=14246)),
        TokenizationToken(text=")", text_range=TextRange(start=14246, end=14247)),
        TokenizationToken(text="▁is", text_range=TextRange(start=14247, end=14250)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=14250, end=14271)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=14271, end=14282)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=14282, end=14294)),
        TokenizationToken(text="▁the", text_range=TextRange(start=14294, end=14298)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=14298, end=14307)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=14307, end=14321)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=14321, end=14327)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=14327, end=14336)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=14336, end=14360)),
        TokenizationToken(text="▁", text_range=TextRange(start=14360, end=14361)),
        TokenizationToken(text="(", text_range=TextRange(start=14361, end=14362)),
        TokenizationToken(text="HAI", text_range=TextRange(start=14362, end=14365)),
        TokenizationToken(text=")", text_range=TextRange(start=14365, end=14366)),
        TokenizationToken(text="▁that", text_range=TextRange(start=14366, end=14371)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=14371, end=14384)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=14384, end=14396)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=14396, end=14408)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=14408, end=14418)),
        TokenizationToken(text=",", text_range=TextRange(start=14418, end=14419)),
        TokenizationToken(text="▁development", text_range=TextRange(start=14419, end=14431)),
        TokenizationToken(text=",", text_range=TextRange(start=14431, end=14432)),
        TokenizationToken(text="▁and", text_range=TextRange(start=14432, end=14436)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=14436, end=14450)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=14450, end=14461)),
        TokenizationToken(text="▁models", text_range=TextRange(start=14461, end=14468)),
        TokenizationToken(text=".", text_range=TextRange(start=14468, end=14469)),
        TokenizationToken(text="The", text_range=TextRange(start=14469, end=14472)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=14472, end=14483)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=14483, end=14495)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=14495, end=14506)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=14506, end=14513)),
        TokenizationToken(text="▁", text_range=TextRange(start=14513, end=14514)),
        TokenizationToken(text="(", text_range=TextRange(start=14514, end=14515)),
        TokenizationToken(text="CRF", text_range=TextRange(start=14515, end=14518)),
        TokenizationToken(text="M", text_range=TextRange(start=14518, end=14519)),
        TokenizationToken(text=")", text_range=TextRange(start=14519, end=14520)),
        TokenizationToken(text="▁is", text_range=TextRange(start=14520, end=14523)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=14523, end=14544)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=14544, end=14555)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=14555, end=14567)),
        TokenizationToken(text="▁the", text_range=TextRange(start=14567, end=14571)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=14571, end=14580)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=14580, end=14594)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=14594, end=14600)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=14600, end=14609)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=14609, end=14633)),
        TokenizationToken(text="▁", text_range=TextRange(start=14633, end=14634)),
        TokenizationToken(text="(", text_range=TextRange(start=14634, end=14635)),
        TokenizationToken(text="HAI", text_range=TextRange(start=14635, end=14638)),
        TokenizationToken(text=")", text_range=TextRange(start=14638, end=14639)),
        TokenizationToken(text="▁that", text_range=TextRange(start=14639, end=14644)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=14644, end=14657)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=14657, end=14669)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=14669, end=14681)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=14681, end=14691)),
        TokenizationToken(text=",", text_range=TextRange(start=14691, end=14692)),
        TokenizationToken(text="▁development", text_range=TextRange(start=14692, end=14704)),
        TokenizationToken(text=",", text_range=TextRange(start=14704, end=14705)),
        TokenizationToken(text="▁and", text_range=TextRange(start=14705, end=14709)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=14709, end=14723)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=14723, end=14734)),
        TokenizationToken(text="▁models", text_range=TextRange(start=14734, end=14741)),
        TokenizationToken(text=".", text_range=TextRange(start=14741, end=14742)),
        TokenizationToken(text="The", text_range=TextRange(start=14742, end=14745)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=14745, end=14756)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=14756, end=14768)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=14768, end=14779)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=14779, end=14786)),
        TokenizationToken(text="▁", text_range=TextRange(start=14786, end=14787)),
        TokenizationToken(text="(", text_range=TextRange(start=14787, end=14788)),
        TokenizationToken(text="CRF", text_range=TextRange(start=14788, end=14791)),
        TokenizationToken(text="M", text_range=TextRange(start=14791, end=14792)),
        TokenizationToken(text=")", text_range=TextRange(start=14792, end=14793)),
        TokenizationToken(text="▁is", text_range=TextRange(start=14793, end=14796)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=14796, end=14817)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=14817, end=14828)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=14828, end=14840)),
        TokenizationToken(text="▁the", text_range=TextRange(start=14840, end=14844)),
        TokenizationToken(text="▁Stanford", text_range=TextRange(start=14844, end=14853)),
        TokenizationToken(text="▁Institute▁for", text_range=TextRange(start=14853, end=14867)),
        TokenizationToken(text="▁Human", text_range=TextRange(start=14867, end=14873)),
        TokenizationToken(text="-Centered", text_range=TextRange(start=14873, end=14882)),
        TokenizationToken(text="▁Artificial▁Intelligence", text_range=TextRange(start=14882, end=14906)),
        TokenizationToken(text="▁", text_range=TextRange(start=14906, end=14907)),
        TokenizationToken(text="(", text_range=TextRange(start=14907, end=14908)),
        TokenizationToken(text="HAI", text_range=TextRange(start=14908, end=14911)),
        TokenizationToken(text=")", text_range=TextRange(start=14911, end=14912)),
        TokenizationToken(text="▁that", text_range=TextRange(start=14912, end=14917)),
        TokenizationToken(text="▁aims▁to▁make", text_range=TextRange(start=14917, end=14930)),
        TokenizationToken(text="▁fundamental", text_range=TextRange(start=14930, end=14942)),
        TokenizationToken(text="▁advances▁in", text_range=TextRange(start=14942, end=14954)),
        TokenizationToken(text="▁the▁study", text_range=TextRange(start=14954, end=14964)),
        TokenizationToken(text=",", text_range=TextRange(start=14964, end=14965)),
        TokenizationToken(text="▁development", text_range=TextRange(start=14965, end=14977)),
        TokenizationToken(text=",", text_range=TextRange(start=14977, end=14978)),
        TokenizationToken(text="▁and", text_range=TextRange(start=14978, end=14982)),
        TokenizationToken(text="▁deployment▁of", text_range=TextRange(start=14982, end=14996)),
        TokenizationToken(text="▁foundation", text_range=TextRange(start=14996, end=15007)),
        TokenizationToken(text="▁models", text_range=TextRange(start=15007, end=15014)),
        TokenizationToken(text=".", text_range=TextRange(start=15014, end=15015)),
        TokenizationToken(text="The", text_range=TextRange(start=15015, end=15018)),
        TokenizationToken(text="▁Center▁for", text_range=TextRange(start=15018, end=15029)),
        TokenizationToken(text="▁Research▁on", text_range=TextRange(start=15029, end=15041)),
        TokenizationToken(text="▁Foundation", text_range=TextRange(start=15041, end=15052)),
        TokenizationToken(text="▁Models", text_range=TextRange(start=15052, end=15059)),
        TokenizationToken(text="▁", text_range=TextRange(start=15059, end=15060)),
        TokenizationToken(text="(", text_range=TextRange(start=15060, end=15061)),
        TokenizationToken(text="CRF", text_range=TextRange(start=15061, end=15064)),
        TokenizationToken(text="M", text_range=TextRange(start=15064, end=15065)),
        TokenizationToken(text=")", text_range=TextRange(start=15065, end=15066)),
        TokenizationToken(text="▁is", text_range=TextRange(start=15066, end=15069)),
        TokenizationToken(text="▁an▁interdisciplinary", text_range=TextRange(start=15069, end=15090)),
        TokenizationToken(text="▁initiative", text_range=TextRange(start=15090, end=15101)),
        TokenizationToken(text="▁born▁out▁of", text_range=TextRange(start=15101, end=15113)),
    ],
)


class TestAI21Tokenizer:
    def setup_method(self):
        self.tokenizer = TokenizerFactory.get_tokenizer("ai21/j1-jumbo", service)

    @mock.patch("proxy.tokenizer.ai21_tokenizer.TokenizerService.tokenize", return_value=REQUEST_RESULT)
    def test_encode(self, mock_tokenize):
        assert self.tokenizer.encode(TEST_PROMPT).tokens == TEST_TOKEN_REPRESENTATIONS

    def test_decode(self):
        assert self.tokenizer.decode(TEST_TOKEN_REPRESENTATIONS, TEST_PROMPT) == TEST_PROMPT
        assert self.tokenizer.decode(TEST_TOKEN_REPRESENTATIONS, TEST_PROMPT)[:-1] == TEST_PROMPT[:-1]

    @mock.patch("proxy.tokenizer.ai21_tokenizer.TokenizerService.tokenize", return_value=REQUEST_RESULT)
    def test_tokenize(self, mock_tokenize):
        assert self.tokenizer.tokenize(TEST_PROMPT) == TEST_TOKENS

    @mock.patch("proxy.tokenizer.ai21_tokenizer.TokenizerService.tokenize", return_value=REQUEST_RESULT)
    def test_fits_within_context_window(self, mock_tokenize):
        # Should fit in the context window since we subtracted the number of tokens of the test prompt
        # from the max context window
        assert self.tokenizer.fits_within_context_window(TEST_PROMPT, 2048 - 36)
        # Should not fit in the context window because we're expecting one more extra token in the completion
        assert not self.tokenizer.fits_within_context_window(TEST_PROMPT, 2048 - 36 + 1)

    @mock.patch(
        "proxy.tokenizer.ai21_tokenizer.TokenizerService.tokenize",
        side_effect=[LONG_REQUEST_RESULT, LONG_REQUEST_RESULT, TRUNCATED_REQUEST_RESULT, TRUNCATED_REQUEST_RESULT],
    )
    def test_truncate_from_right(self, mock_tokenize):
        # Create a prompt that exceed max context length: 36 * 57 = 2052 tokens.
        # Our naive concatenation of the strings here also leads to extra tokens.
        long_prompt: str = TEST_PROMPT * 57
        assert not self.tokenizer.fits_within_context_window(long_prompt)

        # Truncate and ensure it fits within the context window
        truncated_long_prompt: str = self.tokenizer.truncate_from_right(long_prompt)
        assert self.tokenizer.tokenize_and_count(truncated_long_prompt) == 2048
        assert self.tokenizer.fits_within_context_window(truncated_long_prompt)

    @mock.patch("proxy.tokenizer.ai21_tokenizer.TokenizerService.tokenize", return_value=REQUEST_RESULT)
    def test_tokenize_and_count(self, mock_tokenize):
        assert self.tokenizer.tokenize_and_count(TEST_PROMPT) == 36
