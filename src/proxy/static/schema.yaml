adapter:
- name: method
  description: The high-level strategy for converting instances into a prompt for the language model.
  values:
  - name: generation
    description: Given the input, the model generates the output free-form.
  - name: multiple_choice_joint
    description: Given the input, the model selects from multiple-choice options (A., B., C., D., E.).
  - name: multiple_choice_separate_original
    description: Given the input and answer choice, the model assigns the sequence a probability.
  - name: multiple_choice_separate_calibrated
    description: Given the input and answer choice, the model assigns the sequence a probability calibrated by answer choice probability.
  - name: language_modeling
    description: Given the input, the model assigns the sequence a probability.
- name: instructions
  description: The description of the task that is included at the very beginning of the prompt.
- name: input_prefix
  description: The string that is included before each input (e.g., 'Question:').
- name: reference_prefix
  description: The string that is included before each reference (for multiple-choice questions).
- name: output_prefix
  description: The string that is included before the correct answer/predicted output (e.g., 'Answer:').
- name: max_train_instances
  description: Maximum number of training instances to include in the prompt (currently by randomly sampling).
- name: max_eval_instances
  description: Maximum number of instances to evaluate on (over all splits - test, valid, etc.).
- name: num_outputs
  description: Maximum number of possible outputs to generate (either by sampling or top k).
- name: num_train_trials
  description: Number of trials, where in each trial we choose an independent, random set of training instances. Used to compute variance.
- name: model
  description: Name of the language model (<organization>/<model name>) to send requests to.
- name: temperature
  description: Temperature parameter used in generation.
- name: max_tokens
  description: Maximum number of tokens to generate.
- name: stop_sequences
  description: List of sequences, where we stop generation if we encounter any of them.

# TODO: Can we add "display_name" as a core "Field" field? Similar to "name".
metrics:

# Infrastructure metrics:
# TODO

# Accuracy metrics:
- name: exact_match
  description: Measures the average probability over test instances that the predicted output matches a correct reference exactly.
- name: quasi_exact_match
  description: Measures the average probability over test instances that the predicted output matches a correct reference up to light processing.
- name: logprob
  description: Measures the average probability over test instances that the predicted output's log probability (input's log prob for language modeling).
- name: num_perplexity_tokens
  description: Measures the average number of tokens in the output (for language modeling, the input too) across test instances.
- name: num_bytes
  description: Measures the average number of bytes in the output (for language modeling, the input too) across test instances.

# Bias metrics:
- name: bias_metric:mode=associations,demographic_category=race 
  description: Measures uneven association of racial groups (Asian, Hispanic, White) with target professions.
  values:
    display_name: Stereotypical associations (Race, Profession)
- name: bias_metric:mode=associations,demographic_category=gender
  description: Measures uneven association of gender groups (male, female) with target professions.
  values:
    display_name: Stereotypical associations (Gender, Profession)
- name: bias_metric:mode=representation,demographic_category=race
  description: Measures uneven representation of racial groups (Asian, Hispanic, White).
  values:
    display_name: Demographic representation (Race)
- name: bias_metric:mode=representation,demographic_category=gender
  description: Measures uneven representation of gender groups (male, female).
  values:
    display_name: Demographic representation (Gender)

# Toxicity metrics
- name: toxicity_probability
  description: Measures likelihood of toxicity (based on PerspectiveAPI toxicity classifer) in model generations.
  values:
    display_name: Toxicity probability

# Efficiency metrics:
- name: training_co2_cost
  description: Estimates cost of training in CO2.
  values:
    display_name: Estimated training cost (kg CO2)
- name: inference_runtime
  description: Measures the average observed runtime for queries in the scenario to the model.  
  values:
    display_name: Observed inference runtime (s)
- name: inference_idealized_runtime
  description: Estimates the idealized runtime for queries to the model given its architecture.
  values:
    display_name: Idealized inference runtime (s) 

# Perturbations (used to determine robustness and fairness metrics)
perturbations:
- name: TyposPerturbation
  description: Randomly perturbs tokens in the input with p = 0.05. 
  values:
    display_name: Typos
- name: SynonymPerturbation
  description: Randomly substitutes words in the input with WordNet synonyms with p = 0.5.
  values:
    display_name: Synonym
- name: dialect
  description: Deterministically substitutes SAE words in input with AAE counterparts using validated dictionary of Ziems et al. (2022).
  values:
    display_name: SAE -> AAE
- name: person_name
  description: Deterministically substitutes White first names with Black first names sampled from lists of Caliskan et al. (2017).
  values:
    display_name: First names by race (White -> Black)
- name: gender_term 
  description: Deterministically substitutes male gender terms with female gender terms from dictionaries of Garg et al. (2018) and Bolukbasi et al. (2016).
  values:
    display_name: Terms by gender (Male -> Female)


groups:
- name: APPS (Code)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - test_avg
        - strict_acc
- name: bAbI
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: BBQ
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bbq_metric_ambiguous_bias
        - bbq_metric_unambiguous_bias
- name: BLiMP
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - accuracy
- name: BOLD
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - toxicity_probability
- name: BoolQ
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - quasi_exact_match
- name: CivilComments
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: CNN/DailyMail
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - rouge_2
- name: CommonsenseQA
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - quasi_exact_match
- name: Copyright
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - longest_common_prefix_length
        - edit_distance
- name: Data imputation
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: Disinformation (reiteration)
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - self_bleu
        - monte_carlo_entropy
- name: Disinformation (wedging)
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - self_bleu
        - monte_carlo_entropy
- name: Dyck
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - exact_match_indicator
- name: Entity matching
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: GSM8K
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - exact_match_indicator
- name: HellaSwag
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - quasi_exact_match
- name: HumanEval (Code)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - code_eval_acc
        - pass
- name: ICE
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (Canada)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (Hong Kong)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (India)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (Japan)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (Phillippines)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (Singapore)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (USA)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (spoken)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (written)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (female)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (male)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: IMDB
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - quasi_exact_match
- name: LegalSupport
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: LSAT
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: MATH
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - math_equiv
- name: MATH (chain_of_thought)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - math_equiv_chain_of_thought
- name: MMLU
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: MS MARCO (regular)
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - RR@20
- name: MS MARCO (TREC)
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - NDCG@20
- name: NarrativeQA
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - f1_score
- name: NaturalQuestions
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - f1_score
- name: NaturalQuestions (closed-book)
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - f1_score
- name: NaturalQuestions (open-book)
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - f1_score
- name: NewsQA
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - f1_score
- name: Numeracy
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - absolute_value_difference
- name: OpenbookQA
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: QuAC
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - f1_score
- name: RAFT
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: RealToxicityPrompts
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - toxicity_probability
- name: Synthetic reasoning (abstract symbols)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: Synthetic reasoning (natural language)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - f1_set_match
- name: The Pile
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: TruthfulQA
  description: TODO
  values:
    display:
      k: null
      split: valid
      stat_names:
        - quasi_exact_match
- name: Twitter AAE
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: Twitter AAE (AAE)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: Twitter AAE (White)
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: WikiFact
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: WikiText-103
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: XSUM
  description: TODO
  values:
    display:
      k: null
      split: test
      stat_names:
        - rouge_2

table_settings:
- name: default
  values:
    stat_groups:
      - accuracy
      - bias
      - toxicity
      - efficiency

stat_groups:
- name: accuracy
  description: Average performance of model using canonical metric for scenario (e.g. accuracy, F1, ROUGE).
  values:
    display_name: Accuracy
    # stat_names are determined by group.stat_names
    perturbation_names:
    - identity
    - TyposPerturbation
    - SynonymPerturbation
    - dialect
    - person_name
    - gender_term 
- name: bias
  description: Biases (stereotypical associations, demographic associations) in model generations.
  values:
    display_name: Bias
    stat_names:
    - bias_metric:mode=associations,demographic_category=race
    - bias_metric:mode=associations,demographic_category=gender
    - bias_metric:mode=representation,demographic_category=race
    - bias_metric:mode=representation,demographic_category=gender
    perturbation_names:
    - identity
- name: toxicity
  description: Toxicity in model generations.
  values:
    display_name: Toxicity
    stat_names:
    - toxicity_probability
    perturbation_names:
    - identity
- name: efficiency
  description: The efficiency of the model across both training and inference.
  values:
    display_name: Efficiency
    stat_names:
    - inference_runtime
    - inference_idealized_runtime
    - training_co2_cost
    perturbation_names:
    - identity