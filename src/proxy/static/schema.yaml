adapter:
- name: method
  description: The high-level strategy for converting instances into a prompt for the language model.
  values:
  - name: generation
    description: Given the input, the model generates the output free-form.
  - name: multiple_choice_joint
    description: Given the input, the model selects from multiple-choice options (A., B., C., D., E.).
  - name: multiple_choice_separate_original
    description: For each answer choice, the model assigns the input and answer choice a probability, returning the answer with maximum probability.
  - name: multiple_choice_separate_calibrated
    description: For each answer choice, the model assigns the input and answer choice a probability, returning the answer with maximum probability when calibrated by answer choice probability.
  - name: language_modeling
    description: Given the input, the model assigns the sequence a probability.
- name: instructions
  description: The description of the task that is included at the very beginning of the prompt.
- name: input_prefix
  description: The string that is included before each input (e.g., 'Question:').
- name: reference_prefix
  description: The string that is included before each reference (for multiple-choice questions).
- name: output_prefix
  description: The string that is included before the correct answer/predicted output (e.g., 'Answer:').
- name: max_train_instances
  description: Maximum number of training instances to include in the prompt (currently by randomly sampling).
- name: max_eval_instances
  description: Maximum number of instances to evaluate on (over all splits - test, valid, etc.).
- name: num_outputs
  description: Maximum number of possible outputs to generate by sampling multiple outputs.
- name: num_train_trials
  description: Number of trials, where in each trial we choose an independent, random set of training instances. Used to compute variance.
- name: model
  description: Name of the language model (<organization>/<model name>) to send requests to.
- name: temperature
  description: Temperature parameter used in generation.
- name: max_tokens
  description: Maximum number of tokens to generate.
- name: stop_sequences
  description: List of sequences, where we stop generation if we encounter any of them.


metrics:
# Infrastructure metrics:
# TODO
- name: num_perplexity_tokens
  description: Average number of tokens in the predicted output (for language modeling, the input too).
  display_name: '# tokens'
- name: num_bytes
  description: Average number of bytes in the predicted output (for language modeling, the input too).
  display_name: '# bytes'

# Accuracy metrics:
- name: exact_match
  description: Fraction of instances that the predicted output matches a correct reference exactly.
  display_name: Exact match
- name: quasi_exact_match
  description: Fraction of instances that the predicted output matches a correct reference up to light processing.
  display_name: Quasi-exact match
- name: logprob
  description: Predicted output's average log probability (input's log prob for language modeling).
  display_name: Log probability
- name: accuracy
  description: Fraction of instances that the model correctly classifies based on sequence probability. 
  display_name: Accuracy
- name: bits_per_byte
  description: Average number of bits per byte according to model probabilities.
  display_name: Bits/byte
- name: rouge_2
  description: Average ROUGE score (Lin, 2004) based on 2-gram overlap.
  display_name: ROUGE-2
- name: f1_set_match
  description: TODO
  display_name: F1 (set match)
- name: f1_score
  description: Average F1 score in terms of word overlap between the model output and correct reference.
  display_name: F1
- name: absolute_value_difference
  description: Average absolute difference between the model output (converted to a number) and the correct reference.
  display_name: Absolute difference 
- name: NDCG@20
  description: Normalized discounted cumulative gain at 20 in information retrieval.
  display_name: NDCG@20
- name: RR@20
  description: Mean reciprocal rank at 20 in information retrieval.
  display_name: RR@20
- name: math_equiv
  description: Fraction of model outputs that are mathematically equivalent to the correct reference.
  display_name: Equivalent
- name: math_equiv_chain_of_thought
  description: Fraction of model outputs that are mathematically equivalent to the correct reference when using chain-of-thoughts prompting.
  display_name: Equivalent (chain-of-thoughts)
- name: code_eval_acc
  description: TODO
  display_name: Accuracy (code evaluation)
- name: pass
  description: TODO
- name: exact_match_indicator
  description: TODO
  display_name: TODO
- name: test_avg
  description: TODO
  display_name: TODO
- name: strict_acc
  description: TODO
  display_name: TODO

# Bias metrics:
- name: bias_metric:mode=associations,demographic_category=race 
  description: Measures uneven association of racial groups (Asian, Hispanic, White) with target professions.
  display_name: Stereotypical associations (Race, Profession)
- name: bias_metric:mode=associations,demographic_category=gender
  description: Measures uneven association of gender groups (male, female) with target professions.
  display_name: Stereotypical associations (Gender, Profession)
- name: bias_metric:mode=representation,demographic_category=race
  description: Measures uneven representation of racial groups (Asian, Hispanic, White).
  display_name: Demographic representation (Race)
- name: bias_metric:mode=representation,demographic_category=gender
  description: Measures uneven representation of gender groups (male, female).
  display_name: Demographic representation (Gender)
- name: bbq_metric_ambiguous_bias
  description: Metric of Parrish et al. (2022) for BBQ on ambiguous examples.
  display_name: BBQ (ambiguous)
- name: bbq_metric_unambiguous_bias
  description: Metric of Parrish et al. (2022) for BBQ on unambiguous examples.
  display_name: BBQ (unambiguous)

# Toxicity metrics
- name: toxicity_probability
  description: Fraction of model outputs that are toxic (based on the PerspectiveAPI toxicity classifer).
  display_name: Toxicity probability

# Efficiency metrics:
- name: training_co2_cost
  description: Estimate of the cost of training in CO2.
  display_name: Estimated training cost (kg CO2)
- name: inference_runtime
  description: Average observed runtime for requests to the model.  
  display_name: Observed inference runtime (s)
- name: inference_idealized_runtime
  description: Estimate of the average idealized runtime for requests to the model given its architecture.
  display_name: Idealized inference runtime (s)

# Disinformation metrics (measure diversity):
- name: self_bleu
  description: Self-BLEU metric of Zhu et al. (2018) as a measure of diversity in model generations.
  display_name: Self-BLEU
- name: monte_carlo_entropy
  description: Monte Carlo estimate of the entropy as a measure of diversity in model generations.
  display_name: Entropy (Monte Carlo)


# Copyright metrics (measure copying/overlap):
- name: longest_common_prefix_length
  description: Average length of longest common prefix between model generation and reference.
  display_name: Longest common prefix length
- name: edit_distance
  description: Average Levenshtein edit distance between model generation and reference.
  display_name: Edit distance (Levenshtein)


# Perturbations (used to determine robustness and fairness metrics)
perturbations:
- name: TyposPerturbation
  description: Randomly adds typos to each token in the input with probability 0.05. 
  display_name: Typos
- name: SynonymPerturbation
  description: Randomly substitutes words in the input with WordNet synonyms with probability 0.5.
  display_name: Synonym
- name: dialect
  description: Deterministically substitutes SAE words in input with AAE counterparts using validated dictionary of Ziems et al. (2022).
  display_name: SAE -> AAE
- name: person_name
  description: Deterministically substitutes White first names with Black first names sampled from the lists of Caliskan et al. (2017).
  display_name: First names by race (White -> Black)
- name: gender 
  description: Deterministically substitutes male gender terms with female gender terms from dictionaries of Garg et al. (2018) and Bolukbasi et al. (2016).
  display_name: Terms by gender (Male -> Female)


groups:
- name: APPS (Code)
  description: The APPS benchmark for measuring competence on code challenges (Hendrycks et al., 2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - test_avg
        - strict_acc
- name: bAbI
  description: The bAbI benchmark for measuring understanding and reasoning (Weston et al., 2015).
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: BBQ
  description: The BBQ benchmark for measuring social bias in question answering in ambiguous and unambigous context (Parrish et al., 2022).
  values:
    display:
      k: null
      split: test
      stat_names:
        - bbq_metric_ambiguous_bias
        - bbq_metric_unambiguous_bias
- name: BLiMP
  description: The BLiMP benchmark for measuring performance on linguistic phenomena using minimal pair design (Warstadt et al., 2020). 
  values:
    display:
      k: null
      split: test
      stat_names:
        - accuracy
- name: BOLD
  description: The BOLD dataset for measuring biases and toxicity in open-ended language generation (Dhamala et al., 2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - toxicity_probability
- name: BoolQ
  description: The BoolQ benchmark for binary (yes/no) question answering (Clark et al., 2019).
  values:
    display:
      k: null
      split: valid
      stat_names:
        - quasi_exact_match
- name: CivilComments
  description: The CivilComments benchmark for toxicity detection (Borkan et al., 2019).
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: CNN/DailyMail
  description: The CNN/DailyMail benchmark for text summarization (Hermann et al., 2015; Nallapati et al.,2016). 
  values:
    display:
      k: null
      split: test
      stat_names:
        - rouge_2
- name: CommonsenseQA
  description: The CommonsenseQA benchmark for commonsense-inteisive question answering (Talmor et al., 2019).
  values:
    display:
      k: null
      split: valid
      stat_names:
        - quasi_exact_match
- name: Copyright
  description: Scenario introduced in this work to measure copyright and memorization behavior, based off of Carlini et al. (2021). 
  values:
    display:
      k: null
      split: test
      stat_names:
        - longest_common_prefix_length
        - edit_distance
- name: Data imputation
  description: Scenario from Mei et al. (2021) that tests the ability to impute missing entities in a data table. 
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: Disinformation (reiteration)
  description: Scenario from Buchanan et al. (2021) that tests the ability to reiterate disinformation content.
  values:
    display:
      k: null
      split: valid
      stat_names:
        - self_bleu
        - monte_carlo_entropy
- name: Disinformation (wedging)
  description: Scenario from Buchanan et al. (2021) that tests the ability to generate divisive and wedging content.
  values:
    display:
      k: null
      split: valid
      stat_names:
        - self_bleu
        - monte_carlo_entropy
- name: Dyck
  description: Scenario testing hierarchical reasoning through the Dyck formal languages (Suzgun et al., 2019).
  values:
    display:
      k: null
      split: test
      stat_names:
        - exact_match_indicator
- name: Entity matching
  description: Scenario from Mudgal et al. (2018) that tests the ability to determine if two entities match. 
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: GSM8K
  description: The GSM8K benchmark for testing mathematical reasoning on grade-school math problems (Cobbe et al., 2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - exact_match_indicator
- name: HellaSwag
  description: The HellaSwag benchmark for commonsense reasoning in question answering (Zellers et al., 2019).
  values:
    display:
      k: null
      split: valid
      stat_names:
        - quasi_exact_match
- name: HumanEval (Code)
  description: The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings (Chen et al., 2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - code_eval_acc
        - pass
- name: ICE
  description: The International Corpus of English (ICE) for measuring language model performance across English varieties (https://www.ice-corpora.uzh.ch/en.html).
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (Canada)
  description: The subset of the International Corpus of English (ICE) drawn from English speakers in Canada.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (Hong Kong)
  description: The subset of the International Corpus of English (ICE) drawn from English speakers in Hong Kong.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (India)
  description: The subset of the International Corpus of English (ICE) drawn from English speakers in India.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (Japan)
  description: The subset of the International Corpus of English (ICE) drawn from English speakers in Japan.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (Phillippines)
  description: The subset of the International Corpus of English (ICE) drawn from English speakers in the Philliphines.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (Singapore)
  description: The subset of the International Corpus of English (ICE) drawn from English speakers in Singapore.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (USA)
  description: The subset of the International Corpus of English (ICE) drawn from English speakers in the United States of America.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (spoken)
  description: The subset of the International Corpus of English (ICE) based on transcribed speech.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (written)
  description: The subset of the International Corpus of English (ICE) based on written text.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (female)
  description: The subset of the International Corpus of English (ICE) drawn from self-identified female speakers.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: ICE (male)
  description: The subset of the International Corpus of English (ICE) drawn from self-identified male speakers.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: IMDB
  description: The IMDB benchmark for sentiment analysis in movie review (Maas et al., 2011).
  values:
    display:
      k: null
      split: valid
      stat_names:
        - quasi_exact_match
- name: LegalSupport
  description: Scenario introduced in this work to measure fine-grained legal reasoning through reverse entailment.
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: LSAT
  description: The LSAT benchmark for measuring analytical reasoning on the Law School Admission Test (LSAT; Zhong et al., 2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: MATH
  description: The MATH benchmark for measuring mathematical problem solving on competition math problems (Hendrycks et al., 2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - math_equiv
- name: MATH (chain_of_thought)
  description: The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thoughts style reasoning (Hendrycks et al., 2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - math_equiv_chain_of_thought
- name: MMLU
  description: The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intesive question answering across 57 domains (Hendrycks et al., 2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: MS MARCO (regular)
  description: The MS MARCO regular benchmark for passage retrieval in information retrieval (https://microsoft.github.io/msmarco/).
  values:
    display:
      k: null
      split: valid
      stat_names:
        - RR@20
- name: MS MARCO (TREC)
  description: The MS MARCO deep learning TREC benchmark  for passage retrieval in information retrieval (https://trec.nist.gov).
  values:
    display:
      k: null
      split: valid
      stat_names:
        - NDCG@20
- name: NarrativeQA
  description: The NarrativeQA benchmark for reading comprehension over narratives (Kočiský et al., 2017).
  values:
    display:
      k: null
      split: test
      stat_names:
        - f1_score
- name: NaturalQuestions
  description: The NaturalQuestions benchmark for question answering based on naturally-occurring queries through Google Search (Kwiatkowski et al., 2019).
  values:
    display:
      k: null
      split: valid
      stat_names:
        - f1_score
- name: NaturalQuestions (closed-book)
  description: The variant of NaturalQuestions that does not include the Wikipedia page with the answer as input.
  values:
    display:
      k: null
      split: valid
      stat_names:
        - f1_score
- name: NaturalQuestions (open-book)
  description: The variant of NaturalQuestions that does include the Wikipedia page with the answer as input.
  values:
    display:
      k: null
      split: valid
      stat_names:
        - f1_score
- name: NewsQA
  description: The NewsQA benchmark for reading comprehension over CNN news articles (Trischler et al., 2017).
  values:
    display:
      k: null
      split: valid
      stat_names:
        - f1_score
- name: Numeracy
  description: Scenario introduced in this work to test numerical reasoning via symbolic regression.
  values:
    display:
      k: null
      split: test
      stat_names:
        - absolute_value_difference
- name: OpenbookQA
  description: The OpenbookQA benchmark for commonsense-intensive open book question answering (Mihaylov et al., 2018).
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: QuAC
  description: The QuAC benchmark for question answering in the context of dialogues (Choi et al., 2018).
  values:
    display:
      k: null
      split: valid
      stat_names:
        - f1_score
- name: RAFT
  description: The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks (Alex et al., 2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: RealToxicityPrompts
  description: The RealToxicityPrompts dataset for measuring toxicity in prompted model generations (Gehman et al., 2020).
  values:
    display:
      k: null
      split: test
      stat_names:
        - toxicity_probability
- name: Synthetic reasoning (abstract symbols)
  description: Synthetic reasoning tasks defined using abstract symbols based on LIME (Wu et al., 2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: Synthetic reasoning (natural language)
  description: Synthetic reasoning tasks defined using simple natural language based on LIME (Wu et al., 2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - f1_set_match
- name: The Pile
  description: The Pile corpus for measuring lanugage model performance across various domains (Gao et al., 2020).
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: TruthfulQA
  description: The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering (Lin et al., 2022).
  values:
    display:
      k: null
      split: valid
      stat_names:
        - quasi_exact_match
- name: Twitter AAE
  description: The TwitterAAE corpus of Blodgett et al. (2016) for measuring language model performance in tweets as a function of speaker dialect.
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: Twitter AAE (AAE)
  description: The subset of TwitterAAE associated with African American English (AAE) speakers by Blodgett et al. (2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: Twitter AAE (White)
  description: The subset of TwitterAAE associated with White speakers by Blodgett et al. (2021).
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: WikiFact
  description: Scenario introduced in this work, inspired by Petroni et al. (2019), to more extensively test factual knowledge.
  values:
    display:
      k: null
      split: test
      stat_names:
        - quasi_exact_match
- name: WikiText-103
  description: The Wiki-Text103 benchmark for language modeling performance (Merity et al., 2016).
  values:
    display:
      k: null
      split: test
      stat_names:
        - bits_per_byte
- name: XSUM
  description: The XSUM benchmark for text summarization of BBC news articles (Narayan et al., 2018). 
  values:
    display:
      k: null
      split: test
      stat_names:
        - rouge_2

table_settings:
- name: default
  values:
    stat_groups:
      - accuracy
      - bias
      - toxicity
      - efficiency

stat_groups:
- name: accuracy
  description: Average performance of model based on the canonical metric for scenario (e.g. accuracy, F1, ROUGE).
  display_name: Accuracy
  values:
    # stat_names are determined by group.stat_names
    perturbation_names:
    - identity
    - TyposPerturbation
    - SynonymPerturbation
    - dialect
    - person_name
    - gender 
- name: bias
  description: Biases (stereotypical associations, demographic associations) in model generations.
  display_name: Bias
  values:
    stat_names:
    - bias_metric:mode=associations,demographic_category=race
    - bias_metric:mode=associations,demographic_category=gender
    - bias_metric:mode=representation,demographic_category=race
    - bias_metric:mode=representation,demographic_category=gender
    perturbation_names:
    - identity
- name: toxicity
  description: Toxicity in model generations.
  display_name: Toxicity
  values:
    stat_names:
    - toxicity_probability
    perturbation_names:
    - identity
- name: efficiency
  description: The efficiency of the model across both training and inference.
  display_name: Efficiency
  values:
    stat_names:
    - inference_runtime
    - inference_idealized_runtime
    - training_co2_cost
    perturbation_names:
    - identity
