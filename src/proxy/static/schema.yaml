---

adapter:
  - name: method
    description: The high-level strategy for converting instances into a prompt for the language model.
    values:
      - name: generation
        description: Given the input, the model generates the output free-form.
      - name: multiple_choice_joint
        description: Given the input, the model selects from multiple-choice options (A., B., C., D., E.).
      - name: multiple_choice_separate_original
        description: For each answer choice, the model assigns the input and answer choice a probability, returning the answer with maximum probability.
      - name: multiple_choice_separate_calibrated
        description: For each answer choice, the model assigns the input and answer choice a probability, returning the answer with maximum probability when calibrated by answer choice probability.
      - name: language_modeling
        description: Given the input, the model assigns the sequence a probability.
  - name: instructions
    description: The description of the task that is included at the very beginning of the prompt.
  - name: input_prefix
    description: The string that is included before each input (e.g., 'Question:').
  - name: reference_prefix
    description: The string that is included before each reference (for multiple-choice questions).
  - name: output_prefix
    description: The string that is included before the correct answer/predicted output (e.g., 'Answer:').
  - name: max_train_instances
    description: Maximum number of training instances to include in the prompt (currently by randomly sampling).
  - name: max_eval_instances
    description: Maximum number of instances to evaluate on (over all splits - test, valid, etc.).
  - name: num_outputs
    description: Maximum number of possible outputs to generate by sampling multiple outputs.
  - name: num_train_trials
    description: Number of trials, where in each trial we choose an independent, random set of training instances. Used to compute variance.
  - name: model
    description: Name of the language model (<organization>/<model name>) to send requests to.
  - name: temperature
    description: Temperature parameter used in generation.
  - name: max_tokens
    description: Maximum number of tokens to generate.
  - name: stop_sequences
    description: List of sequences, where we stop generation if we encounter any of them.


metrics:
  # Infrastructure metrics:
  # TODO
  - name: num_perplexity_tokens
    display_name: '# tokens'
    description: Average number of tokens in the predicted output (for language modeling, the input too).
  - name: num_bytes
    display_name: '# bytes'
    description: Average number of bytes in the predicted output (for language modeling, the input too).
  # Accuracy metrics:
  - name: exact_match
    display_name: Exact match
    description: Fraction of instances that the predicted output matches a correct reference exactly.
  - name: quasi_exact_match
    display_name: Quasi-exact match
    description: Fraction of instances that the predicted output matches a correct reference up to light processing.
  - name: logprob
    display_name: Log probability
    description: Predicted output's average log probability (input's log prob for language modeling).
  - name: accuracy
    display_name: Accuracy
    description: Fraction of instances that the model correctly classifies based on sequence probability. 
  - name: bits_per_byte
    display_name: Bits/byte
    description: Average number of bits per byte according to model probabilities.
  - name: rouge_2
    display_name: ROUGE-2
    description: Average ROUGE score (Lin, 2004) based on 2-gram overlap.
  - name: f1_set_match
    display_name: F1 (set match)
    description: Average F1 score in terms of set overlap between the model predicted set and correct reference set.
  - name: f1_score
    display_name: F1
    description: Average F1 score in terms of word overlap between the model output and correct reference.
  - name: absolute_value_difference
    display_name: Absolute difference
    description: Average absolute difference between the model output (converted to a number) and the correct reference.
  - name: NDCG@20
    display_name: NDCG@20
    description: Normalized discounted cumulative gain at 20 in information retrieval.
  - name: RR@20
    display_name: RR@20
    description: Mean reciprocal rank at 20 in information retrieval.
  - name: math_equiv
    display_name: Equivalent
    description: Fraction of model outputs that are mathematically equivalent to the correct reference.
  - name: math_equiv_chain_of_thought
    display_name: Equivalent (chain-of-thoughts)
    description: Fraction of model outputs that are mathematically equivalent to the correct reference when using chain-of-thoughts prompting.
  - name: exact_match_indicator
    display_name: Exact match (up to specified indicator)
    description: Fraction of instances that the predicted output matches a correct reference exactly, ignoring text preceding the specified indicator.
  #  Code metrics
  - name: code_eval_acc
    display_name: Accuracy (code evaluation)
    description: Fraction of instances that the model output evaluates to the correct answer.
  - name: pass
    display_name: Pass rate
    description: Fraction of model outputs that pass the associated test cases.
  - name: test_avg
    display_name: 'Avg. # tests'
    description: Average number of tests passed by model outputs.
  - name: strict_acc
    display_name: Strict accuracy 
    description: Fraction of models outputs that pass all associated test cases.
  # Bias metrics:
  - name: bias_metric:mode=associations,demographic_category=race 
    display_name: Stereotypical associations (Race, Profession)
    description: Measures uneven association of racial groups (Asian, Hispanic, White) with target professions.
  - name: bias_metric:mode=associations,demographic_category=gender
    display_name: Stereotypical associations (Gender, Profession)
    description: Measures uneven association of gender groups (male, female) with target professions.
  - name: bias_metric:mode=representation,demographic_category=race
    display_name: Demographic representation (Race)
    description: Measures uneven representation of racial groups (Asian, Hispanic, White).
  - name: bias_metric:mode=representation,demographic_category=gender
    display_name: Demographic representation (Gender)
    description: Measures uneven representation of gender groups (male, female).
  - name: bbq_metric_ambiguous_bias
    display_name: BBQ (ambiguous)
    description: Metric of Parrish et al. (2022) for BBQ on ambiguous examples.
  - name: bbq_metric_unambiguous_bias
    display_name: BBQ (unambiguous)
    description: Metric of Parrish et al. (2022) for BBQ on unambiguous examples.
  # Toxicity metrics
  - name: toxicity_probability
    display_name: Toxicity probability
    description: Fraction of model outputs that are toxic (based on the PerspectiveAPI toxicity classifer).
  # Efficiency metrics:
  - name: training_co2_cost
    display_name: Estimated training cost (kg CO2)
    description: Estimate of the cost of training in CO2.
  - name: inference_runtime
    display_name: Observed inference runtime (s)
    description: Average observed runtime for requests to the model.  
  - name: inference_idealized_runtime
    display_name: Idealized inference runtime (s)
    description: Estimate of the average idealized runtime for requests to the model given its architecture.
  # Disinformation metrics (measure diversity):
  - name: self_bleu
    display_name: Self-BLEU
    description: Self-BLEU metric of Zhu et al. (2018) as a measure of diversity in model generations.
  - name: monte_carlo_entropy
    display_name: Entropy (Monte Carlo)
    description: Monte Carlo estimate of the entropy as a measure of diversity in model generations.
  # Copyright metrics (measure copying/overlap):
  - name: longest_common_prefix_length
    display_name: Longest common prefix length
    description: Average length of longest common prefix between model generation and reference.
  - name: edit_distance
    display_name: Edit distance (Levenshtein)
    description: Average Levenshtein edit distance between model generation and reference.


perturbations:
  - name: worst_TyposPerturbation # TODO: typos
    display_name: Typos, Worst
    description: >
      Randomly adds typos to each token in the input with probability 0.05 and computes the per-instance worst-case
      performance between perturbed and unperturbed versions.
  - name: worst_SynonymPerturbation # TODO: synonym
    display_name: Synonym, Worst
    description: >
      Randomly substitutes words in the input with WordNet synonyms with probability 0.5 and computes the per-instance
      worst-case performance between perturbed and unperturbed versions.
  - name: worst_dialect
    display_name: SAE -> AAE, Worst
    description: >
      Deterministically substitutes SAE words in input with AAE counterparts using validated dictionary of Ziems et al.
      (2022) and computes the per-instance worst-case performance between perturbed and unperturbed versions.
  - name: worst_person_name
    display_name: First names by race (White -> Black), Worst
    description: >
      Deterministically substitutes White first names with Black first names sampled from the lists of Caliskan et al.
      (2017) and computes the per-instance worst-case performance between perturbed and unperturbed versions.
  - name: worst_gender 
    display_name: Pronouns by gender (Male -> Female), Worst
    description: >
      Deterministically substitutes male pronouns with female pronouns and computes the per-instance worst-case
      performance between perturbed and unperturbed versions.
  # TODO: Remove the perturbations below. Included for now to help us ensure
  # the correctness of our perturbations.
  - name: TyposPerturbation # TODO: typos
    display_name: Typos
  - name: SynonymPerturbation # TODO: synonym
    display_name: Synonym
  - name: dialect
    display_name: SAE -> AAE
  - name: person_name
    display_name: First names by race (White -> Black)
  - name: gender
    display_name: Pronouns by gender (Male -> Female)


metric_groups:
  - name: accuracy
    display_name: Accuracy
    description: Average performance of model based on the canonical metric for scenario (e.g. accuracy, F1, ROUGE).
    display_k: 1
    # stat_names are determined by group.stat_names
    # TODO: Once the perturbation name field changes are in, move the
    # perturbation name structure here to display the worst.
    perturbation_names:
      - identity
      - TyposPerturbation # TODO: typos
      - worst_TyposPerturbation # TODO: typos
      - SynonymPerturbation # TODO: synonym
      - worst_SynonymPerturbation # TODO: synonym
      - dialect
      - worst_dialect
      - person_name
      - worst_person_name
      - gender
      - worst_gender 
  - name: bias
    display_name: Bias
    description: Biases (stereotypical associations, demographic associations) in model generations.
    display_k: 1
    stat_names:
      - bias_metric:mode=associations,demographic_category=race
      - bias_metric:mode=associations,demographic_category=gender
      - bias_metric:mode=representation,demographic_category=race
      - bias_metric:mode=representation,demographic_category=gender
    perturbation_names:
      - identity
  - name: toxicity
    display_name: Toxicity
    description: Toxicity in model generations.
    display_k: 1
    stat_names:
      - toxicity_probability
    perturbation_names:
      - identity
  - name: efficiency
    display_name: Efficiency
    description: The efficiency of the model across both training and inference.
    display_k: null
    stat_names:
      - inference_runtime
      - inference_idealized_runtime
      - training_co2_cost
    perturbation_names:
      - identity


groups:
  - name: APPS (Code) # TODO: code_apps
    display_name: APPS (Code)
    description: The APPS benchmark for measuring competence on code challenges (Hendrycks et al., 2021).
    display:
      split: test
      stat_names:
        - test_avg
        - strict_acc
  - name: bAbI # TODO: babi_qa
    display_name: bAbI 
    description: The bAbI benchmark for measuring understanding and reasoning (Weston et al., 2015).
    display:
      split: test
      stat_names:
        - quasi_exact_match
  - name: BBQ # TODO: bbq
    display_name: BBQ (Bias Benchmark for Question Answering)
    description: The Bias Benchmark for Question Answering (BBQ) for measuring social bias in question answering in ambiguous and unambigous context (Parrish et al., 2022).
    display:
      split: test
      stat_names:
        - bbq_metric_ambiguous_bias
        - bbq_metric_unambiguous_bias
  - name: BLiMP # TODO: blimp
    display_name: BLiMP (The Benchmark of Linguistic Minimal Pairs for English)
    description: The Benchmark of Linguistic Minimal Pairs for English (BLiMP) for measuring performance on linguistic phenomena using minimal pair design (Warstadt et al., 2020). 
    display:
      split: test
      stat_names:
        - accuracy
  - name: BOLD # TODO: bold
    display_name: BOLD (Bias in Open-Ended Language Generation Dataset)
    description: The  Bias in Open-Ended Language Generation Dataset (BOLD) for measuring biases and toxicity in open-ended language generation (Dhamala et al., 2021).
    display:
      split: test
      stat_names:
        - toxicity_probability
  - name: BoolQ # TODO: boolq
    display_name: BoolQ
    description: The BoolQ benchmark for binary (yes/no) question answering (Clark et al., 2019).
    display:
      split: valid
      stat_names:
        - quasi_exact_match
  - name: CivilComments # TODO: civil_comments
    display_name: CivilComments
    description: The CivilComments benchmark for toxicity detection (Borkan et al., 2019).
    display:
      split: test
      stat_names:
        - quasi_exact_match
  - name: CNN/DailyMail 
    display_name: CNN/DailyMail
    description: The CNN/DailyMail benchmark for text summarization (Hermann et al., 2015; Nallapati et al.,2016). 
    display:
      split: test
      stat_names:
        - rouge_2
  - name: CommonsenseQA # TODO: commonsenseqa
    display_name: CommonsenseQA
    description: The CommonsenseQA benchmark for commonsense-inteisive question answering (Talmor et al., 2019).
    display:
      split: valid
      stat_names:
        - quasi_exact_match
  - name: Copyright # TODO: copyright
    display_name: Copyright
    description: Scenario introduced in this work to measure copyright and memorization behavior, based off of Carlini et al. (2021). 
    display:
      split: test
      stat_names:
        - longest_common_prefix_length
        - edit_distance
  - name: Data imputation # TODO: entity_data_imputation
    display_name: Data imputation
    description: Scenario from Mei et al. (2021) that tests the ability to impute missing entities in a data table. 
    display:
      split: test
      stat_names:
        - quasi_exact_match
  - name: Disinformation # TODO: disinformation
    display_name: Disinformation
    description: Scenario from Buchanan et al. (2021) that tests the ability to reiterate or generate disinformation content.
    display:
      split: valid
      stat_names:
        - self_bleu
        - monte_carlo_entropy
  - name: Disinformation (reiteration) # TODO: disinformation_reiteration
    display_name: Disinformation (reiteration)
    description: Scenario from Buchanan et al. (2021) that tests the ability to reiterate disinformation content.
    display:
      split: valid
      stat_names:
        - self_bleu
        - monte_carlo_entropy
  - name: Disinformation (wedging) # TODO: disinformation_wedging
    display_name: Disinformation (wedging)
    description: Scenario from Buchanan et al. (2021) that tests the ability to generate divisive and wedging content.
    display:
      split: valid
      stat_names:
        - self_bleu
        - monte_carlo_entropy
  - name: Dyck # TODO: dyck_language
    display_name: Dyck
    description: Scenario testing hierarchical reasoning through the Dyck formal languages (Suzgun et al., 2019).
    display:
      split: test
      stat_names:
        - exact_match_indicator
  - name: Entity matching # TODO: entity_matching
    display_name: Entity matching
    description: Scenario from Mudgal et al. (2018) that tests the ability to determine if two entities match. 
    display:
      split: test
      stat_names:
        - quasi_exact_match
  - name: GSM8K # TODO: gsm
    display_name: GSM8K (Grade school math word problems)
    description: The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems (Cobbe et al., 2021).
    display:
      split: test
      stat_names:
        - exact_match_indicator
  - name: HellaSwag  # TODO: hellaswag
    display_name: HellaSwag
    description: The HellaSwag benchmark for commonsense reasoning in question answering (Zellers et al., 2019).
    display:
      split: valid
      stat_names:
        - quasi_exact_match
  - name: HumanEval (Code) # TODO: code_humaneval
    display_name: HumanEval (Code)
    description: The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings (Chen et al., 2021).
    display:
      split: test
      stat_names:
        - code_eval_acc
        - pass
  - name: ICE # TODO: ice
    display_name: ICE (International Corpus of English)
    description: The International Corpus of English (ICE) for measuring language model performance across English varieties (https://www.ice-corpora.uzh.ch/en.html).
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: ICE (Canada) # TODO: ice_can
    display_name: ICE (International Corpus of English - Canada)
    description: The subset of the International Corpus of English (ICE) drawn from English speakers in Canada.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: ICE (Hong Kong) # TODO: ice_hk
    display_name: ICE (International Corpus of English - Hong Kong)
    description: The subset of the International Corpus of English (ICE) drawn from English speakers in Hong Kong.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: ICE (India) # TODO: ice_ind
    display_name: ICE (International Corpus of English - India)
    description: The subset of the International Corpus of English (ICE) drawn from English speakers in India.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: ICE (Jamaica) # TODO: ice_ja
    display_name: ICE (International Corpus of English - Jamaica)
    description: The subset of the International Corpus of English (ICE) drawn from English speakers in Jamaica.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: ICE (Philippines) # TODO: ice_phi
    display_name: ICE (International Corpus of English - Philippines)
    description: The subset of the International Corpus of English (ICE) drawn from English speakers in the Philippines.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: ICE (Singapore) # TODO: ice_sin
    display_name: ICE (International Corpus of English - Singapore)
    description: The subset of the International Corpus of English (ICE) drawn from English speakers in Singapore.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: ICE (USA) # TODO: ice_usa
    display_name: ICE (International Corpus of English - USA)
    description: The subset of the International Corpus of English (ICE) drawn from English speakers in the United States of America.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: ICE (spoken) # TODO: ice_spoken
    display_name: ICE (International Corpus of English - spoken)
    description: The subset of the International Corpus of English (ICE) based on transcribed speech.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: ICE (written) # TODO: ice_written
    display_name: ICE (International Corpus of English - written)
    description: The subset of the International Corpus of English (ICE) based on written text.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: ICE (female) # TODO: ice_female
    display_name: ICE (International Corpus of English - female)
    description: The subset of the International Corpus of English (ICE) drawn from self-identified female speakers.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: ICE (male) # TODO: ice_male
    display_name: ICE (International Corpus of English - male)
    description: The subset of the International Corpus of English (ICE) drawn from self-identified male speakers.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: IMDB # TODO: imdb
    display_name: IMDB
    description: The IMDB benchmark for sentiment analysis in movie review (Maas et al., 2011).
    display:
      split: valid
      stat_names:
        - quasi_exact_match
  - name: LegalSupport # TODO: legal_support
    display_name: LegalSupport
    description: Scenario introduced in this work to measure fine-grained legal reasoning through reverse entailment.
    display:
      split: test
      stat_names:
        - quasi_exact_match
  - name: LSAT # TODO: last_qa
    display_name: LSAT
    description: The LSAT benchmark for measuring analytical reasoning on the Law School Admission Test (LSAT; Zhong et al., 2021).
    display:
      split: test
      stat_names:
        - quasi_exact_match
  - name: MATH # TODO: math_regular
    display_name: MATH
    description: The MATH benchmark for measuring mathematical problem solving on competition math problems (Hendrycks et al., 2021).
    display:
      split: test
      stat_names:
        - math_equiv
  - name: MATH (chain_of_thought) # TODO: math_chain_of_thought
    display_name: MATH (chain-of-thoughts)
    description: The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thoughts style reasoning (Hendrycks et al., 2021).
    display:
      split: test
      stat_names:
        - math_equiv_chain_of_thought
  - name: MMLU # TODO: mmlu
    display_name: MMLU (Massive Multitask Language Understanding)
    description: The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intesive question answering across 57 domains (Hendrycks et al., 2021).
    display:
      split: test
      stat_names:
        - quasi_exact_match
  - name: MS MARCO (regular) # TODO: msmarco_regular
    display_name: MS MARCO (regular track)
    description: The MS MARCO benchmark's regular track for passage retrieval in information retrieval (https://microsoft.github.io/msmarco/).
    display:
      split: valid
      stat_names:
        - RR@20
  - name: MS MARCO (TREC) # TODO: msmarco_trec
    display_name: MS MARCO (TREC track)
    description: The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval (https://trec.nist.gov).
    display:
      split: valid
      stat_names:
        - NDCG@20
  - name: NarrativeQA # TODO: narrative_qa
    display_name: NarrativeQA
    description: The NarrativeQA benchmark for reading comprehension over narratives (Kočiský et al., 2017).
    display:
      split: test
      stat_names:
        - f1_score
  - name: NaturalQuestions # TODO: natural_qa
    display_name: NaturalQuestions
    description: The NaturalQuestions benchmark for question answering based on naturally-occurring queries through Google Search (Kwiatkowski et al., 2019).
    display:
      split: valid
      stat_names:
        - f1_score
  - name: NaturalQuestions (closed-book) # TODO: natural_qa_closedbook
    display_name: NaturalQuestions (closed-book)
    description: The variant of NaturalQuestions that does not include the Wikipedia page with the answer as input.
    display:
      split: valid
      stat_names:
        - f1_score
  - name: NaturalQuestions (open-book) # TODO: natural_qa_openbook_longans
    display_name: NaturalQuestions (open-book)
    description: The variant of NaturalQuestions that includes the Wikipedia page with the answer as input.
    display:
      split: valid
      stat_names:
        - f1_score
  - name: NewsQA # TODO: news_qa
    display_name: NewsQA
    description: The NewsQA benchmark for reading comprehension over CNN news articles (Trischler et al., 2017).
    display:
      split: valid
      stat_names:
        - f1_score
  - name: Numeracy # TODO: numeracy
    display_name: Numerical reasoning
    description: Scenario introduced in this work to test numerical reasoning via symbolic regression.
    display:
      split: test
      stat_names:
        - absolute_value_difference
  - name: OpenbookQA # TODO: openbookqa
    display_name: OpenbookQA
    description: The OpenbookQA benchmark for commonsense-intensive open book question answering (Mihaylov et al., 2018).
    display:
      split: test
      stat_names:
        - quasi_exact_match
  - name: QuAC # TODO: quac
    display_name: QuAC (Question Answering in Context)
    description: The QuAC benchmark for question answering in the context of dialogues (Choi et al., 2018).
    display:
      split: valid
      stat_names:
        - f1_score
  - name: RAFT # TODO: raft
    display_name: RAFT (Real-world Annotated Few-Shot)
    description: The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks (Alex et al., 2021).
    display:
      split: test
      stat_names:
        - quasi_exact_match
  - name: RealToxicityPrompts # TODO: real_toxicity_prompts
    display_name: RealToxicityPrompts
    description: The RealToxicityPrompts dataset for measuring toxicity in prompted model generations (Gehman et al., 2020).
    display:
      split: test
      stat_names:
        - toxicity_probability
  - name: Synthetic reasoning (abstract symbols) # TODO: synthetic_reasoning_abstract
    display_name: Synthetic reasoning (abstract symbols)
    description: Synthetic reasoning tasks defined using abstract symbols based on LIME (Wu et al., 2021).
    display:
      split: test
      stat_names:
        - quasi_exact_match
  - name: Synthetic reasoning (natural language) # TODO: synthetic_reasoning_natural
    display_name: Synthetic reasoning (natural language)
    description: Synthetic reasoning tasks defined using simple natural language based on LIME (Wu et al., 2021).
    display:
      split: test
      stat_names:
        - f1_set_match
  - name: The Pile # TODO: the_pile
    display_name: The Pile
    description: The Pile corpus for measuring lanugage model performance across various domains (Gao et al., 2020).
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: TruthfulQA # TODO: truthful_qa
    display_name: TruthfulQA
    description: The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering (Lin et al., 2022).
    display:
      split: valid
      stat_names:
        - quasi_exact_match
  - name: Twitter AAE # TODO: twitter_aae
    display_name: TwitterAAE
    description: The TwitterAAE corpus of Blodgett et al. (2016) for measuring language model performance in tweets as a function of speaker dialect.
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: Twitter AAE (AAE) # TODO: twitter_aae_aa
    display_name: TwitterAAE (African American English)
    description: The subset of TwitterAAE associated with African American English (AAE) speakers by Blodgett et al. (2021).
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: Twitter AAE (White) # TODO: twitter_aae_white
    display_name: TwitterAAE (White)
    description: The subset of TwitterAAE associated with White speakers by Blodgett et al. (2021).
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: WikiFact # TODO: wikifact
    display_name: WikiFact
    description: Scenario introduced in this work, inspired by Petroni et al. (2019), to more extensively test factual knowledge.
    display:
      split: test
      stat_names:
        - quasi_exact_match
  - name: WikiText-103 # TODO: wikitext_103
    display_name: WikiText-103
    description: The Wiki-Text103 benchmark for language modeling performance (Merity et al., 2016).
    display:
      split: test
      stat_names:
        - bits_per_byte
  - name: XSUM # TODO: summarization_xsum
    display_name: XSUM
    description: The XSUM benchmark for text summarization of BBC news articles (Narayan et al., 2018). 
    display:
      split: test
      stat_names:
        - rouge_2
