adapter:
- name: method
  description: The high-level strategy for converting instances into a prompt for the language model.
  values:
  - name: generation
    description: Include the input and ask for the output free-form generation.
  - name: multiple_choice
    description: All the references are included in their prompt as multiple-choice options (A., B., C., D., E.).
- name: instructions
  description: The description of the task that is included at the beginning of the prompt.
- name: input_prefix
  description: The string that is included before each input (e.g., 'Question:').
- name: reference_prefix
  description: The string that is included before each reference (for multiple-choice questions).
- name: output_prefix
  description: The string that is included before each reference/predicted output (e.g., 'Answer:').
- name: max_train_instances
  description: Maximum number of training instances to include in the prompt (currently by randomly sampling).
- name: max_eval_instances
  description: Maximum number of instances to evaluate on (over all splits - test, valid, etc.).
- name: num_outputs
  description: Maximum number of possible outputs to generate (either sampling or top k).
- name: num_train_trials
  description: Number of trials, where in each trial we choose an independent, random set of training instances.  Used to compute variance.
- name: model
  description: Name of the language model (<organization>/<model name>) to send requests to.
- name: temperature
  description: Temperature parameter used in generation.
- name: max_tokens
  description: Maximum number of tokens to generate.
- name: stop_sequences
  description: List of sequences, where we stop generation if we encouter any of them.

metrics:
- name: exact_match
  description: Whether the predicted output matches one of the correct references.
- name: quasi_exact_match
  description: Whether the predicted output matches one of the correct references up to lightweight processing.
- name: logprob
  description: Log-probability of the predicted output (for language modeling, the input too).
- name: num_perplexity_tokens
  description: Number of tokens in the output (for language modeling, the input too).
- name: num_bytes
  description: Number of bytes in the output (for language modeling, the input too).

scenarioGroups:
- name: NarrativeQA
  display:
    split: valid
    stat_names:
      - f1_score
  class_name: benchmark.narrativeqa_scenario.NarrativeQAScenario
- name: BoolQ
  display:
    split: valid
    stat_names:
      - quasi_exact_match
  class_name: benchmark.boolq_scenario.BoolQScenario
- name: QuAC
  display:
    split: valid
    stat_names:
      - f1_score
  class_name: benchmark.quac_scenario.QuACScenario
- name: NaturalQuestions
  display:
    split: valid
    stat_names:
      - f1_score
  class_name: benchmark.natural_qa_scenario.NaturalQAScenario
- name: TruthfulQA
  display:
    split: valid
    stat_names:
      - quasi_exact_match
  class_name: benchmark.truthful_qa_scenario.TruthfulQAScenario
- name: MMLU
  display:
    split: valid
    stat_names:
      - quasi_exact_match
  class_name: benchmark.mmlu_scenario.MMLUScenario
- name: IMDB
  display:
    split: valid
    stat_names:
      - quasi_exact_match
  class_name: benchmark.imdb_scenario.IMDBScenario
- name: RAFT
  display:
    split: test
    stat_names:
      - quasi_exact_match
  class_name: benchmark.raft_scenario.RAFTScenario
- name: Entity matching
  display:
    split: valid
    stat_names:
      - quasi_exact_match
  class_name: benchmark.entity_matching_scenario.EntityMatchingScenario
- name: Data imputation
  display:
    split: valid
    stat_names:
      - quasi_exact_match
  class_name: benchmark.entity_data_imputation_scenario.EntityDataImputationScenario
- name: CivilComments
  display:
    split: test
    stat_names:
      - quasi_exact_match
  class_name: benchmark.civil_comments_scenario.CivilCommentsScenario
- name: The Pile
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.the_pile_scenario.ThePileScenario
- name: WikiFact
  display:
    split: valid
    stat_names:
      - absolute_value_difference
  class_name: benchmark.wikifact_scenario.WIKIFactScenario
- name: Numeracy
  display:
    split: test
    stat_names:
      - absolute_value_difference
  class_name: benchmark.numeracy_scenario.NumeracyScenario
- name: Synthetic reasoning (abstract symbols)
  display:
    split: test
    stat_names:
      - quasi_exact_match
  class_name: benchmark.synthetic_reasoning_scenario.SyntheticReasoningScenario
- name: Synthetic reasoning (natural language)
  display:
    split: test
    stat_names:
      - quasi_exact_match
  class_name: benchmark.synthetic_reasoning_natural_scenario.SRNScenario
- name: bAbI
  display:
    split: test
    stat_names:
      - quasi_exact_match
  class_name: benchmark.babi_qa_scenario.BabiQAScenario
- name: Dyck
  display:
    split: test
    stat_names:
      - exact_match_indicator
  class_name: benchmark.dyck_language_scenario.DyckLanguageScenario
- name: GSM8K
  display:
    split: test
    stat_names:
      - exact_match_indicator
  class_name: benchmark.gsm_scenario.GSM8KScenario
- name: LSAT
  display:
    split: test
    stat_names:
      - quasi_exact_match
  class_name: benchmark.lsat_qa_scenario.LSATScenario
- name: LegalSupport
  display:
    split: test
    stat_names:
      - quasi_exact_match
  class_name: benchmark.legal_support_scenario.LegalSupportScenario
- name: Copyright
  display:
    split: test
    stat_names:
      - longest_common_prefix_length
      - edit_distance
  class_name: benchmark.copyright_scenario.CopyrightScenario
- name: BBQ
  display:
    split: test
    stat_names:
      - TODO
  class_name: benchmark.bbq_scenario.BBQScenario
- name: RealToxicityPrompts
  display:
    split: test
    stat_names:
      - toxicity_probability
  class_name: benchmark.real_toxicity_prompts_scenario.RealToxicityPromptsScenario
- name: BOLD
  display:
    split: test
    stat_names:
      - toxicity_probability
  class_name: benchmark.bold_scenario.BOLDScenario
- name: WikiText-103
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.wikitext_103_scenario.Wikitext103Scenario
- name: BLiMP
  display:
    split: test
    stat_names:
      - accuracy
  class_name: benchmark.blimp_scenario.BLiMPScenario
- name: NewsQA
  display:
    split: test
    stat_names:
      - f1_score
  class_name: benchmark.newsqa_scenario.NewsQAScenario
- name: MATH
  display:
    split: test
    stat_names:
      - math_equiv
  class_name: benchmark.math_scenario.MATHScenario
  args:
    use_chain_of_thought: False
- name: MATH (chain_of_thought)
  display:
    split: test
    stat_names:
      - math_equiv_chain_of_thought
  class_name: benchmark.math_scenario.MATHScenario
  args:
    use_chain_of_thought: True
- name: Code (HumanEval)
  display:
    split: test
    stat_names:
      - code_eval_acc
      - pass
  class_name: benchmark.code_scenario.CodeScenario
  args:
    dataset: HumanEval
- name: Code (APPS)
  display:
    split: test
    stat_names:
      - test_avg
      - strict_acc
  class_name: benchmark.code_scenario.CodeScenario
  args:
    dataset: APPS
- name: Disinformation (reiteration)
  display:
    split: test
    stat_names:
      - self_bleu
      - monte_carlo_entropy
  class_name: benchmark.disinformation_scenario.DisinformationScenario
  args:
    capability: reiteration
- name: Disinformation (wedging)
  display:
    split: test
    stat_names:
      - self_bleu
      - monte_carlo_entropy
  class_name: benchmark.disinformation_scenario.DisinformationScenario
  args:
    capability: wedging
- name: HellaSwag
  display:
    split: test
    stat_names:
      - quasi_exact_match
  class_name: benchmark.commonsense_scenario.CommonSenseScenario
  args:
    dataset: hellaswag
- name: OpenbookQA
  display:
    split: test
    stat_names:
      - quasi_exact_match
  class_name: benchmark.commonsense_scenario.CommonSenseScenario
  args:
    dataset: openbookqa
- name: CommonsenseQA
  display:
    split: valid
    stat_names:
      - quasi_exact_match
  class_name: benchmark.commonsense_scenario.CommonSenseScenario
  args:
    dataset: commonsenseqa
- name: MS MARCO (regular)
  display:
    split: test
    stat_names:
      - RR@20
  class_name: benchmark.msmarco_scenario.MSMARCOScenario
  args:
    track: regular
- name: MS MARCO (TREC)
  display:
    split: test
    stat_names:
      - NDCG@20
  class_name: benchmark.msmarco_scenario.MSMARCOScenario
  args:
    track: trec
- name: CNN/DailyMail
  display:
    split: test
    stat_names:
      - rouge_2
  class_name: benchmark.summarization_scenario.SummarizationScenario
  args:
    dataset_name: cnn-dm
- name: XSUM
  display:
    split: test
    stat_names:
      - rouge_2
  class_name: benchmark.summarization_scenario.SummarizationScenario
  args:
    dataset_name: xsum-sampled
- name: TwitterAAE (AAE)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.twitter_aae_scenario.TwitterAAEScenario
  args:
    demographic: aa
- name: TwitterAAE (White)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.twitter_aae_scenario.TwitterAAEScenario
  args:
    demographic: white
- name: ICE (Canada)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.ice_scenario.ICEScenario
  args:
    subset: CAN
- name: ICE (Hong Kong)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.ice_scenario.ICEScenario
  args:
    subset: HK
- name: ICE (India)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.ice_scenario.ICEScenario
  args:
    subset: IND
- name: ICE (Japan)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.ice_scenario.ICEScenario
  args:
    subset: JA
- name: ICE (Phillipines)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.ice_scenario.ICEScenario
  args:
    subset: PHI
- name: ICE (Singapore)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.ice_scenario.ICEScenario
  args:
    subset: SIN
- name: ICE (USA)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.ice_scenario.ICEScenario
  args:
    subset: USA
- name: ICE (spoken)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.ice_scenario.ICEScenario
  args:
    split: spoken
- name: ICE (written)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.ice_scenario.ICEScenario
  args:
    split: written
- name: ICE (female)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.ice_scenario.ICEScenario
  args:
    gender: F
- name: ICE (male)
  display:
    split: test
    stat_names:
      - bits_per_byte
  class_name: benchmark.ice_scenario.ICEScenario
  args:
    gender: M
