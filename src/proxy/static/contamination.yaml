---

contamination:
  - models:
    - together/t0pp
    scenario_groups:
    - hellaswag
    - openbookqa
    - boolq
    - summarization_xsum
    - summarization_cnndm
    - imdb
    description: The T0++ model is explicitly trained on these datasets. See Table 5 on page 24 of https://arxiv.org/pdf/2110.08207.pdf.
  - models:
    - openai/davinci
    - openai/curie
    - openai/babbage
    - openai/ada
    - openai/text-davinci-001
    - openai/text-curie-001
    - openai/text-babbage-001
    - openai/text-ada-001
    - openai/text-davinci-002
    # - openai/text-curie-002
    # - openai/text-babbage-002
    # - openai/text-ada-002
    - openai/code-davinci-002
    - openai/code-davinci-001
    - openai/code-cushman-001
    scenario_groups:
    - natural_qa
    - natural_qa_closedbook
    - natural_qa_openbook_longans
    - hellaswag
    - openbookqa
    - boolq
    - quac
    description: Brown et al. perform an analysis of the contamination for GPT-3 and its derivatives. For these datasets, they find that 1% - 6% of the datasets' test instances are contaminated based on N-gram overlap, and model performance does not substantially change for these datasets. See Table C.1 on page 45 of https://arxiv.org/pdf/2005.14165.pdf.
